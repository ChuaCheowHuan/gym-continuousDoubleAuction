{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f05ZH97QkoJf"
   },
   "source": [
    "# Sample training script with naive competitive self-play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GPU Diagnostics\n",
    "# import torch\n",
    "# print(\"=\"*50)\n",
    "# print(\"GPU Diagnostics:\")\n",
    "# print(\"=\"*50)\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# print(f\"CUDA version (built with): {torch.version.cuda}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "#     print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "#     print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "# else:\n",
    "#     print(\"âŒ No GPU detected by PyTorch!\")\n",
    "#     print(\"\\nPossible solutions:\")\n",
    "#     print(\"1. Install PyTorch with CUDA support:\")\n",
    "#     print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "#     print(\"2. Check NVIDIA drivers: nvidia-smi\")\n",
    "#     print(\"3. Verify CUDA toolkit is installed\")\n",
    "# print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcLSdJuUkTrX"
   },
   "source": [
    "### Switch directory in Google drive so as to import CDA env.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756087231922,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "0roHXj0tvvLg"
   },
   "outputs": [],
   "source": [
    "is_colab = False\n",
    "# is_colab = True\n",
    "\n",
    "# is_1st_run = False\n",
    "is_1st_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1756087232001,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "PAqVG2cqjLXM"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# %cd \"/root/ray_results/\"\n",
    "# !ls -l\n",
    "# #!rm -rf PPO_continuousDoubleAuction-v0_*\n",
    "# !ls -l\n",
    "# !pwd\n",
    "\n",
    "# %cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
    "# !ls -l\n",
    "\n",
    "# #!pip install -r requirements.txt\n",
    "\n",
    "# #!pip install tensorflow==2.2.0\n",
    "# #!pip install ray[rllib]==0.8.5\n",
    "\n",
    "# #!pip show tensorflow\n",
    "# #!pip show ray\n",
    "\n",
    "# #!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232034,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "_ZJO7gUwngjr",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if is_colab == False and is_1st_run == True:\n",
    "    !pip install sortedcontainers\n",
    "    !!pip install scikit-learn\n",
    "    !pip install tabulate\n",
    "    !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232036,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "vgzysJOX0HZJ"
   },
   "outputs": [],
   "source": [
    "# !pip install -U ipywidgets\n",
    "# !pip install pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232038,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "e9q-QyPhngjt"
   },
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1756087232056,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "ZtVHJhPMngju"
   },
   "outputs": [],
   "source": [
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756087232069,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "CsWAV-_mngju"
   },
   "outputs": [],
   "source": [
    "# !pip install -e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1756087232086,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "uZpGXbLJngju"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall continuousDoubleAuction\n",
    "# !pip uninstall continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232116,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "t8WyPN_qngju"
   },
   "outputs": [],
   "source": [
    "# !pip show continuousDoubleAuction\n",
    "# !pip show continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232118,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "DYuxehQengjv"
   },
   "outputs": [],
   "source": [
    "# os.chdir('gym_continuousDoubleAuction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232119,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "r5E-HRDDngjv"
   },
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17865,
     "status": "ok",
     "timestamp": 1756087249985,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "D9EIlrs1pFq6",
    "outputId": "1fbfa0a4-3d1a-469e-f192-ec15a35c53de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_colab == True:\n",
    "    !pip install -U ray[rllib]==2.48.0\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "    %cd 'gdrive/MyDrive/Colab Notebooks/MARL/gym-continuousDoubleAuction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18197,
     "status": "ok",
     "timestamp": 1756087268180,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "WavBRshypJfb",
    "outputId": "caa88e03-1469-4d4a-e271-1b3079b750e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 11:15:33,112\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-12-13 11:15:34,410\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version: 2.47.1\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib\n",
    "import ray.tune\n",
    "\n",
    "print(\"Ray version:\", ray.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3777,
     "status": "ok",
     "timestamp": 1756087271959,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "auFbWGSNpFyK",
    "outputId": "198343fe-c5a0-427a-faed-035053791616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gymnasium\n",
      "Version: 1.0.0\n",
      "Summary: A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym).\n",
      "Home-page: https://farama.org\n",
      "Author: \n",
      "Author-email: Farama Foundation <contact@farama.org>\n",
      "License: MIT License\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: cloudpickle, farama-notifications, numpy, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7ZHcwBWkXVM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1756087272286,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "7UW3INjDipTC",
    "outputId": "e75fd1c2-c9a3-4a6e-a19b-86c497bfd501",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports all OK.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::DeprecationWarning'\n",
    "\n",
    "import argparse\n",
    "\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
    "\n",
    "from gym_continuousDoubleAuction.train.model.model_handler import CustomRLModule\n",
    "\n",
    "from gym_continuousDoubleAuction.train.policy.policy_handler import (\n",
    "    # make_RandomPolicy,\n",
    "    # gen_policy,\n",
    "    # set_agents_policies,\n",
    "    # create_train_policy_list,\n",
    "    create_multi_agent_config,\n",
    "    policy_mapping_fn,\n",
    "    # create_and_train_algorithm,\n",
    ")\n",
    "from gym_continuousDoubleAuction.train.weight.weight_handler import (\n",
    "    get_trained_policies_name, get_max_reward_ind, cp_weight)\n",
    "from gym_continuousDoubleAuction.train.storage.store_handler import storage\n",
    "\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.train.callbk.callbk_handler import store_eps_hist_data\n",
    "from gym_continuousDoubleAuction.train.callbk.self_play_callback_mod import SelfPlayCallback\n",
    "\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.train.logger.log_handler import (\n",
    "    create_dir, log_g_store, load_g_store)\n",
    "from gym_continuousDoubleAuction.train.plotter.plot_handler import (\n",
    "    plot_storage, plot_LOB_subplot, plot_sum_ord_imb, plot_mid_prices)\n",
    "from gym_continuousDoubleAuction.train.helper.helper import (\n",
    "    ord_imb, sum_ord_imb, mid_price)\n",
    "\n",
    "\n",
    "print(f'Imports all OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDnpi8k5kbYo"
   },
   "source": [
    "### Global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9751,
     "status": "ok",
     "timestamp": 1756087282038,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "UqzjVWUsPykm",
    "outputId": "29b59972-64ec-4d61-e448-ad4b94ab11c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder creation failed or folder already exists: results/\n",
      "Folder creation failed or folder already exists: results/log_g_store/\n",
      "['agent_1', 'agent_3', 'agent_2', 'agent_0']\n",
      "Box(-inf, inf, (40,), float32)\n",
      "Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 11:15:36,478\tWARNING services.py:2152 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.86gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-12-13 11:15:36,620\tINFO worker.py:1917 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m It looks like you're creating a detached actor in an anonymous namespace. In order to access this actor in the future, you will need to explicitly connect to this namespace with ray.init(namespace=\"5f837e27-3363-4c0e-99cf-ab8c18df4626\", ...)\n"
     ]
    }
   ],
   "source": [
    "# CDA_env args\n",
    "num_agents = 4\n",
    "num_trained_agent = 2 #\n",
    "num_policies = num_agents # Each agent is using a separate policy\n",
    "num_of_traders = num_agents\n",
    "tape_display_length = 10\n",
    "tick_size = 1\n",
    "init_cash = 1000000\n",
    "# max_step = 4096 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n",
    "max_step = 1024 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n",
    "is_render = False\n",
    "\n",
    "# RLlib config\n",
    "# train_policy_list = create_train_policy_list(num_trained_agent, \"policy_\")\n",
    "#num_cpus = 0.25\n",
    "num_gpus = 0.75 #0\n",
    "num_cpus_per_worker = 0.25\n",
    "num_gpus_per_worker = 0\n",
    "num_workers = 2\n",
    "num_envs_per_worker = 4\n",
    "batch_mode = \"complete_episodes\"\n",
    "# rollout_fragment_length = 128\n",
    "num_episodes_per_iter = 4\n",
    "# agent_time_step_per_episode = max_step * num_agents\n",
    "# train_batch_size = agent_time_step_per_episode * num_episodes_per_iter\n",
    "train_batch_size = max_step * num_episodes_per_iter\n",
    "# sgd_minibatch_size = 256\n",
    "num_iters = 16\n",
    "\n",
    "# log_base_dir = \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/results/\"\n",
    "log_base_dir = \"results/\"\n",
    "log_dir = log_base_dir + \"ray_results/\"\n",
    "\n",
    "# Chkpt & restore\n",
    "local_dir = log_base_dir + \"chkpt/\"\n",
    "chkpt_freq = 10\n",
    "chkpt = 320\n",
    "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
    "is_restore = True # True / False\n",
    "\n",
    "# log & load\n",
    "log_g_store_dir = log_base_dir + \"log_g_store/\"\n",
    "create_dir(log_base_dir)\n",
    "create_dir(log_g_store_dir)\n",
    "\n",
    "# Environment configuration\n",
    "env_config = {\n",
    "    \"num_of_agents\": num_agents,\n",
    "    \"init_cash\": init_cash,\n",
    "    \"tick_size\": tick_size,\n",
    "    \"tape_display_length\": tape_display_length,\n",
    "    \"max_step\": max_step,\n",
    "    \"is_render\": is_render\n",
    "}\n",
    "\n",
    "# get obs & act spaces from dummy CDA env\n",
    "# single_CDA_env = continuousDoubleAuctionEnv(\n",
    "#     num_of_traders,\n",
    "#     init_cash,\n",
    "#     tick_size,\n",
    "#     tape_display_length,\n",
    "#     max_step,\n",
    "#     is_render)\n",
    "single_CDA_env = continuousDoubleAuctionEnv(env_config)\n",
    "obs_space = single_CDA_env.get_observation_space(single_CDA_env.agents[0])\n",
    "act_space = single_CDA_env.get_action_space(single_CDA_env.agents[0])\n",
    "print(single_CDA_env.agents)  # Should be a non-empty list\n",
    "print(single_CDA_env.get_observation_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "print(single_CDA_env.get_action_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return continuousDoubleAuctionEnv(env_config)\n",
    "\n",
    "# Register environment with ray.tune - this is the key fix!\n",
    "tune.register_env(\"continuousDoubleAuction-v0\", env_creator)\n",
    "\n",
    "# register custom model (neural network)\n",
    "ModelCatalog.register_custom_model(\"model_disc\", CustomRLModule)\n",
    "\n",
    "ray.shutdown()\n",
    "# start ray\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    log_to_driver=True,\n",
    "    num_cpus=2,\n",
    "    dashboard_host=\"127.0.0.1\",  # replaces webui_host\n",
    "    dashboard_port=8265,          # default port; replaces webui_port\n",
    "    # include_dashboard=True,        # default True\n",
    "    include_dashboard=False,        # default True\n",
    "\n",
    ")\n",
    "\n",
    "# Global storage, a ray actor that run on it's own process & it needs to be declared after ray.init().\n",
    "g_store = storage.options(name=\"g_store\", lifetime=\"detached\").remote(num_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cknk9Cnoke_u"
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1756087282068,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "X_CVJpl4ngjw",
    "outputId": "e46188db-3568-44f0-cb04-79a9f7c342ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policies: {'policy_0': <ray.rllib.policy.policy.PolicySpec object at 0x739cdb63c520>, 'policy_1': <ray.rllib.policy.policy.PolicySpec object at 0x739cdb63c580>, 'policy_2': <ray.rllib.policy.policy.PolicySpec object at 0x739cdb63c5e0>, 'policy_3': <ray.rllib.policy.policy.PolicySpec object at 0x739cdb63c370>}\n",
      "policies_to_train: ['policy_0', 'policy_1']\n"
     ]
    }
   ],
   "source": [
    "policies, policies_to_train = create_multi_agent_config(\n",
    "    obs_space, act_space, num_agents, num_trained_agents=num_trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEnp5UpxkDve"
   },
   "source": [
    "### RLlib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback instance with champion configuration\n",
    "callback_instance = SelfPlayCallback(\n",
    "    num_trainable_policies=num_trained_agent, \n",
    "    num_random_policies= num_agents - num_trained_agent,\n",
    "    std_dev_multiplier=0.1,      # Snapshot when return > mean + 2*std\n",
    "    max_champions=2,             # Keep last 5 champions (rolling window)\n",
    "    min_iterations_between_champions=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1756087282137,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "AnniWlAwngjx"
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "\n",
    "def get_config():\n",
    "\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(\n",
    "            \"continuousDoubleAuction-v0\",\n",
    "            # continuousDoubleAuctionEnv,\n",
    "            # env_config={\n",
    "            #     \"num_of_agents\": num_of_traders,\n",
    "            #     \"init_cash\": init_cash,\n",
    "            #     \"tick_size\": tick_size,\n",
    "            #     \"tape_display_length\": tape_display_length,\n",
    "            #     \"max_step\": max_step - 1,\n",
    "            #     \"is_render\": is_render,\n",
    "            # }\n",
    "            env_config=env_config,\n",
    "            # env_config={\"disable_env_checker\": True},\n",
    "        )\n",
    "        .multi_agent(\n",
    "            policies=policies,\n",
    "            \n",
    "            # policy_mapping_fn=policy_mapping_fn,\n",
    "            policy_mapping_fn=SelfPlayCallback.get_mapping_fn(callback_instance),\n",
    "            \n",
    "            policies_to_train=policies_to_train,\n",
    "\n",
    "            count_steps_by = \"env_steps\"  # DEFAULT - but this changes everything!\n",
    "            # count_steps_by=\"agent_steps\",  # â† ADD THIS!\n",
    "        )\n",
    "        # .training(\n",
    "        #     model={\n",
    "        #         \"custom_model\": CustomLSTMRLModule,\n",
    "        #         # \"custom_model_config\": {\n",
    "        #         #     \"fcnet_hiddens\": [256, 256],  # Neural network architecture\n",
    "        #         #     \"fcnet_activation\": \"relu\",\n",
    "        #         # },\n",
    "        #     }\n",
    "        # )\n",
    "        .env_runners(\n",
    "            # num_env_runners=num_workers,\n",
    "\n",
    "            num_env_runners=0, \n",
    "            \n",
    "            # num_envs_per_env_runner=num_envs_per_worker,\n",
    "            # rollout_fragment_length=rollout_fragment_length,\n",
    "            # batch_mode=batch_mode,\n",
    "        )\n",
    "        .learners(\n",
    "            \n",
    "            # Local Learner running on the main process (driver/head node).\n",
    "            # Training runs on CPUs by default, or on a single GPU if num_gpus_per_learner > 0 is set. \n",
    "            # This is suitable for single-node training or simple, non-distributed setups.\n",
    "            num_learners=0,  # Typically 1 learner unless using distributed training\n",
    "\n",
    "            num_gpus_per_learner=num_gpus,  # Trainer GPU allocation\n",
    "            # num_cpus_per_learner=num_cpus_per_worker,\n",
    "        )\n",
    "        .training(\n",
    "            # train_batch_size_per_learner=train_batch_size / 4,\n",
    "            train_batch_size_per_learner=train_batch_size,\n",
    "            train_batch_size=train_batch_size,\n",
    "            num_epochs=4,\n",
    "        )\n",
    "        # .callbacks(SelfPlayCallback)\n",
    "        # .callbacks(lambda: SelfPlayCallback(win_rate_threshold=0.60))           \n",
    "        # .callbacks(lambda: MinimalLeagueCallback(\n",
    "        #     return_threshold=100.0,\n",
    "        #     check_every_n_iters=1,\n",
    "        # ))\n",
    "        \n",
    "        # .callbacks(lambda: SelfPlayCallback(\n",
    "        #     # win_rate_threshold=0.10,\n",
    "        #     ))\n",
    "        .callbacks(lambda: callback_instance)\n",
    "\n",
    "        # .output_dir(log_dir)\n",
    "        .framework(\"torch\")  # Explicitly set framework if needed\n",
    "        .debugging(log_level=\"DEBUG\")\n",
    "        # .api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)\n",
    "    )\n",
    "\n",
    "    # # Optional: Configure resources more granularly if needed\n",
    "    # if num_gpus_per_worker > 0:\n",
    "    #     config.env_runners(\n",
    "    #         num_gpus_per_env_runner=num_gpus_per_worker\n",
    "    #     )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKLNyViDkI9O"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163996,
     "status": "ok",
     "timestamp": 1756087446130,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "_Cq_T6fungjx",
    "outputId": "ed6c1255-2795-4496-ac2f-744d5fad9dfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 11:15:36,986\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-12-13 11:15:36,988\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEBUG: train_batch_size = 4096\n",
      "DEBUG: Expected episodes per iter = 4\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-12-13 11:15:37,225\tINFO connector_pipeline_v2.py:272 -- Added AddObservationsFromEpisodesToBatch to the end of EnvToModulePipeline.\n",
      "2025-12-13 11:15:37,235\tINFO connector_pipeline_v2.py:272 -- Added AddTimeDimToBatchAndZeroPad to the end of EnvToModulePipeline.\n",
      "2025-12-13 11:15:37,244\tINFO connector_pipeline_v2.py:272 -- Added AddStatesFromEpisodesToBatch to the end of EnvToModulePipeline.\n",
      "2025-12-13 11:15:37,262\tINFO connector_pipeline_v2.py:272 -- Added AgentToModuleMapping to the end of EnvToModulePipeline.\n",
      "2025-12-13 11:15:37,273\tINFO connector_pipeline_v2.py:272 -- Added BatchIndividualItems to the end of EnvToModulePipeline.\n",
      "2025-12-13 11:15:37,283\tINFO connector_pipeline_v2.py:272 -- Added NumpyToTensor to the end of EnvToModulePipeline.\n",
      "2025-12-13 11:15:37,286\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-12-13 11:15:37,300\tINFO connector_pipeline_v2.py:258 -- Added RemoveSingleTsTimeRankFromBatch to the beginning of ModuleToEnvPipeline.\n",
      "2025-12-13 11:15:37,300\tINFO connector_pipeline_v2.py:258 -- Added ModuleToAgentUnmapping to the beginning of ModuleToEnvPipeline.\n",
      "2025-12-13 11:15:37,301\tINFO connector_pipeline_v2.py:258 -- Added UnBatchToIndividualItems to the beginning of ModuleToEnvPipeline.\n",
      "2025-12-13 11:15:37,301\tINFO connector_pipeline_v2.py:258 -- Added TensorToNumpy to the beginning of ModuleToEnvPipeline.\n",
      "2025-12-13 11:15:37,302\tINFO connector_pipeline_v2.py:258 -- Added GetActions to the beginning of ModuleToEnvPipeline.\n",
      "2025-12-13 11:15:37,317\tINFO connector_pipeline_v2.py:272 -- Added NormalizeAndClipActions to the end of ModuleToEnvPipeline.\n",
      "2025-12-13 11:15:37,317\tINFO connector_pipeline_v2.py:272 -- Added ListifyDataForVectorEnv to the end of ModuleToEnvPipeline.\n",
      "2025-12-13 11:15:37,318\tINFO env_runner_group.py:320 -- Inferred observation/action spaces from remote worker (local worker has no env): {'__env__': (None, None), '__env_single__': (Dict('agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32), 'agent_2': Box(-inf, inf, (40,), float32), 'agent_3': Box(-inf, inf, (40,), float32)), Dict('agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_2': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_3': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)))), 'policy_0': (Box(-inf, inf, (40,), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))), 'policy_1': (Box(-inf, inf, (40,), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))), 'policy_2': (Box(-inf, inf, (40,), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))), 'policy_3': (Box(-inf, inf, (40,), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)))}\n",
      "2025-12-13 11:15:37,356\tINFO connector_pipeline_v2.py:272 -- Added AddObservationsFromEpisodesToBatch to the end of LearnerConnectorPipeline.\n",
      "2025-12-13 11:15:37,356\tINFO connector_pipeline_v2.py:272 -- Added AddColumnsFromEpisodesToTrainBatch to the end of LearnerConnectorPipeline.\n",
      "2025-12-13 11:15:37,366\tINFO connector_pipeline_v2.py:272 -- Added AddTimeDimToBatchAndZeroPad to the end of LearnerConnectorPipeline.\n",
      "2025-12-13 11:15:37,375\tINFO connector_pipeline_v2.py:272 -- Added AddStatesFromEpisodesToBatch to the end of LearnerConnectorPipeline.\n",
      "2025-12-13 11:15:37,386\tINFO connector_pipeline_v2.py:272 -- Added AgentToModuleMapping to the end of LearnerConnectorPipeline.\n",
      "2025-12-13 11:15:37,396\tINFO connector_pipeline_v2.py:272 -- Added BatchIndividualItems to the end of LearnerConnectorPipeline.\n",
      "2025-12-13 11:15:37,405\tINFO connector_pipeline_v2.py:272 -- Added NumpyToTensor to the end of LearnerConnectorPipeline.\n",
      "2025-12-13 11:15:38,577\tINFO connector_pipeline_v2.py:258 -- Added AddOneTsToEpisodesAndTruncate to the beginning of LearnerConnectorPipeline.\n",
      "2025-12-13 11:15:38,612\tINFO connector_pipeline_v2.py:272 -- Added GeneralAdvantageEstimation to the end of LearnerConnectorPipeline.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ACTUAL CONFIG train_batch_size: 4096\n",
      "ACTUAL CONFIG num_env_runners: 0\n",
      "ACTUAL CONFIG num_envs_per_env_runner: 1\n",
      "================================================================================\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "========================================\n",
      "Episode f5215f1da3864229b17c8bdcd64fe4f1 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -4578462360.273043, 'agent_1': -2026177341.3384748, 'agent_2': -11143991752.029222, 'agent_3': -19955278857.16171} id_=f5215f1da3864229b17c8bdcd64fe4f1)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "========================================\n",
      "Episode 4c9da549ba324fe4b869ccdd29ff2573 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -17223576570.204998, 'agent_1': -8885672871.11277, 'agent_2': -21975374680.922855, 'agent_3': -27322352618.33416} id_=4c9da549ba324fe4b869ccdd29ff2573)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "========================================\n",
      "Episode 900f4233ee814a30aab49b5f049c4337 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -12888866875.701845, 'agent_1': -7089119599.454851, 'agent_2': -6364287710.61319, 'agent_3': -15360771611.251534} id_=900f4233ee814a30aab49b5f049c4337)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "========================================\n",
      "Episode 58b91f501f504ae68e2280d8ed5c75bb Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 1 League Stats:\n",
      "Mean: -12901161070.70 | Std: 5318165657.58 | Threshold: -12369344504.94\n",
      "Policy Returns: {'policy_2': -13161218047.85509, 'policy_0': -11563635268.72663, 'policy_3': -20879467695.582466, 'policy_1': -6000323270.6353655}\n",
      "Best Trainable: policy_1 (-6000323270.64)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "ðŸ† CREATING CHAMPION SNAPSHOT ðŸ†\n",
      "Champion ID: champion_1\n",
      "Source Policy: policy_1\n",
      "Return: -6000323270.64\n",
      "Iteration: 1\n",
      "********************************************************************************\n",
      "\n",
      "âœ“ Champion champion_1 created successfully!\n",
      "âœ“ League size now: 3 (2 trainable + 1 champions)\n",
      "âœ“ Active champions: ['champion_1']\n",
      "\n",
      "on_episode_end:MAEps(len=4 done=True Rs={'agent_0': 0.0, 'agent_1': 0.0, 'agent_2': 0.0, 'agent_3': 0.0} id_=58b91f501f504ae68e2280d8ed5c75bb)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "========================================\n",
      "Episode 6c2c96bf8add4ad1882fb0f773704b5d Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_1\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -4458749607.26534, 'agent_1': -4861493555.968543, 'agent_2': -13378176265.732227, 'agent_3': -4517271927.839093} id_=6c2c96bf8add4ad1882fb0f773704b5d)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "========================================\n",
      "Episode 13e7b57241e44c41831f8927ae0bd921 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -13529999884.823715, 'agent_1': -2025692408.3082173, 'agent_2': -6850070910.818234, 'agent_3': -17397477836.267544} id_=13e7b57241e44c41831f8927ae0bd921)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "========================================\n",
      "Episode 57cf76cb1d8c4bbdb3afc90a7b91837d Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -12491818850.80719, 'agent_1': -2865463323.626625, 'agent_2': -6368768445.496297, 'agent_3': -10583792497.61556} id_=57cf76cb1d8c4bbdb3afc90a7b91837d)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "========================================\n",
      "Episode 2932fd7e66bf494896c153c6cd9de6b3 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 2 League Stats:\n",
      "Mean: -9887104092.67 | Std: 3779809270.83 | Threshold: -9509123165.59\n",
      "Policy Returns: {'policy_2': -10651973019.312576, 'policy_0': -10096372778.48491, 'policy_3': -14693641930.012285, 'policy_1': -4106428642.8795457}\n",
      "Best Trainable: policy_1 (-4106428642.88)\n",
      "================================================================================\n",
      "\n",
      "Skipping champion creation: only 1 iterations since last champion (min: 2)\n",
      "on_episode_end:MAEps(len=8 done=True Rs={'agent_0': -73866877.71523678, 'agent_1': -330060.8797286198, 'agent_2': -81186397.91595858, 'agent_3': -6488936.438746439} id_=2932fd7e66bf494896c153c6cd9de6b3)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "========================================\n",
      "Episode 7ee2217b89434c439f1e3f2a8f06bcec Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -22022442421.05589, 'agent_1': -5408896014.725496, 'agent_2': -2320845119.324675, 'agent_3': -17547942122.103844} id_=7ee2217b89434c439f1e3f2a8f06bcec)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "========================================\n",
      "Episode e491220b601e4e61870bb9e3efc44819 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_1\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -3248611218.3691974, 'agent_1': -13872963854.833977, 'agent_2': -8596249468.667845, 'agent_3': -10021702030.079027} id_=e491220b601e4e61870bb9e3efc44819)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "========================================\n",
      "Episode d75c8b00fb4a4583920b594d994c66d5 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -15167112942.362745, 'agent_1': -3189166973.382932, 'agent_2': -6173355088.259338, 'agent_3': -18142298658.735153} id_=d75c8b00fb4a4583920b594d994c66d5)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "========================================\n",
      "Episode 1ac5c7121a86454ea67cd9211f587573 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_1\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 3 League Stats:\n",
      "Mean: -9684472343.46 | Std: 3504522875.79 | Threshold: -9334020055.88\n",
      "Policy Returns: {'policy_2': -8887202967.878504, 'policy_0': -10584871728.809263, 'policy_3': -14515275971.617487, 'policy_1': -4750538705.539344}\n",
      "Best Trainable: policy_1 (-4750538705.54)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "ðŸ† CREATING CHAMPION SNAPSHOT ðŸ†\n",
      "Champion ID: champion_2\n",
      "Source Policy: policy_1\n",
      "Return: -4750538705.54\n",
      "Iteration: 3\n",
      "********************************************************************************\n",
      "\n",
      "âœ“ Champion champion_2 created successfully!\n",
      "âœ“ League size now: 4 (2 trainable + 2 champions)\n",
      "âœ“ Active champions: ['champion_1', 'champion_2']\n",
      "\n",
      "on_episode_end:MAEps(len=12 done=True Rs={'agent_0': -1397174400.0, 'agent_1': -59749888.0, 'agent_2': 8943.70802919708, 'agent_3': -179349368.0} id_=1ac5c7121a86454ea67cd9211f587573)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "========================================\n",
      "Episode 2ec058f918fe4503bc7c8f7984eab984 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_2\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -1503213727.4605472, 'agent_1': -2747718515.1030993, 'agent_2': -14201265249.75459, 'agent_3': -12859442971.011469} id_=2ec058f918fe4503bc7c8f7984eab984)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "========================================\n",
      "Episode 521e43a81c74493e818c854677137fdc Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_1\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -16772894584.001352, 'agent_1': -6648384704.758266, 'agent_2': -16768631703.002289, 'agent_3': -7454938699.946864} id_=521e43a81c74493e818c854677137fdc)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "========================================\n",
      "Episode 6409f5f2446d4edfbb49dc18cd3f2c41 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_1\n",
      "  agent_3 -> champion_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -4416499535.474456, 'agent_1': -2844675464.0833535, 'agent_2': -9937652779.05056, 'agent_3': -3838051428.4231625} id_=6409f5f2446d4edfbb49dc18cd3f2c41)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "========================================\n",
      "Episode d7c427ef5a5e4b52ad6cb8bf0de52c89 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_1\n",
      "  agent_3 -> champion_2\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 4 League Stats:\n",
      "Mean: -9364756553.04 | Std: 3015885033.24 | Threshold: -9063168049.72\n",
      "Policy Returns: {'policy_2': -10251444616.583094, 'policy_0': -10349577930.964392, 'policy_3': -12483313578.571507, 'policy_1': -4374690086.046994}\n",
      "Best Trainable: policy_1 (-4374690086.05)\n",
      "================================================================================\n",
      "\n",
      "Skipping champion creation: only 1 iterations since last champion (min: 2)\n",
      "on_episode_end:MAEps(len=16 done=True Rs={'agent_0': -705374692.3924918, 'agent_1': -121805905.305479, 'agent_2': -464498436.71966225, 'agent_3': -61887644.712283} id_=d7c427ef5a5e4b52ad6cb8bf0de52c89)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "========================================\n",
      "Episode 9f6db6f17e3148af9f09f6c91c4d4fd3 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_1\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -9867026150.062199, 'agent_1': -1839810730.4829297, 'agent_2': -17665427968.132298, 'agent_3': -8678335600.969576} id_=9f6db6f17e3148af9f09f6c91c4d4fd3)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "========================================\n",
      "Episode 05cb3de2e1f049c5b839d2203ef8238a Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -16258740519.552893, 'agent_1': -9145285392.471012, 'agent_2': -11532318954.914043, 'agent_3': -19081515249.75463} id_=05cb3de2e1f049c5b839d2203ef8238a)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "========================================\n",
      "Episode aba86aba744e456188f504bcabddbcc4 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_2\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -14948937700.318327, 'agent_1': -2687996954.9158125, 'agent_2': -9839627690.013735, 'agent_3': -3723349890.222518} id_=aba86aba744e456188f504bcabddbcc4)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "========================================\n",
      "Episode 03535fc6930847288fb833cf94a8a37c Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_1\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_1', 'champion_2']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 5 League Stats:\n",
      "Mean: -9756678459.68 | Std: 3072335075.74 | Threshold: -9449444952.10\n",
      "Policy Returns: {'policy_2': -11176109540.11454, 'policy_0': -11772747600.65779, 'policy_3': -11628936775.937386, 'policy_1': -4448919921.995704}\n",
      "Best Trainable: policy_1 (-4448919922.00)\n",
      "================================================================================\n",
      "\n",
      "Max champions (2) reached, will remove oldest\n",
      "\n",
      "âš ï¸  Removing oldest champion: champion_1 (from iteration 1, return=-6000323270.64)\n",
      "âœ“ Champion removed. Active champions: ['champion_2']\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "ðŸ† CREATING CHAMPION SNAPSHOT ðŸ†\n",
      "Champion ID: champion_3\n",
      "Source Policy: policy_1\n",
      "Return: -4448919922.00\n",
      "Iteration: 5\n",
      "********************************************************************************\n",
      "\n",
      "âœ“ Champion champion_3 created successfully!\n",
      "âœ“ League size now: 4 (2 trainable + 2 champions)\n",
      "âœ“ Active champions: ['champion_2', 'champion_3']\n",
      "\n",
      "on_episode_end:MAEps(len=20 done=True Rs={'agent_0': -341276890.77686346, 'agent_1': -15621871.73738407, 'agent_2': -447884184.97835296, 'agent_3': -214856860.17251462} id_=03535fc6930847288fb833cf94a8a37c)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "========================================\n",
      "Episode 9bc87f98c7314b0b9593213878b9ae9c Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_3\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -9440440551.896484, 'agent_1': -2211731944.5510316, 'agent_2': -2100251513.626825, 'agent_3': -4792472034.959653} id_=9bc87f98c7314b0b9593213878b9ae9c)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "========================================\n",
      "Episode d4c5d602d6404642bb36c0129153bec0 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_3\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -12050444357.86582, 'agent_1': -12284701717.980867, 'agent_2': -2168651429.1441445, 'agent_3': -1689208423.3177745} id_=d4c5d602d6404642bb36c0129153bec0)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "========================================\n",
      "Episode 1a8b14e00e274531bda1031e74e9d135 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_3\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -3876138797.023352, 'agent_1': -787793318.2326038, 'agent_2': -2606886635.5181255, 'agent_3': -5379793443.965162} id_=1a8b14e00e274531bda1031e74e9d135)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "========================================\n",
      "Episode 1cd9606ddfcc4943ae3351d799f0185c Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_2\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 6 League Stats:\n",
      "Mean: -9023844507.08 | Std: 2702215340.86 | Threshold: -8753622973.00\n",
      "Policy Returns: {'policy_2': -10098548393.27133, 'policy_0': -11318164633.654814, 'policy_3': -10264551835.04685, 'policy_1': -4414113166.366234}\n",
      "Best Trainable: policy_1 (-4414113166.37)\n",
      "================================================================================\n",
      "\n",
      "Skipping champion creation: only 1 iterations since last champion (min: 2)\n",
      "on_episode_end:MAEps(len=24 done=True Rs={'agent_0': -84599261.135833, 'agent_1': -86365354.84522843, 'agent_2': -368883347.0390345, 'agent_3': -29269580.207771793} id_=1cd9606ddfcc4943ae3351d799f0185c)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "========================================\n",
      "Episode 1d61b71122204ef696ecdb428f6012e1 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_3\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -3377116059.3520484, 'agent_1': -13366715469.499191, 'agent_2': -5685287983.435388, 'agent_3': -9448710585.34835} id_=1d61b71122204ef696ecdb428f6012e1)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "========================================\n",
      "Episode 8d2e6ca20ca84843930b97f80b7ff630 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_3\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -2859531327.860128, 'agent_1': -1029430353.2770494, 'agent_2': -3149941472.8175216, 'agent_3': -6287189156.754756} id_=8d2e6ca20ca84843930b97f80b7ff630)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "========================================\n",
      "Episode 845fb36116104ab998eb6855e4e73e78 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_3\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -3931675202.048002, 'agent_1': -1128330249.2660968, 'agent_2': -810925508.6954119, 'agent_3': -4465102269.314265} id_=845fb36116104ab998eb6855e4e73e78)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "========================================\n",
      "Episode 4088b8409df746898fe72f2e51d33d37 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_2', 'champion_3']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 7 League Stats:\n",
      "Mean: -8379683410.25 | Std: 2277549432.23 | Threshold: -8151928467.03\n",
      "Policy Returns: {'policy_2': -9323198073.601421, 'policy_0': -10091880450.256466, 'policy_3': -9640329511.025997, 'policy_1': -4463325606.132942}\n",
      "Best Trainable: policy_1 (-4463325606.13)\n",
      "================================================================================\n",
      "\n",
      "Max champions (2) reached, will remove oldest\n",
      "\n",
      "âš ï¸  Removing oldest champion: champion_2 (from iteration 3, return=-4750538705.54)\n",
      "âœ“ Champion removed. Active champions: ['champion_3']\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "ðŸ† CREATING CHAMPION SNAPSHOT ðŸ†\n",
      "Champion ID: champion_4\n",
      "Source Policy: policy_1\n",
      "Return: -4463325606.13\n",
      "Iteration: 7\n",
      "********************************************************************************\n",
      "\n",
      "âœ“ Champion champion_4 created successfully!\n",
      "âœ“ League size now: 4 (2 trainable + 2 champions)\n",
      "âœ“ Active champions: ['champion_3', 'champion_4']\n",
      "\n",
      "on_episode_end:MAEps(len=28 done=True Rs={'agent_0': -733443214.1987652, 'agent_1': -174347102.18039498, 'agent_2': -16103895.276728326, 'agent_3': -875957024.917932} id_=4088b8409df746898fe72f2e51d33d37)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "========================================\n",
      "Episode fcffd3ee023c472e959cdffccd65f432 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -6986416215.97488, 'agent_1': -7160903859.702178, 'agent_2': -6815368878.87636, 'agent_3': -6738224271.000582} id_=fcffd3ee023c472e959cdffccd65f432)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "========================================\n",
      "Episode 533ded55b8024fd7ae0d7b5f6b31fff0 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_3\n",
      "  agent_3 -> champion_4\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -8852227731.561525, 'agent_1': -7667389346.547079, 'agent_2': -4846995709.358439, 'agent_3': -5302773717.973799} id_=533ded55b8024fd7ae0d7b5f6b31fff0)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "========================================\n",
      "Episode 1307843e3e8444fd9145d72fbdb8f9b2 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_4\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -3299139241.7252235, 'agent_1': -8166413154.24136, 'agent_2': -1844853050.6811945, 'agent_3': -4539354011.013253} id_=1307843e3e8444fd9145d72fbdb8f9b2)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "========================================\n",
      "Episode 99bc0e73c98a4dfab81022140764d233 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 8 League Stats:\n",
      "Mean: -8151328697.84 | Std: 2059678955.83 | Threshold: -7945360802.25\n",
      "Policy Returns: {'policy_2': -8645916285.400534, 'policy_0': -9859494113.269642, 'policy_3': -9435387506.725443, 'policy_1': -4664516885.954534}\n",
      "Best Trainable: policy_1 (-4664516885.95)\n",
      "================================================================================\n",
      "\n",
      "Skipping champion creation: only 1 iterations since last champion (min: 2)\n",
      "on_episode_end:MAEps(len=32 done=True Rs={'agent_0': -494931441.8202212, 'agent_1': -58608711.29237514, 'agent_2': -202993697.84033215, 'agent_3': -874305377.2097064} id_=99bc0e73c98a4dfab81022140764d233)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "========================================\n",
      "Episode 5d0fe11f42354200bcbb5582466ae1c3 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -10833116142.560745, 'agent_1': -8212061236.0740595, 'agent_2': -27985936512.487286, 'agent_3': -7824384530.474829} id_=5d0fe11f42354200bcbb5582466ae1c3)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "========================================\n",
      "Episode da129d73a2184cb1aa99d6c2f755f339 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -28229111594.795918, 'agent_1': -6805907374.1354265, 'agent_2': -14882202286.723383, 'agent_3': -4057752787.6226115} id_=da129d73a2184cb1aa99d6c2f755f339)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "========================================\n",
      "Episode 1be2991a77a140bb919973db3befc13c Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -5117431135.557665, 'agent_1': -5258791795.26239, 'agent_2': -14711613560.72906, 'agent_3': -11531667992.18197} id_=1be2991a77a140bb919973db3befc13c)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "========================================\n",
      "Episode 588979196f9a4cec939f1ad733a9bdd0 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_4\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_3', 'champion_4']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 9 League Stats:\n",
      "Mean: -8610686615.38 | Std: 2178404241.22 | Threshold: -8392846191.25\n",
      "Policy Returns: {'policy_2': -9417897850.3148, 'policy_0': -10337913251.400497, 'policy_3': -9806716617.958994, 'policy_1': -4880218741.826958}\n",
      "Best Trainable: policy_1 (-4880218741.83)\n",
      "================================================================================\n",
      "\n",
      "Max champions (2) reached, will remove oldest\n",
      "\n",
      "âš ï¸  Removing oldest champion: champion_3 (from iteration 5, return=-4448919922.00)\n",
      "âœ“ Champion removed. Active champions: ['champion_4']\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "ðŸ† CREATING CHAMPION SNAPSHOT ðŸ†\n",
      "Champion ID: champion_5\n",
      "Source Policy: policy_1\n",
      "Return: -4880218741.83\n",
      "Iteration: 9\n",
      "********************************************************************************\n",
      "\n",
      "âœ“ Champion champion_5 created successfully!\n",
      "âœ“ League size now: 4 (2 trainable + 2 champions)\n",
      "âœ“ Active champions: ['champion_4', 'champion_5']\n",
      "\n",
      "on_episode_end:MAEps(len=36 done=True Rs={'agent_0': -76591872.05665429, 'agent_1': -31221196.123011246, 'agent_2': -687685222.9388562, 'agent_3': -194342817.28104252} id_=588979196f9a4cec939f1ad733a9bdd0)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode 3916dff0cf384e8896ddebd59c03a1a3 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_4\n",
      "  agent_3 -> champion_5\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -2578924209.2312455, 'agent_1': -973259823.8225093, 'agent_2': -2259574004.570418, 'agent_3': -2929213252.9060965} id_=3916dff0cf384e8896ddebd59c03a1a3)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode 775ac1bf81634e479c156cc44a9d239a Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_4\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -6519632915.8900385, 'agent_1': -4361244684.5294485, 'agent_2': -3276458119.764494, 'agent_3': -9095796470.10637} id_=775ac1bf81634e479c156cc44a9d239a)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode a15672292c95406e87d53a80bae6830a Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_4\n",
      "  agent_3 -> champion_5\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -3877314971.2875605, 'agent_1': -917987768.7436754, 'agent_2': -1835673313.4967146, 'agent_3': -7173200700.170995} id_=a15672292c95406e87d53a80bae6830a)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode b6515579a880479ca5e902747f35c119 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_5\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=662 done=True Rs={'agent_0': -3314258258.88989, 'agent_1': -1589378858.5586271, 'agent_2': -10120701329.99129, 'agent_3': -4596022749.75449} id_=b6515579a880479ca5e902747f35c119)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode bf8ac1f542cf452091c1f5bc26e45fad Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_4\n",
      "  agent_3 -> champion_5\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 10 League Stats:\n",
      "Mean: -8051584880.38 | Std: 2072467138.87 | Threshold: -7844338166.49\n",
      "Policy Returns: {'policy_2': -8810174011.472225, 'policy_0': -9526271070.855244, 'policy_3': -9377953974.136784, 'policy_1': -4491940465.036952}\n",
      "Best Trainable: policy_1 (-4491940465.04)\n",
      "================================================================================\n",
      "\n",
      "Skipping champion creation: only 1 iterations since last champion (min: 2)\n",
      "on_episode_end:MAEps(len=702 done=True Rs={'agent_0': -5455762180.1618185, 'agent_1': -783496659.0359725, 'agent_2': -2471290298.9843864, 'agent_3': -1667072837.602836} id_=bf8ac1f542cf452091c1f5bc26e45fad)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode 675c227d53d849fc8e435e67598e2ebf Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -5292456882.952694, 'agent_1': -5726541747.343465, 'agent_2': -7616986119.0152645, 'agent_3': -9228362329.59338} id_=675c227d53d849fc8e435e67598e2ebf)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode 5b64655078de470d9a1c7adaf695333c Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_4\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -7032519533.390818, 'agent_1': -3389662029.129509, 'agent_2': -7228248063.481755, 'agent_3': -3817139989.0828032} id_=5b64655078de470d9a1c7adaf695333c)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode a7631f82e096454797b93c8c73734484 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_4\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -4805325709.422151, 'agent_1': -4590313167.411036, 'agent_2': -8075995886.877802, 'agent_3': -2270381330.356031} id_=a7631f82e096454797b93c8c73734484)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "========================================\n",
      "Episode 6b636722236e42e8bfba887e1c5bed5a Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_4\n",
      "  agent_3 -> champion_5\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_4', 'champion_5']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 11 League Stats:\n",
      "Mean: -7773890105.14 | Std: 1950756360.73 | Threshold: -7578814469.07\n",
      "Policy Returns: {'policy_2': -8588119787.313455, 'policy_0': -9176590025.41654, 'policy_3': -8916411314.856997, 'policy_1': -4414439292.972957}\n",
      "Best Trainable: policy_1 (-4414439292.97)\n",
      "================================================================================\n",
      "\n",
      "Max champions (2) reached, will remove oldest\n",
      "\n",
      "âš ï¸  Removing oldest champion: champion_4 (from iteration 7, return=-4463325606.13)\n",
      "âœ“ Champion removed. Active champions: ['champion_5']\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "ðŸ† CREATING CHAMPION SNAPSHOT ðŸ†\n",
      "Champion ID: champion_6\n",
      "Source Policy: policy_1\n",
      "Return: -4414439292.97\n",
      "Iteration: 11\n",
      "********************************************************************************\n",
      "\n",
      "âœ“ Champion champion_6 created successfully!\n",
      "âœ“ League size now: 4 (2 trainable + 2 champions)\n",
      "âœ“ Active champions: ['champion_5', 'champion_6']\n",
      "\n",
      "on_episode_end:MAEps(len=706 done=True Rs={'agent_0': -1738979415.3551505, 'agent_1': -1362110412.206199, 'agent_2': -1040174941.1362394, 'agent_3': -619255141.2613583} id_=6b636722236e42e8bfba887e1c5bed5a)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "========================================\n",
      "Episode 86320a5179fb4e3385cecee33bb48690 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -3749494014.3471637, 'agent_1': -1833653951.545301, 'agent_2': -7532661472.815259, 'agent_3': -12468351358.072105} id_=86320a5179fb4e3385cecee33bb48690)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "========================================\n",
      "Episode fe364e3275c441ff8e3966d40827feb4 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_5\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -6194750919.901007, 'agent_1': -3176587735.2442, 'agent_2': -8668003874.159544, 'agent_3': -4763454701.683817} id_=fe364e3275c441ff8e3966d40827feb4)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "========================================\n",
      "Episode 150fd3fc86da4a4ab1ba6b755d90186a Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -17995426421.8993, 'agent_1': -2525274471.7151794, 'agent_2': -5488683514.823218, 'agent_3': -16399061010.21303} id_=150fd3fc86da4a4ab1ba6b755d90186a)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "========================================\n",
      "Episode 4b6511f7a0224bb7992d9fc2df555118 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_5\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 12 League Stats:\n",
      "Mean: -7626150799.29 | Std: 1975621539.92 | Threshold: -7428588645.30\n",
      "Policy Returns: {'policy_2': -8347520912.217697, 'policy_0': -9033898825.534666, 'policy_3': -8890069827.672415, 'policy_1': -4233113631.7270637}\n",
      "Best Trainable: policy_1 (-4233113631.73)\n",
      "================================================================================\n",
      "\n",
      "Skipping champion creation: only 1 iterations since last champion (min: 2)\n",
      "on_episode_end:MAEps(len=710 done=True Rs={'agent_0': -1937188323.0571718, 'agent_1': -3779638722.837615, 'agent_2': -11600148372.17471, 'agent_3': -3370216783.908351} id_=4b6511f7a0224bb7992d9fc2df555118)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "========================================\n",
      "Episode 11f5018c995b459b8763c75eff85bee8 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_5\n",
      "  agent_3 -> champion_6\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -5725715492.338925, 'agent_1': -1724237583.6711972, 'agent_2': -3426254839.5971723, 'agent_3': -835871339.5199896} id_=11f5018c995b459b8763c75eff85bee8)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "========================================\n",
      "Episode 631d4392c8fe48b6ad0178ae4c09ba87 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_6\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -2775414836.997695, 'agent_1': -4632840964.941298, 'agent_2': -4175491338.5247865, 'agent_3': -4395644143.996312} id_=631d4392c8fe48b6ad0178ae4c09ba87)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "========================================\n",
      "Episode a4088361fd744c5fb8856af1d807875d Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -11026173441.474693, 'agent_1': -2065227589.9614356, 'agent_2': -7362197322.354754, 'agent_3': -22121794851.380615} id_=a4088361fd744c5fb8856af1d807875d)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "========================================\n",
      "Episode 2b64956f9cdb45e9a9068eb30e11a35d Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_5\n",
      "  agent_3 -> champion_6\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_5', 'champion_6']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 13 League Stats:\n",
      "Mean: -7487517131.73 | Std: 1935595837.96 | Threshold: -7293957547.93\n",
      "Policy Returns: {'policy_2': -8233424331.619435, 'policy_0': -8754623513.800594, 'policy_3': -8804593459.20647, 'policy_1': -4157427222.281743}\n",
      "Best Trainable: policy_1 (-4157427222.28)\n",
      "================================================================================\n",
      "\n",
      "Max champions (2) reached, will remove oldest\n",
      "\n",
      "âš ï¸  Removing oldest champion: champion_5 (from iteration 9, return=-4880218741.83)\n",
      "âœ“ Champion removed. Active champions: ['champion_6']\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "ðŸ† CREATING CHAMPION SNAPSHOT ðŸ†\n",
      "Champion ID: champion_7\n",
      "Source Policy: policy_1\n",
      "Return: -4157427222.28\n",
      "Iteration: 13\n",
      "********************************************************************************\n",
      "\n",
      "âœ“ Champion champion_7 created successfully!\n",
      "âœ“ League size now: 4 (2 trainable + 2 champions)\n",
      "âœ“ Active champions: ['champion_6', 'champion_7']\n",
      "\n",
      "on_episode_end:MAEps(len=714 done=True Rs={'agent_0': -1926079580.5695755, 'agent_1': -1525851525.9200492, 'agent_2': -1671986902.4701047, 'agent_3': -1680726409.9706135} id_=2b64956f9cdb45e9a9068eb30e11a35d)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "========================================\n",
      "Episode 40e4982acb9642c7b0b44b0eebcdb5cb Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -4799752272.132416, 'agent_1': -2012817932.4241538, 'agent_2': -8353211274.305655, 'agent_3': -10933060401.907726} id_=40e4982acb9642c7b0b44b0eebcdb5cb)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "========================================\n",
      "Episode 9e887682e0aa42e9b3084ad61356ae32 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_6\n",
      "  agent_3 -> champion_7\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -2388298619.5410933, 'agent_1': -6500766853.172201, 'agent_2': -2066283729.930329, 'agent_3': -6759557313.037237} id_=9e887682e0aa42e9b3084ad61356ae32)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "========================================\n",
      "Episode 000184be42ea4a98b83f8d23b4057b8f Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_2\n",
      "  agent_3 -> policy_3\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -4179584335.7041907, 'agent_1': -3652556992.1130433, 'agent_2': -9216290360.10433, 'agent_3': -6882914839.86668} id_=000184be42ea4a98b83f8d23b4057b8f)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "========================================\n",
      "Episode db6d9980c8e6434ab9f71d5dca1acf7c Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_6\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 14 League Stats:\n",
      "Mean: -7286367862.72 | Std: 1849422359.02 | Threshold: -7101425626.82\n",
      "Policy Returns: {'policy_2': -8026277446.318139, 'policy_0': -8367608283.914382, 'policy_3': -8645882234.984198, 'policy_1': -4105703485.6711197}\n",
      "Best Trainable: policy_1 (-4105703485.67)\n",
      "================================================================================\n",
      "\n",
      "Skipping champion creation: only 1 iterations since last champion (min: 2)\n",
      "on_episode_end:MAEps(len=718 done=True Rs={'agent_0': -8138877000.056708, 'agent_1': -2508554728.8621316, 'agent_2': -13772651894.687664, 'agent_3': -5096286675.812657} id_=db6d9980c8e6434ab9f71d5dca1acf7c)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "========================================\n",
      "Episode d7ceb46a0d774a73aaad436eb2baad4c Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_6\n",
      "  agent_3 -> champion_7\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -2190179259.1394024, 'agent_1': -1647748091.214463, 'agent_2': -2869836336.6926947, 'agent_3': -3993935681.078483} id_=d7ceb46a0d774a73aaad436eb2baad4c)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "========================================\n",
      "Episode 269abeccaf034e7bad9dd093ee33398a Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_6\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -1726257648.1825006, 'agent_1': -7512455721.027372, 'agent_2': -12644271774.45026, 'agent_3': -2475308287.847787} id_=269abeccaf034e7bad9dd093ee33398a)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "========================================\n",
      "Episode 7d1ee834d05a4c1e895f9d4f8794cdbe Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_6\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -22502609829.987408, 'agent_1': -4410247904.703206, 'agent_2': -15397782053.124243, 'agent_3': -10044975925.940207} id_=7d1ee834d05a4c1e895f9d4f8794cdbe)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "========================================\n",
      "Episode 726c02bb21cf4082a523d706bf3c246f Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_6\n",
      "  agent_3 -> champion_7\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_6', 'champion_7']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 15 League Stats:\n",
      "Mean: -7291648878.65 | Std: 1841452360.21 | Threshold: -7107503642.62\n",
      "Policy Returns: {'policy_2': -8241102875.627912, 'policy_0': -8389341439.474958, 'policy_3': -8431636416.391114, 'policy_1': -4104514783.0896444}\n",
      "Best Trainable: policy_1 (-4104514783.09)\n",
      "================================================================================\n",
      "\n",
      "Max champions (2) reached, will remove oldest\n",
      "\n",
      "âš ï¸  Removing oldest champion: champion_6 (from iteration 11, return=-4414439292.97)\n",
      "âœ“ Champion removed. Active champions: ['champion_7']\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "ðŸ† CREATING CHAMPION SNAPSHOT ðŸ†\n",
      "Champion ID: champion_8\n",
      "Source Policy: policy_1\n",
      "Return: -4104514783.09\n",
      "Iteration: 15\n",
      "********************************************************************************\n",
      "\n",
      "âœ“ Champion champion_8 created successfully!\n",
      "âœ“ League size now: 4 (2 trainable + 2 champions)\n",
      "âœ“ Active champions: ['champion_7', 'champion_8']\n",
      "\n",
      "on_episode_end:MAEps(len=722 done=True Rs={'agent_0': -5147621861.545534, 'agent_1': -6450955622.347209, 'agent_2': -1727743893.9729111, 'agent_3': -3832443228.526074} id_=726c02bb21cf4082a523d706bf3c246f)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "========================================\n",
      "Episode a2baad957e934ae583fd6c5ecffd05e1 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_8\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -4651268312.069451, 'agent_1': -5032944824.822384, 'agent_2': -11904358999.197498, 'agent_3': -9245597621.985035} id_=a2baad957e934ae583fd6c5ecffd05e1)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "========================================\n",
      "Episode 81de564879424676a08a1bdb14a090ee Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_7\n",
      "  agent_3 -> champion_8\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -2214450417.485029, 'agent_1': -1308010408.7318237, 'agent_2': -7007353516.112201, 'agent_3': -10518039768.15548} id_=81de564879424676a08a1bdb14a090ee)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "========================================\n",
      "Episode 0166e586863a4392a39d91ea46cf3e68 Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> policy_3\n",
      "  agent_3 -> champion_7\n",
      "========================================\n",
      "\n",
      "on_episode_end:MAEps(len=1025 done=True Rs={'agent_0': -6573848424.072945, 'agent_1': -3581429206.7304254, 'agent_2': -7716439823.532953, 'agent_3': -2994525665.965258} id_=0166e586863a4392a39d91ea46cf3e68)\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "========================================\n",
      "Episode c2d5fe4f8e4c40b3bb31c216bd2cd8dc Started - Policy Map:\n",
      "  agent_0 -> policy_0\n",
      "  agent_1 -> policy_1\n",
      "  agent_2 -> champion_8\n",
      "  agent_3 -> policy_2\n",
      "========================================\n",
      "\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "\n",
      "Candidates: ['policy_2', 'policy_3', 'champion_7', 'champion_8']\n",
      "Warning: policy_reward_mean not found, falling back to agent returns\n",
      "\n",
      "================================================================================\n",
      "Iteration 16 League Stats:\n",
      "Mean: -7190835768.93 | Std: 1779781417.57 | Threshold: -7012857627.17\n",
      "Policy Returns: {'policy_2': -8169991112.939548, 'policy_0': -8162458589.81531, 'policy_3': -8320787960.37237, 'policy_1': -4110105412.5811796}\n",
      "Best Trainable: policy_1 (-4110105412.58)\n",
      "================================================================================\n",
      "\n",
      "Skipping champion creation: only 1 iterations since last champion (min: 2)\n"
     ]
    }
   ],
   "source": [
    "def go_train(config):\n",
    "    # trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")\n",
    "\n",
    "    # In your notebook, add this right before config.build():\n",
    "    print(\"=\" * 80)  \n",
    "    print(f\"DEBUG: train_batch_size = {train_batch_size}\")\n",
    "    print(f\"DEBUG: Expected episodes per iter = {num_episodes_per_iter}\")\n",
    "    # print(f\"DEBUG: Agent timesteps per episode = {agent_time_step_per_episode}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    algo = config.build()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ACTUAL CONFIG train_batch_size: {algo.config.train_batch_size}\")\n",
    "    print(f\"ACTUAL CONFIG num_env_runners: {algo.config.num_env_runners}\")\n",
    "    print(f\"ACTUAL CONFIG num_envs_per_env_runner: {algo.config.num_envs_per_env_runner}\")  # â† KEY!\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # if is_restore == True:\n",
    "    #     trainer.restore(restore_path)\n",
    "\n",
    "    # g_store = ray.util.get_actor(\"g_store\")\n",
    "    # result = None\n",
    "    for i in range(num_iters):\n",
    "        result = algo.train()\n",
    "\n",
    "    #     print(pretty_print(result)) # includes result[\"custom_metrics\"]\n",
    "    #     print(\"training loop = {} of {}\".format(i + 1, num_iters))\n",
    "    #     print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n",
    "\n",
    "    #     if i % chkpt_freq == 0:\n",
    "    #         checkpoint = algo.save(local_dir)\n",
    "    #         print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "    # checkpoint = algo.save(local_dir)\n",
    "    # print(\"checkpoint saved at\", checkpoint)\n",
    "    # print(\"result['experiment_id']\", result[\"experiment_id\"])\n",
    "\n",
    "                # Print step counts\n",
    "        env_runner_results = result.get('env_runners', {})\n",
    "        \n",
    "        # print(f\"\\n=== Iteration {i+1} ===\")\n",
    "        # print(f\"num_env_steps_sampled: {env_runner_results.get('num_env_steps_sampled', 'N/A')}\")\n",
    "        # print(f\"num_agent_steps_sampled: {env_runner_results.get('num_agent_steps_sampled', 'N/A')}\")\n",
    "        # print(f\"num_env_steps_trained: {env_runner_results.get('num_env_steps_trained', 'N/A')}\")\n",
    "        # print(f\"num_agent_steps_trained: {env_runner_results.get('num_agent_steps_trained', 'N/A')}\")\n",
    "\n",
    "    # return result[\"experiment_id\"]\n",
    "    return None\n",
    "\n",
    "# run everything\n",
    "experiment_id = go_train(get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756087446179,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "BMikbPugngj9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1756087446266,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "MrcLYiHrngj9",
    "outputId": "9a2fee4b-538b-4286-ad42-c2fb9af8f535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-13 11:20:29\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now()\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(formatted_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087446269,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "sF-9OyQangj-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
