{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f05ZH97QkoJf"
   },
   "source": [
    "# Sample training script with naive competitive self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcLSdJuUkTrX"
   },
   "source": [
    "### Switch directory in Google drive so as to import CDA env.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "PAqVG2cqjLXM",
    "outputId": "2cd035eb-6b74-4432-b82e-76b9162f439a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# %cd \"/root/ray_results/\"\n",
    "# !ls -l\n",
    "# #!rm -rf PPO_continuousDoubleAuction-v0_*\n",
    "# !ls -l\n",
    "# !pwd\n",
    "\n",
    "# %cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
    "# !ls -l\n",
    "\n",
    "# #!pip install -r requirements.txt\n",
    "\n",
    "# #!pip install tensorflow==2.2.0\n",
    "# #!pip install ray[rllib]==0.8.5\n",
    "\n",
    "# #!pip show tensorflow\n",
    "# #!pip show ray\n",
    "\n",
    "# #!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sortedcontainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall continuousDoubleAuction\n",
    "# !pip uninstall continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show continuousDoubleAuction\n",
    "# !pip show continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('gym_continuousDoubleAuction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7ZHcwBWkXVM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UW3INjDipTC"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "\n",
    "# from ray.rllib.agents.ppo import ppo\n",
    "# from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "\n",
    "\n",
    "# from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "# from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "\n",
    "\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
    "\n",
    "\n",
    "\n",
    "# from gym_continuousDoubleAuction.train.model.model_handler import CustomModel_1\n",
    "from gym_continuousDoubleAuction.train.model.model_handler import CustomLSTMRLModule\n",
    "\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.train.policy.policy_handler import make_RandomPolicy, gen_policy, set_agents_policies, create_train_policy_list\n",
    "from gym_continuousDoubleAuction.train.weight.weight_handler import get_trained_policies_name, get_max_reward_ind, cp_weight\n",
    "from gym_continuousDoubleAuction.train.storage.store_handler import storage\n",
    "from gym_continuousDoubleAuction.train.callbk.callbk_handler import store_eps_hist_data\n",
    "from gym_continuousDoubleAuction.train.logger.log_handler import create_dir, log_g_store, load_g_store\n",
    "from gym_continuousDoubleAuction.train.plotter.plot_handler import plot_storage, plot_LOB_subplot, plot_sum_ord_imb, plot_mid_prices\n",
    "from gym_continuousDoubleAuction.train.helper.helper import ord_imb, sum_ord_imb, mid_price\n",
    "\n",
    "# tf = try_import_tf()\n",
    "print(f'Imports all OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDnpi8k5kbYo"
   },
   "source": [
    "### Global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "UqzjVWUsPykm",
    "outputId": "18a6a936-1477-4e90-98c6-cdf7be851dc4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CDA_env args\n",
    "num_agents = 9\n",
    "num_trained_agent = 3 # \n",
    "num_policies = num_agents # Each agent is using a separate policy\n",
    "num_of_traders = num_agents\n",
    "tape_display_length = 10 \n",
    "tick_size = 1\n",
    "init_cash = 1000000\n",
    "max_step = 4096 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n",
    "is_render = False \n",
    "\n",
    "# RLlib config \n",
    "train_policy_list = create_train_policy_list(num_trained_agent, \"policy_\")\n",
    "#num_cpus = 0.25                                \n",
    "num_gpus = 0.75 #0                       \n",
    "num_cpus_per_worker = 0.25                                \n",
    "num_gpus_per_worker = 0\n",
    "num_workers = 2\n",
    "num_envs_per_worker = 4\n",
    "batch_mode = \"complete_episodes\" \n",
    "rollout_fragment_length = 128\n",
    "train_batch_size = max_step\n",
    "sgd_minibatch_size = 256\n",
    "num_iters = 2\n",
    "\n",
    "# log_base_dir = \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/results/\"\n",
    "log_base_dir = \"results/\"\n",
    "log_dir = log_base_dir + \"ray_results/\"\n",
    "\n",
    "# Chkpt & restore\n",
    "local_dir = log_base_dir + \"chkpt/\"\n",
    "chkpt_freq = 10\n",
    "chkpt = 320\n",
    "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
    "is_restore = True # True / False\n",
    "\n",
    "# log & load \n",
    "log_g_store_dir = log_base_dir + \"log_g_store/\"\n",
    "create_dir(log_base_dir)    \n",
    "create_dir(log_g_store_dir)    \n",
    "\n",
    "# get obs & act spaces from dummy CDA env\n",
    "single_CDA_env = continuousDoubleAuctionEnv(\n",
    "    num_of_traders, \n",
    "    init_cash, \n",
    "    tick_size, \n",
    "    tape_display_length, \n",
    "    max_step, \n",
    "    is_render)\n",
    "obs_space = single_CDA_env.get_observation_space(single_CDA_env.agents[0])\n",
    "act_space = single_CDA_env.get_action_space(single_CDA_env.agents[0])\n",
    "print(single_CDA_env.agents)  # Should be a non-empty list\n",
    "print(single_CDA_env.get_observation_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "print(single_CDA_env.get_action_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "\n",
    "# register CDA env with RLlib \n",
    "# register_env(\n",
    "#     \"continuousDoubleAuction-v0\",\n",
    "#     lambda _: continuousDoubleAuctionEnv(\n",
    "#         num_of_traders, \n",
    "#         init_cash, \n",
    "#         tick_size, \n",
    "#         tape_display_length,\n",
    "#         max_step-1, \n",
    "#         is_render\n",
    "#     )\n",
    "# )\n",
    "# register_env(\n",
    "#     \"continuousDoubleAuction-v0\",\n",
    "#     lambda cfg: continuousDoubleAuctionEnv(\n",
    "#         num_of_traders, \n",
    "#         init_cash, \n",
    "#         tick_size, \n",
    "#         tape_display_length,\n",
    "#         max_step-1, \n",
    "#         is_render\n",
    "#     )\n",
    "# )\n",
    "# register_env(\n",
    "#     \"continuousDoubleAuction-v0\",\n",
    "#     lambda _: PettingZooEnv(\n",
    "#         continuousDoubleAuctionEnv(\n",
    "#             num_of_traders, \n",
    "#             init_cash, \n",
    "#             tick_size, \n",
    "#             tape_display_length,\n",
    "#             max_step-1, \n",
    "#             is_render\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "# register custom model (neural network)\n",
    "# ModelCatalog.register_custom_model(\"model_disc\", CustomModel_1) \n",
    "ModelCatalog.register_custom_model(\"model_disc\", CustomLSTMRLModule) \n",
    "\n",
    "ray.shutdown()\n",
    "# start ray\n",
    "# ray.init(ignore_reinit_error=True, log_to_driver=True, webui_host='127.0.0.1', num_cpus=2) \n",
    "# ray.init(ignore_reinit_error=True, log_to_driver=True, dashboard_host=\"127.0.0.1\", num_cpus=2)\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    log_to_driver=True,\n",
    "    num_cpus=2,\n",
    "    dashboard_host=\"127.0.0.1\",  # replaces webui_host\n",
    "    dashboard_port=8265,          # default port; replaces webui_port\n",
    "    # include_dashboard=True,        # default True\n",
    "    include_dashboard=False,        # default True\n",
    "\n",
    ")\n",
    "\n",
    "# Global storage, a ray actor that run on it's own process & it needs to be declared after ray.init().\n",
    "# g_store = storage.options(name=\"g_store\", detached=True).remote(num_agents)\n",
    "g_store = storage.options(name=\"g_store\", lifetime=\"detached\").remote(num_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cknk9Cnoke_u"
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "QN5IfMMvP4VA",
    "outputId": "7ff317cf-ab1b-49a1-dd83-24818635e8be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary of policies\n",
    "policies = {\"policy_{}\".format(i): gen_policy(i, obs_space, act_space) for i in range(num_policies)}\n",
    "set_agents_policies(policies, obs_space, act_space, num_agents, num_trained_agent)\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "def policy_mapper(agent_id):\n",
    "    \"\"\"\n",
    "    Required in RLlib config.\n",
    "    \"\"\"\n",
    "    for i in range(num_agents):\n",
    "        if agent_id == i:            \n",
    "            return \"policy_{}\".format(i)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4x0UTsxUhys"
   },
   "source": [
    "### Call back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqW_h3UrRs4z"
   },
   "outputs": [],
   "source": [
    "class MyCallbacks(DefaultCallbacks):\n",
    "    # def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                      policies: Dict[str, Policy],\n",
    "    #                      episode: MultiAgentEpisode, **kwargs):\n",
    "    def on_episode_start(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        print(f\"Episode {episode.episode_id} started.\")\n",
    "        \"\"\"\n",
    "        info[\"episode\"] is a MultiAgentEpisode object.\n",
    "\n",
    "        user_data dicts at 100000 items max, will auto replace old with new item at 1st index.\n",
    "        hist_data dicts at 100 items max, will auto replace old with new item at 1st index.\n",
    "        \"\"\"\n",
    "        #print(\"on_episode_start {}, _agent_to_policy {}\".format(episode.episode_id, episode._agent_to_policy))\n",
    "\n",
    "        prefix = \"agt_\"\n",
    "        for i in range(num_agents):\n",
    "            episode.user_data[prefix + str(i) + \"_obs\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_act\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_reward\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_NAV\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_num_trades\"] = []\n",
    "\n",
    "            episode.hist_data[prefix + str(i) + \"_reward\"] = []\n",
    "            episode.hist_data[prefix + str(i) + \"_NAV\"] = []\n",
    "            episode.hist_data[prefix + str(i) + \"_num_trades\"] = []\n",
    "\n",
    "    # def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                     episode: MultiAgentEpisode, **kwargs):\n",
    "    def on_episode_step(self, *, worker, base_env, episode, **kwargs):\n",
    "        # Example: track something each step\n",
    "        episode.user_data[\"my_metric\"].append(1)        \n",
    "        \"\"\"\n",
    "        pole_angle = abs(episode.last_observation_for()[2])\n",
    "        raw_angle = abs(episode.last_raw_obs_for()[2])\n",
    "        assert pole_angle == raw_angle\n",
    "        episode.user_data[\"pole_angles\"].append(pole_angle)\n",
    "        \"\"\"\n",
    "\n",
    "        prefix = \"agt_\"\n",
    "        for i in range(num_agents):\n",
    "            obs = episode.last_raw_obs_for(i)\n",
    "            #obs = episode.last_observation_for(i)\n",
    "            act = episode.last_action_for(i)\n",
    "            reward = episode.last_info_for(i).get(\"reward\")\n",
    "            NAV = episode.last_info_for(i).get(\"NAV\")\n",
    "            NAV = None if NAV is None else float(NAV)\n",
    "            num_trades = episode.last_info_for(i).get(\"num_trades\")\n",
    "        \n",
    "            if reward is None:      # goto next agent.\n",
    "                continue\n",
    "\n",
    "            episode.user_data[prefix + str(i) + \"_obs\"].append(obs)    \n",
    "            episode.user_data[prefix + str(i) + \"_act\"].append(act)    \n",
    "            episode.user_data[prefix + str(i) + \"_reward\"].append(reward)    \n",
    "            episode.user_data[prefix + str(i) + \"_NAV\"].append(NAV)    \n",
    "            episode.user_data[prefix + str(i) + \"_num_trades\"].append(num_trades)          \n",
    "\n",
    "    # def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                    policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "    #                    **kwargs):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        # Example: summarize at end\n",
    "        mean_metric = sum(episode.user_data[\"my_metric\"]) / len(episode.user_data[\"my_metric\"])\n",
    "        print(f\"Episode {episode.episode_id} ended. Mean my_metric: {mean_metric}\")        \n",
    "        #print(\"on_episode_end {}, episode.agent_rewards {}\".format(episode.episode_id, episode.agent_rewards))\n",
    "        \"\"\"\n",
    "        arg: {\"env\": .., \"episode\": ...}\n",
    "        \"\"\"\n",
    "\n",
    "        g_store = ray.util.get_actor(\"g_store\")\n",
    "        prefix = \"agt_\"\n",
    "        for agt_id in range(num_agents):\n",
    "            obs_key = prefix + str(agt_id) + \"_obs\"\n",
    "            act_key = prefix + str(agt_id) + \"_act\"\n",
    "            reward_key = prefix + str(agt_id) + \"_reward\"\n",
    "            NAV_key = prefix + str(agt_id) + \"_NAV\"\n",
    "            num_trades_key = prefix + str(agt_id) + \"_num_trades\"      \n",
    "\n",
    "            # store into episode.hist_data\n",
    "            store_eps_hist_data(episode, reward_key)\n",
    "            store_eps_hist_data(episode, NAV_key)\n",
    "            store_eps_hist_data(episode, num_trades_key)\n",
    "\n",
    "            # store step data\n",
    "            obs = episode.user_data[obs_key]\n",
    "            act = episode.user_data[act_key]\n",
    "            reward = episode.user_data[reward_key]\n",
    "            NAV = episode.user_data[NAV_key]\n",
    "            num_trades = episode.user_data[num_trades_key]\n",
    "            ray.get(g_store.store_agt_step.remote(agt_id, obs, act, reward, NAV, num_trades))       \n",
    "        \n",
    "            # Store episode data.   \n",
    "            eps_reward = np.sum(reward)\n",
    "            eps_NAV = np.sum(NAV)\n",
    "            eps_num_trades = np.sum(num_trades)\n",
    "            ray.get(g_store.store_agt_eps.remote(agt_id, eps_reward, eps_NAV, eps_num_trades))\n",
    "\n",
    "        ray.get(g_store.inc_eps_counter.remote())  \n",
    "\n",
    "    # def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n",
    "    #                   **kwargs):\n",
    "    #     \"\"\"\n",
    "    #     arg: {\"samples\": .., \"worker\": ...}\n",
    "\n",
    "    #     Notes:\n",
    "    #         https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py        \n",
    "    #     \"\"\"\n",
    "    #     #print(\"on_sample_end returned sample batch of size {}\".format(samples.count))\n",
    "\n",
    "    #     \"\"\"\n",
    "    #     MultiAgentBatch_obj = info[\"samples\"]\n",
    "    #     MultiAgentBatch_policy_batches = MultiAgentBatch_obj.policy_batches\n",
    "    #     MultiAgentBatch_count = MultiAgentBatch_obj.count\n",
    "\n",
    "    #     access_sample_batches(MultiAgentBatch_policy_batches)\n",
    "    #     print(\"info['samples'].policy_batches = {}\".format(info[\"samples\"].policy_batches))\n",
    "    #     print(\"info['worker'] = {}\".format(info[\"worker\"])) # RolloutWorker object\n",
    "    #     \"\"\"\n",
    "\n",
    "    def on_train_result(self, trainer, result: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        info[\"trainer\"] is the trainer object.\n",
    "\n",
    "        info[\"result\"] contains a bunch of info such as episodic rewards for \n",
    "        each policy in info[\"result\"][hist_stats] dictionary.\n",
    "        \"\"\"\n",
    "        #print(\"trainer.train() result: {} -> {} episodes\".format(trainer, result[\"episodes_this_iter\"]))\n",
    "        # you can mutate the result dict to add new fields to return\n",
    "        result[\"callback_ok\"] = True\n",
    "        #print(\"on_train_result result\", result)\n",
    "    \n",
    "        train_policies_name = get_trained_policies_name(policies, num_trained_agent)    \n",
    "        max_reward_ind = get_max_reward_ind(result, train_policies_name)\n",
    "        max_reward_policy_name = train_policies_name[max_reward_ind]\n",
    "        cp_weight(trainer, train_policies_name, max_reward_policy_name)    \n",
    "\n",
    "        g_store = ray.util.get_actor(\"g_store\")      \n",
    "        prefix = \"policy_policy_\"\n",
    "        suffix = \"_reward\"\n",
    "        hist_stats = result[\"hist_stats\"]\n",
    "        eps_this_iter = result[\"episodes_this_iter\"]\n",
    "        for agt_id in range(num_agents):\n",
    "            key = prefix + str(agt_id) + suffix\n",
    "            for i in range(eps_this_iter):\n",
    "                ray.get(g_store.store_agt_train.remote(agt_id, hist_stats[key][i]))\n",
    "\n",
    "        #print(\"on_train_result info['result'] {}\".format(info[\"result\"]))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEnp5UpxkDve"
   },
   "source": [
    "### RLlib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fr_a0y6hjn6"
   },
   "outputs": [],
   "source": [
    "# def get_config():\n",
    "#     config = ppo.DEFAULT_CONFIG.copy()\n",
    "#     config[\"multiagent\"] = {\"policies_to_train\": train_policy_list,\n",
    "#                             \"policies\": policies,\n",
    "#                             \"policy_mapping_fn\": policy_mapper,\n",
    "#                            }    \n",
    "#     #config[\"num_cpus\"] = num_cpus     # trainer, applicable only when using tune.\n",
    "#     config[\"num_gpus\"] = num_gpus     # trainer\n",
    "#     config[\"num_cpus_per_worker\"] = num_cpus_per_worker                                \n",
    "#     config[\"num_gpus_per_worker\"] = num_gpus_per_worker                      \n",
    "#     config[\"num_workers\"] = num_workers\n",
    "#     config[\"num_envs_per_worker\"] = num_envs_per_worker  \n",
    "#     config[\"batch_mode\"] = batch_mode       # \"complete_episodes\" / \"truncate_episodes\"\n",
    "#     config[\"train_batch_size\"] = train_batch_size # Training batch size, if applicable. Should be >= rollout_fragment_length.\n",
    "#                                                   # Samples batches will be concatenated together to a batch of this size,\n",
    "#                                                   # which is then passed to SGD.\n",
    "#     config[\"rollout_fragment_length\"] = rollout_fragment_length # replaced \"sample_batch_size\",\n",
    "#     config[\"sgd_minibatch_size\"] = sgd_minibatch_size \n",
    "#     config[\"log_level\"] = \"WARN\" # WARN/INFO/DEBUG \n",
    "#     config[\"callbacks\"] = MyCallbacks\n",
    "#     config[\"output\"] = log_dir\n",
    "\n",
    "#     return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "\n",
    "def get_config():\n",
    "    config = (\n",
    "        PPOConfig()            \n",
    "        .environment(\n",
    "            # \"continuousDoubleAuction-v0\",\n",
    "            continuousDoubleAuctionEnv,\n",
    "            env_config={\n",
    "                \"num_of_agents\": num_of_traders, \n",
    "                \"init_cash\": init_cash, \n",
    "                \"tick_size\": tick_size, \n",
    "                \"tape_display_length\": tape_display_length,\n",
    "                \"max_step\": max_step - 1, \n",
    "                \"is_render\": is_render\n",
    "            }\n",
    "            # env_config={\"disable_env_checker\": True},            \n",
    "        ) \n",
    "        .multi_agent(\n",
    "            policies=policies,\n",
    "            policy_mapping_fn=policy_mapper,\n",
    "            policies_to_train=train_policy_list,\n",
    "        )\n",
    "        .env_runners(\n",
    "            # num_env_runners=num_workers,\n",
    "            num_env_runners=0,\n",
    "            \n",
    "            num_envs_per_env_runner=num_envs_per_worker,\n",
    "            rollout_fragment_length=rollout_fragment_length,\n",
    "            batch_mode=batch_mode,\n",
    "        )\n",
    "        .learners(\n",
    "            num_learners=1,  # Typically 1 learner unless using distributed training\n",
    "            num_gpus_per_learner=num_gpus,  # Trainer GPU allocation\n",
    "            num_cpus_per_learner=num_cpus_per_worker,\n",
    "        )\n",
    "        .training(\n",
    "            train_batch_size_per_learner=train_batch_size,\n",
    "            # sgd_minibatch_size,\n",
    "        )\n",
    "        .callbacks(MyCallbacks)\n",
    "        # .output_dir(log_dir)\n",
    "        .framework(\"torch\")  # Explicitly set framework if needed\n",
    "    )\n",
    "\n",
    "    # Optional: Configure resources more granularly if needed\n",
    "    if num_gpus_per_worker > 0:\n",
    "        config.env_runners(\n",
    "            num_gpus_per_env_runner=num_gpus_per_worker\n",
    "        )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKLNyViDkI9O"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iyMEBA0uC8ZW",
    "outputId": "b490fc96-4321-4150-b3c4-34d391389446"
   },
   "outputs": [],
   "source": [
    "# def go_train(config):    \n",
    "#     trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")\n",
    "    \n",
    "#     if is_restore == True:\n",
    "#         trainer.restore(restore_path) \n",
    "\n",
    "#     g_store = ray.util.get_actor(\"g_store\")          \n",
    "#     result = None\n",
    "#     for i in range(num_iters):\n",
    "#         result = trainer.train()       \n",
    "#         print(pretty_print(result)) # includes result[\"custom_metrics\"]\n",
    "#         print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
    "#         print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n",
    "\n",
    "#         if i % chkpt_freq == 0:\n",
    "#             checkpoint = trainer.save(local_dir)\n",
    "#             print(\"checkpoint saved at\", checkpoint)\n",
    "    \n",
    "#     checkpoint = trainer.save(local_dir)\n",
    "#     print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "#     print(\"result['experiment_id']\", result[\"experiment_id\"])\n",
    "    \n",
    "#     return result[\"experiment_id\"]\n",
    "    \n",
    "# # run everything\n",
    "# experiment_id = go_train(get_config())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def go_train(config):    \n",
    "    # trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")      \n",
    "    # algo = config.build()   \n",
    "    algo = config.build()\n",
    "    \n",
    "    # if is_restore == True:\n",
    "    #     trainer.restore(restore_path) \n",
    "\n",
    "    # g_store = ray.util.get_actor(\"g_store\")          \n",
    "    # result = None\n",
    "    # for i in range(num_iters):\n",
    "    #     result = algo.train()       \n",
    "\n",
    "    #     print(pretty_print(result)) # includes result[\"custom_metrics\"]\n",
    "    #     print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
    "    #     print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n",
    "\n",
    "    #     if i % chkpt_freq == 0:\n",
    "    #         checkpoint = algo.save(local_dir)\n",
    "    #         print(\"checkpoint saved at\", checkpoint)\n",
    "    \n",
    "    # checkpoint = algo.save(local_dir)\n",
    "    # print(\"checkpoint saved at\", checkpoint)\n",
    "    # print(\"result['experiment_id']\", result[\"experiment_id\"])\n",
    "\n",
    "    # return result[\"experiment_id\"]\n",
    "    return None\n",
    "    \n",
    "# run everything\n",
    "experiment_id = go_train(get_config())            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmRFarjpD7Wt"
   },
   "source": [
    "### Plot all steps.\n",
    "\n",
    "Agt_0, 1, 2 are trained agents (with PPO) while the rest are random agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "wzMc3PY_D7qv",
    "outputId": "17e774a8-2d2f-46a8-ec0b-d56894c84250"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "e0NDLLyiESgx",
    "outputId": "f16045e7-bb85-44a4-f67a-73fd1b0e861f"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"NAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "GrqwrFwsETlK",
    "outputId": "0da3cef4-aa85-47ae-c771-9d2f84b84ba6"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-1mV5YFYwkN"
   },
   "source": [
    "### Log/load last episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "khD_kiwaJVSQ",
    "outputId": "01e3109b-a024-44fc-8374-98db6be9f170"
   },
   "outputs": [],
   "source": [
    "# log_g_store(log_g_store_dir, num_agents, experiment_id)\n",
    "# load_g_store(log_g_store_dir, num_agents, experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSu7XmUSDVOE"
   },
   "source": [
    "### Plot steps from last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "rNraawhLmH68",
    "outputId": "d9d3d2b3-b345-4597-c49f-863b52355c2e"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "Lke086wkL4If",
    "outputId": "8bc0af37-3841-456f-8d1d-ba581cae5221"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"NAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "2uqneDvkL4x6",
    "outputId": "d93996d1-a637-4383-bedf-c5687e250d5b"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dL1TZkz-Wvqx"
   },
   "source": [
    "### LOB from last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR5iU78gCePl"
   },
   "outputs": [],
   "source": [
    "# g_store = ray.util.get_actor(\"g_store\")          \n",
    "# #store = ray.get(g_rere.get_storage.remote())\n",
    "\n",
    "# depth = 10\n",
    "# bid_size, bid_price, ask_size, ask_price = ray.get(g_store.get_obs_from_agt.remote(0, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPa_j4M2Z6mh"
   },
   "source": [
    "### LOB order imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2PBgQwhTBOqQ",
    "outputId": "0ecf046a-2ed1-4878-d1b4-7d827b4e8a96",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ord_imb_store = ord_imb(bid_size, ask_size)\n",
    "# plot_LOB_subplot(ord_imb_store, depth, '_ord_imb') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9Kb9NpmaHg4"
   },
   "source": [
    "### LOB sum of order imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "MXydsI2nC76g",
    "outputId": "6078375e-4473-448c-9d64-f7f19f302101"
   },
   "outputs": [],
   "source": [
    "# ord_imb_store = np.asarray(ord_imb_store)\n",
    "# sum_ord_imb_store = sum_ord_imb(ord_imb_store)\n",
    "# plot_sum_ord_imb(sum_ord_imb_store, \"sum_ord_imb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12iZkIMdaS_l"
   },
   "source": [
    "### LOB mid price (subplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YhRaxfpg_w2t",
    "outputId": "48a2657d-8c53-4a33-f8f7-949ce90fc2b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mid_price_store = mid_price(bid_price, ask_price)\n",
    "# plot_LOB_subplot(mid_price_store, depth, '_mid_price')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWPlbUqebcJi"
   },
   "source": [
    "### LOB mid prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "CZ724VMdbbWs",
    "outputId": "1a8af35b-4592-4b5d-e17d-9ddeb9f409c2"
   },
   "outputs": [],
   "source": [
    "# plot_mid_prices(mid_price_store,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JV5v-yYZdqO"
   },
   "source": [
    "### LOB bid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PS-hHA9Mx1CZ",
    "outputId": "1f3f4ea1-e607-4612-a8d8-2efd8a833bfa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(bid_size, depth, '_bid_size')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUotX7pQZiXP"
   },
   "source": [
    "### LOB ask size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iYuL9s5ex5So",
    "outputId": "4108d574-fa52-4ee6-bf24-efeeb8c7471c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(ask_size, depth, '_ask_size')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKiDC3C2ZnW4"
   },
   "source": [
    "### LOB bid price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "X2qQ-O01x5ER",
    "outputId": "90f18010-dd45-476b-cc78-dc42f613703c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(bid_price, depth, '_bid_price')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3HcZH-KZuLd"
   },
   "source": [
    "### LOB ask price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FifBnZAdx6uT",
    "outputId": "562f8092-b909-44dd-d1bd-cd3637164986",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(ask_price, depth, '_ask_price')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 07:07:11,226\tWARNING services.py:2152 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.83gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-20 07:07:12,371\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-07-20 07:07:12,847\tWARNING deprecation.py:50 -- DeprecationWarning: `config.training(num_sgd_iter=..)` has been deprecated. Use `config.training(num_epochs=..)` instead. This will raise an error in the future!\n",
      "2025-07-20 07:07:12,848\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-07-20 07:07:12,849\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-07-20 07:07:12,884\tERROR multi_agent_env_runner.py:834 -- 'dict' object has no attribute 'sample'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/multi_agent_env_runner.py\", line 832, in make_env\n",
      "    check_multiagent_environments(env.unwrapped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py\", line 57, in check_multiagent_environments\n",
      "    sampled_action = {\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py\", line 58, in <dictcomp>\n",
      "    aid: env.get_action_space(aid).sample() for aid in reset_obs.keys()\n",
      "AttributeError: 'dict' object has no attribute 'sample'\n",
      "2025-07-20 07:07:12,996\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: training completed\n",
      "Iteration 1: training completed\n",
      "Iteration 2: training completed\n",
      "Iteration 3: training completed\n",
      "Iteration 4: training completed\n",
      "Iteration 5: training completed\n",
      "Iteration 6: training completed\n",
      "Iteration 7: training completed\n",
      "Iteration 8: training completed\n",
      "Iteration 9: training completed\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray import tune  # Important: use ray.tune, not ray.tune.registry\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "class CustomMARLEnv(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        self.agents = [\"agent_0\", \"agent_1\", \"agent_2\"]\n",
    "        self._agent_ids = set(self.agents)\n",
    "        \n",
    "        # Define spaces as dictionaries mapping agent_id to space\n",
    "        obs_space = spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "        act_space = spaces.Discrete(3)\n",
    "        \n",
    "        self.observation_space = {agent: obs_space for agent in self.agents}\n",
    "        self.action_space = {agent: act_space for agent in self.agents}\n",
    "        \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        observations = {agent: self.observation_space[agent].sample() for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        return observations, infos\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        observations = {agent: self.observation_space[agent].sample() for agent in self.agents}\n",
    "        rewards = {agent: np.random.random() for agent in self.agents}\n",
    "        \n",
    "        terminateds = {agent: False for agent in self.agents}\n",
    "        truncateds = {agent: False for agent in self.agents}\n",
    "        # Global episode termination (required!)\n",
    "        terminateds[\"__all__\"] = False  # Set to True when episode should end\n",
    "        truncateds[\"__all__\"] = False   # Set to True when episode should be truncated        \n",
    "        \n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        \n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return CustomMARLEnv(env_config)\n",
    "\n",
    "# Initialize Ray first\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Register environment with ray.tune - this is the key fix!\n",
    "tune.register_env(\"custom_marl_env\", env_creator)\n",
    "\n",
    "# Create config with correct new API\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"custom_marl_env\")\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"shared_policy\": PolicySpec(),\n",
    "        },\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: \"shared_policy\",\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=0,\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        num_sgd_iter=10,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build and train\n",
    "try:\n",
    "    algo = config.build()\n",
    "    \n",
    "    for i in range(10):\n",
    "        result = algo.train()\n",
    "        # Try different possible result keys\n",
    "        reward_key = None\n",
    "        for key in ['env_runners/episode_reward_mean', 'episode_reward_mean', 'hist_stats/episode_reward']:\n",
    "            if key in result:\n",
    "                reward_key = key\n",
    "                break\n",
    "        \n",
    "        if reward_key:\n",
    "            print(f\"Iteration {i}: reward = {result[reward_key]}\")\n",
    "        else:\n",
    "            print(f\"Iteration {i}: training completed\")\n",
    "    \n",
    "    algo.stop()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    if 'algo' in locals():\n",
    "        algo.stop()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CDA_NSP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
