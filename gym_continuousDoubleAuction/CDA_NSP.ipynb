{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f05ZH97QkoJf"
   },
   "source": [
    "# Sample training script with naive competitive self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcLSdJuUkTrX"
   },
   "source": [
    "### Switch directory in Google drive so as to import CDA env.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "PAqVG2cqjLXM",
    "outputId": "2cd035eb-6b74-4432-b82e-76b9162f439a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# %cd \"/root/ray_results/\"\n",
    "# !ls -l\n",
    "# #!rm -rf PPO_continuousDoubleAuction-v0_*\n",
    "# !ls -l\n",
    "# !pwd\n",
    "\n",
    "# %cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
    "# !ls -l\n",
    "\n",
    "# #!pip install -r requirements.txt\n",
    "\n",
    "# #!pip install tensorflow==2.2.0\n",
    "# #!pip install ray[rllib]==0.8.5\n",
    "\n",
    "# #!pip show tensorflow\n",
    "# #!pip show ray\n",
    "\n",
    "# #!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sortedcontainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7ZHcwBWkXVM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UW3INjDipTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports all OK.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "\n",
    "# from ray.rllib.agents.ppo import ppo\n",
    "# from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "\n",
    "\n",
    "# from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "# from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "\n",
    "\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
    "\n",
    "\n",
    "\n",
    "# from gym_continuousDoubleAuction.train.model.model_handler import CustomModel_1\n",
    "from gym_continuousDoubleAuction.train.model.model_handler import CustomLSTMRLModule\n",
    "\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.train.policy.policy_handler import make_RandomPolicy, gen_policy, set_agents_policies, create_train_policy_list\n",
    "from gym_continuousDoubleAuction.train.weight.weight_handler import get_trained_policies_name, get_max_reward_ind, cp_weight\n",
    "from gym_continuousDoubleAuction.train.storage.store_handler import storage\n",
    "from gym_continuousDoubleAuction.train.callbk.callbk_handler import store_eps_hist_data\n",
    "from gym_continuousDoubleAuction.train.logger.log_handler import create_dir, log_g_store, load_g_store\n",
    "from gym_continuousDoubleAuction.train.plotter.plot_handler import plot_storage, plot_LOB_subplot, plot_sum_ord_imb, plot_mid_prices\n",
    "from gym_continuousDoubleAuction.train.helper.helper import ord_imb, sum_ord_imb, mid_price\n",
    "\n",
    "# tf = try_import_tf()\n",
    "print(f'Imports all OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDnpi8k5kbYo"
   },
   "source": [
    "### Global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "UqzjVWUsPykm",
    "outputId": "18a6a936-1477-4e90-98c6-cdf7be851dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_policy_list =  ['policy_0', 'policy_1', 'policy_2']\n",
      "Folder creation failed or folder already exists: results/\n",
      "Folder creation failed or folder already exists: results/log_g_store/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 00:03:32,817\tWARNING services.py:2152 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.82gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-12 00:03:33,867\tINFO worker.py:1917 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m It looks like you're creating a detached actor in an anonymous namespace. In order to access this actor in the future, you will need to explicitly connect to this namespace with ray.init(namespace=\"888efc26-28ed-45be-9b76-448f3f238800\", ...)\n"
     ]
    }
   ],
   "source": [
    "# CDA_env args\n",
    "num_agents = 9\n",
    "num_trained_agent = 3 # \n",
    "num_policies = num_agents # Each agent is using a separate policy\n",
    "num_of_traders = num_agents\n",
    "tape_display_length = 10 \n",
    "tick_size = 1\n",
    "init_cash = 1000000\n",
    "max_step = 4096 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n",
    "is_render = False \n",
    "\n",
    "# RLlib config \n",
    "train_policy_list = create_train_policy_list(num_trained_agent, \"policy_\")\n",
    "#num_cpus = 0.25                                \n",
    "num_gpus = 0.75 #0                       \n",
    "num_cpus_per_worker = 0.25                                \n",
    "num_gpus_per_worker = 0\n",
    "num_workers = 2\n",
    "num_envs_per_worker = 4\n",
    "batch_mode = \"complete_episodes\" \n",
    "rollout_fragment_length = 128\n",
    "train_batch_size = max_step\n",
    "sgd_minibatch_size = 256\n",
    "num_iters = 2\n",
    "\n",
    "# log_base_dir = \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/results/\"\n",
    "log_base_dir = \"results/\"\n",
    "log_dir = log_base_dir + \"ray_results/\"\n",
    "\n",
    "# Chkpt & restore\n",
    "local_dir = log_base_dir + \"chkpt/\"\n",
    "chkpt_freq = 10\n",
    "chkpt = 320\n",
    "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
    "is_restore = True # True / False\n",
    "\n",
    "# log & load \n",
    "log_g_store_dir = log_base_dir + \"log_g_store/\"\n",
    "create_dir(log_base_dir)    \n",
    "create_dir(log_g_store_dir)    \n",
    "\n",
    "# get obs & act spaces from dummy CDA env\n",
    "single_CDA_env = continuousDoubleAuctionEnv(num_of_traders, init_cash, tick_size, tape_display_length, max_step, is_render)\n",
    "obs_space = single_CDA_env.observation_space\n",
    "act_space = single_CDA_env.action_space\n",
    "\n",
    "# register CDA env with RLlib \n",
    "register_env(\"continuousDoubleAuction-v0\", lambda _: continuousDoubleAuctionEnv(num_of_traders, \n",
    "                                                                                init_cash, \n",
    "                                                                                tick_size, \n",
    "                                                                                tape_display_length,\n",
    "                                                                                max_step-1, \n",
    "                                                                                is_render))\n",
    "\n",
    "# register custom model (neural network)\n",
    "# ModelCatalog.register_custom_model(\"model_disc\", CustomModel_1) \n",
    "ModelCatalog.register_custom_model(\"model_disc\", CustomLSTMRLModule) \n",
    "\n",
    "ray.shutdown()\n",
    "# start ray\n",
    "# ray.init(ignore_reinit_error=True, log_to_driver=True, webui_host='127.0.0.1', num_cpus=2) \n",
    "# ray.init(ignore_reinit_error=True, log_to_driver=True, dashboard_host=\"127.0.0.1\", num_cpus=2)\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    log_to_driver=True,\n",
    "    num_cpus=2,\n",
    "    dashboard_host=\"127.0.0.1\",  # replaces webui_host\n",
    "    dashboard_port=8265,          # default port; replaces webui_port\n",
    "    # include_dashboard=True,        # default True\n",
    "    include_dashboard=False,        # default True\n",
    "\n",
    ")\n",
    "\n",
    "# Global storage, a ray actor that run on it's own process & it needs to be declared after ray.init().\n",
    "# g_store = storage.options(name=\"g_store\", detached=True).remote(num_agents)\n",
    "g_store = storage.options(name=\"g_store\", lifetime=\"detached\").remote(num_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cknk9Cnoke_u"
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "QN5IfMMvP4VA",
    "outputId": "7ff317cf-ab1b-49a1-dd83-24818635e8be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policies: {'policy_0': (None, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {}), 'policy_1': (None, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {}), 'policy_2': (None, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {}), 'policy_3': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {}), 'policy_4': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {}), 'policy_5': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {}), 'policy_6': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {}), 'policy_7': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {}), 'policy_8': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, Box(-inf, inf, (4, 10), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), {})}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of policies\n",
    "policies = {\"policy_{}\".format(i): gen_policy(i, obs_space, act_space) for i in range(num_policies)}\n",
    "set_agents_policies(policies, obs_space, act_space, num_agents, num_trained_agent)\n",
    "policy_ids = list(policies.keys())\n",
    "\n",
    "def policy_mapper(agent_id):\n",
    "    \"\"\"\n",
    "    Required in RLlib config.\n",
    "    \"\"\"\n",
    "    for i in range(num_agents):\n",
    "        if agent_id == i:            \n",
    "            return \"policy_{}\".format(i)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4x0UTsxUhys"
   },
   "source": [
    "### Call back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqW_h3UrRs4z"
   },
   "outputs": [],
   "source": [
    "class MyCallbacks(DefaultCallbacks):\n",
    "    # def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                      policies: Dict[str, Policy],\n",
    "    #                      episode: MultiAgentEpisode, **kwargs):\n",
    "    def on_episode_start(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        print(f\"Episode {episode.episode_id} started.\")\n",
    "        \"\"\"\n",
    "        info[\"episode\"] is a MultiAgentEpisode object.\n",
    "\n",
    "        user_data dicts at 100000 items max, will auto replace old with new item at 1st index.\n",
    "        hist_data dicts at 100 items max, will auto replace old with new item at 1st index.\n",
    "        \"\"\"\n",
    "        #print(\"on_episode_start {}, _agent_to_policy {}\".format(episode.episode_id, episode._agent_to_policy))\n",
    "\n",
    "        prefix = \"agt_\"\n",
    "        for i in range(num_agents):\n",
    "            episode.user_data[prefix + str(i) + \"_obs\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_act\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_reward\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_NAV\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_num_trades\"] = []\n",
    "\n",
    "            episode.hist_data[prefix + str(i) + \"_reward\"] = []\n",
    "            episode.hist_data[prefix + str(i) + \"_NAV\"] = []\n",
    "            episode.hist_data[prefix + str(i) + \"_num_trades\"] = []\n",
    "\n",
    "    # def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                     episode: MultiAgentEpisode, **kwargs):\n",
    "    def on_episode_step(self, *, worker, base_env, episode, **kwargs):\n",
    "        # Example: track something each step\n",
    "        episode.user_data[\"my_metric\"].append(1)        \n",
    "        \"\"\"\n",
    "        pole_angle = abs(episode.last_observation_for()[2])\n",
    "        raw_angle = abs(episode.last_raw_obs_for()[2])\n",
    "        assert pole_angle == raw_angle\n",
    "        episode.user_data[\"pole_angles\"].append(pole_angle)\n",
    "        \"\"\"\n",
    "\n",
    "        prefix = \"agt_\"\n",
    "        for i in range(num_agents):\n",
    "            obs = episode.last_raw_obs_for(i)\n",
    "            #obs = episode.last_observation_for(i)\n",
    "            act = episode.last_action_for(i)\n",
    "            reward = episode.last_info_for(i).get(\"reward\")\n",
    "            NAV = episode.last_info_for(i).get(\"NAV\")\n",
    "            NAV = None if NAV is None else float(NAV)\n",
    "            num_trades = episode.last_info_for(i).get(\"num_trades\")\n",
    "        \n",
    "            if reward is None:      # goto next agent.\n",
    "                continue\n",
    "\n",
    "            episode.user_data[prefix + str(i) + \"_obs\"].append(obs)    \n",
    "            episode.user_data[prefix + str(i) + \"_act\"].append(act)    \n",
    "            episode.user_data[prefix + str(i) + \"_reward\"].append(reward)    \n",
    "            episode.user_data[prefix + str(i) + \"_NAV\"].append(NAV)    \n",
    "            episode.user_data[prefix + str(i) + \"_num_trades\"].append(num_trades)          \n",
    "\n",
    "    # def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                    policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "    #                    **kwargs):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        # Example: summarize at end\n",
    "        mean_metric = sum(episode.user_data[\"my_metric\"]) / len(episode.user_data[\"my_metric\"])\n",
    "        print(f\"Episode {episode.episode_id} ended. Mean my_metric: {mean_metric}\")        \n",
    "        #print(\"on_episode_end {}, episode.agent_rewards {}\".format(episode.episode_id, episode.agent_rewards))\n",
    "        \"\"\"\n",
    "        arg: {\"env\": .., \"episode\": ...}\n",
    "        \"\"\"\n",
    "\n",
    "        g_store = ray.util.get_actor(\"g_store\")\n",
    "        prefix = \"agt_\"\n",
    "        for agt_id in range(num_agents):\n",
    "            obs_key = prefix + str(agt_id) + \"_obs\"\n",
    "            act_key = prefix + str(agt_id) + \"_act\"\n",
    "            reward_key = prefix + str(agt_id) + \"_reward\"\n",
    "            NAV_key = prefix + str(agt_id) + \"_NAV\"\n",
    "            num_trades_key = prefix + str(agt_id) + \"_num_trades\"      \n",
    "\n",
    "            # store into episode.hist_data\n",
    "            store_eps_hist_data(episode, reward_key)\n",
    "            store_eps_hist_data(episode, NAV_key)\n",
    "            store_eps_hist_data(episode, num_trades_key)\n",
    "\n",
    "            # store step data\n",
    "            obs = episode.user_data[obs_key]\n",
    "            act = episode.user_data[act_key]\n",
    "            reward = episode.user_data[reward_key]\n",
    "            NAV = episode.user_data[NAV_key]\n",
    "            num_trades = episode.user_data[num_trades_key]\n",
    "            ray.get(g_store.store_agt_step.remote(agt_id, obs, act, reward, NAV, num_trades))       \n",
    "        \n",
    "            # Store episode data.   \n",
    "            eps_reward = np.sum(reward)\n",
    "            eps_NAV = np.sum(NAV)\n",
    "            eps_num_trades = np.sum(num_trades)\n",
    "            ray.get(g_store.store_agt_eps.remote(agt_id, eps_reward, eps_NAV, eps_num_trades))\n",
    "\n",
    "        ray.get(g_store.inc_eps_counter.remote())  \n",
    "\n",
    "    # def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n",
    "    #                   **kwargs):\n",
    "    #     \"\"\"\n",
    "    #     arg: {\"samples\": .., \"worker\": ...}\n",
    "\n",
    "    #     Notes:\n",
    "    #         https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py        \n",
    "    #     \"\"\"\n",
    "    #     #print(\"on_sample_end returned sample batch of size {}\".format(samples.count))\n",
    "\n",
    "    #     \"\"\"\n",
    "    #     MultiAgentBatch_obj = info[\"samples\"]\n",
    "    #     MultiAgentBatch_policy_batches = MultiAgentBatch_obj.policy_batches\n",
    "    #     MultiAgentBatch_count = MultiAgentBatch_obj.count\n",
    "\n",
    "    #     access_sample_batches(MultiAgentBatch_policy_batches)\n",
    "    #     print(\"info['samples'].policy_batches = {}\".format(info[\"samples\"].policy_batches))\n",
    "    #     print(\"info['worker'] = {}\".format(info[\"worker\"])) # RolloutWorker object\n",
    "    #     \"\"\"\n",
    "\n",
    "    def on_train_result(self, trainer, result: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        info[\"trainer\"] is the trainer object.\n",
    "\n",
    "        info[\"result\"] contains a bunch of info such as episodic rewards for \n",
    "        each policy in info[\"result\"][hist_stats] dictionary.\n",
    "        \"\"\"\n",
    "        #print(\"trainer.train() result: {} -> {} episodes\".format(trainer, result[\"episodes_this_iter\"]))\n",
    "        # you can mutate the result dict to add new fields to return\n",
    "        result[\"callback_ok\"] = True\n",
    "        #print(\"on_train_result result\", result)\n",
    "    \n",
    "        train_policies_name = get_trained_policies_name(policies, num_trained_agent)    \n",
    "        max_reward_ind = get_max_reward_ind(result, train_policies_name)\n",
    "        max_reward_policy_name = train_policies_name[max_reward_ind]\n",
    "        cp_weight(trainer, train_policies_name, max_reward_policy_name)    \n",
    "\n",
    "        g_store = ray.util.get_actor(\"g_store\")      \n",
    "        prefix = \"policy_policy_\"\n",
    "        suffix = \"_reward\"\n",
    "        hist_stats = result[\"hist_stats\"]\n",
    "        eps_this_iter = result[\"episodes_this_iter\"]\n",
    "        for agt_id in range(num_agents):\n",
    "            key = prefix + str(agt_id) + suffix\n",
    "            for i in range(eps_this_iter):\n",
    "                ray.get(g_store.store_agt_train.remote(agt_id, hist_stats[key][i]))\n",
    "\n",
    "        #print(\"on_train_result info['result'] {}\".format(info[\"result\"]))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEnp5UpxkDve"
   },
   "source": [
    "### RLlib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fr_a0y6hjn6"
   },
   "outputs": [],
   "source": [
    "# def get_config():\n",
    "#     config = ppo.DEFAULT_CONFIG.copy()\n",
    "#     config[\"multiagent\"] = {\"policies_to_train\": train_policy_list,\n",
    "#                             \"policies\": policies,\n",
    "#                             \"policy_mapping_fn\": policy_mapper,\n",
    "#                            }    \n",
    "#     #config[\"num_cpus\"] = num_cpus     # trainer, applicable only when using tune.\n",
    "#     config[\"num_gpus\"] = num_gpus     # trainer\n",
    "#     config[\"num_cpus_per_worker\"] = num_cpus_per_worker                                \n",
    "#     config[\"num_gpus_per_worker\"] = num_gpus_per_worker                      \n",
    "#     config[\"num_workers\"] = num_workers\n",
    "#     config[\"num_envs_per_worker\"] = num_envs_per_worker  \n",
    "#     config[\"batch_mode\"] = batch_mode       # \"complete_episodes\" / \"truncate_episodes\"\n",
    "#     config[\"train_batch_size\"] = train_batch_size # Training batch size, if applicable. Should be >= rollout_fragment_length.\n",
    "#                                                   # Samples batches will be concatenated together to a batch of this size,\n",
    "#                                                   # which is then passed to SGD.\n",
    "#     config[\"rollout_fragment_length\"] = rollout_fragment_length # replaced \"sample_batch_size\",\n",
    "#     config[\"sgd_minibatch_size\"] = sgd_minibatch_size \n",
    "#     config[\"log_level\"] = \"WARN\" # WARN/INFO/DEBUG \n",
    "#     config[\"callbacks\"] = MyCallbacks\n",
    "#     config[\"output\"] = log_dir\n",
    "\n",
    "#     return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .framework(\"torch\")  # or \"tf\"\n",
    "        .env_runners(\n",
    "            num_env_runners=num_workers,              # renamed param here\n",
    "            num_envs_per_env_runner=num_envs_per_worker,  # renamed param here\n",
    "            rollout_fragment_length=rollout_fragment_length,\n",
    "            batch_mode=batch_mode,\n",
    "        )\n",
    "        .training(\n",
    "            train_batch_size=train_batch_size,\n",
    "            sgd_minibatch_size=sgd_minibatch_size,\n",
    "        )\n",
    "        .resources(\n",
    "            num_gpus=num_gpus,\n",
    "            num_cpus_per_worker=num_cpus_per_worker,\n",
    "            num_gpus_per_worker=num_gpus_per_worker,\n",
    "        )\n",
    "        .multi_agent(\n",
    "            policies=policies,\n",
    "            policy_mapping_fn=policy_mapper,\n",
    "            policies_to_train=train_policy_list,\n",
    "        )\n",
    "        .callbacks(MyCallbacks)\n",
    "        .debugging(log_level=\"WARN\")\n",
    "        .output(log_dir)\n",
    "    )\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKLNyViDkI9O"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iyMEBA0uC8ZW",
    "outputId": "b490fc96-4321-4150-b3c4-34d391389446"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AlgorithmConfig.training() got an unexpected keyword argument 'sgd_minibatch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# run everything\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m go_train(\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)            \n",
      "Cell \u001b[0;32mIn[145], line 3\u001b[0m, in \u001b[0;36mget_config\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_config\u001b[39m():\n\u001b[1;32m      2\u001b[0m     config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or \"tf\"\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runners\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# renamed param here\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_envs_per_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs_per_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# renamed param here\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrollout_fragment_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_fragment_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43msgd_minibatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msgd_minibatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m.\u001b[39mresources(\n\u001b[1;32m     16\u001b[0m             num_gpus\u001b[38;5;241m=\u001b[39mnum_gpus,\n\u001b[1;32m     17\u001b[0m             num_cpus_per_worker\u001b[38;5;241m=\u001b[39mnum_cpus_per_worker,\n\u001b[1;32m     18\u001b[0m             num_gpus_per_worker\u001b[38;5;241m=\u001b[39mnum_gpus_per_worker,\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;241m.\u001b[39mmulti_agent(\n\u001b[1;32m     21\u001b[0m             policies\u001b[38;5;241m=\u001b[39mpolicies,\n\u001b[1;32m     22\u001b[0m             policy_mapping_fn\u001b[38;5;241m=\u001b[39mpolicy_mapper,\n\u001b[1;32m     23\u001b[0m             policies_to_train\u001b[38;5;241m=\u001b[39mtrain_policy_list,\n\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;241m.\u001b[39mcallbacks(MyCallbacks)\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;241m.\u001b[39mdebugging(log_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;241m.\u001b[39moutput(log_dir)\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/ppo/ppo.py:246\u001b[0m, in \u001b[0;36mPPOConfig.training\u001b[0;34m(self, use_critic, use_gae, lambda_, use_kl_loss, kl_coeff, kl_target, vf_loss_coeff, entropy_coeff, entropy_coeff_schedule, clip_param, vf_clip_param, grad_clip, lr_schedule, vf_share_layers, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the training related configuration.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    This updated AlgorithmConfig object.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Pass kwargs onto super's `training()` method.\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_critic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotProvided:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_critic \u001b[38;5;241m=\u001b[39m use_critic\n",
      "\u001b[0;31mTypeError\u001b[0m: AlgorithmConfig.training() got an unexpected keyword argument 'sgd_minibatch_size'"
     ]
    }
   ],
   "source": [
    "def go_train(config):    \n",
    "    trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")\n",
    "    \n",
    "    if is_restore == True:\n",
    "        trainer.restore(restore_path) \n",
    "\n",
    "    g_store = ray.util.get_actor(\"g_store\")          \n",
    "    result = None\n",
    "    for i in range(num_iters):\n",
    "        result = trainer.train()       \n",
    "        print(pretty_print(result)) # includes result[\"custom_metrics\"]\n",
    "        print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
    "        print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n",
    "\n",
    "        if i % chkpt_freq == 0:\n",
    "            checkpoint = trainer.save(local_dir)\n",
    "            print(\"checkpoint saved at\", checkpoint)\n",
    "    \n",
    "    checkpoint = trainer.save(local_dir)\n",
    "    print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "    print(\"result['experiment_id']\", result[\"experiment_id\"])\n",
    "    \n",
    "    return result[\"experiment_id\"]\n",
    "    \n",
    "# run everything\n",
    "experiment_id = go_train(get_config())            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmRFarjpD7Wt"
   },
   "source": [
    "### Plot all steps.\n",
    "\n",
    "Agt_0, 1, 2 are trained agents (with PPO) while the rest are random agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "wzMc3PY_D7qv",
    "outputId": "17e774a8-2d2f-46a8-ec0b-d56894c84250"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "e0NDLLyiESgx",
    "outputId": "f16045e7-bb85-44a4-f67a-73fd1b0e861f"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"NAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "GrqwrFwsETlK",
    "outputId": "0da3cef4-aa85-47ae-c771-9d2f84b84ba6"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-1mV5YFYwkN"
   },
   "source": [
    "### Log/load last episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "khD_kiwaJVSQ",
    "outputId": "01e3109b-a024-44fc-8374-98db6be9f170"
   },
   "outputs": [],
   "source": [
    "# log_g_store(log_g_store_dir, num_agents, experiment_id)\n",
    "# load_g_store(log_g_store_dir, num_agents, experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSu7XmUSDVOE"
   },
   "source": [
    "### Plot steps from last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "rNraawhLmH68",
    "outputId": "d9d3d2b3-b345-4597-c49f-863b52355c2e"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "Lke086wkL4If",
    "outputId": "8bc0af37-3841-456f-8d1d-ba581cae5221"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"NAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "2uqneDvkL4x6",
    "outputId": "d93996d1-a637-4383-bedf-c5687e250d5b"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dL1TZkz-Wvqx"
   },
   "source": [
    "### LOB from last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR5iU78gCePl"
   },
   "outputs": [],
   "source": [
    "# g_store = ray.util.get_actor(\"g_store\")          \n",
    "# #store = ray.get(g_rere.get_storage.remote())\n",
    "\n",
    "# depth = 10\n",
    "# bid_size, bid_price, ask_size, ask_price = ray.get(g_store.get_obs_from_agt.remote(0, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPa_j4M2Z6mh"
   },
   "source": [
    "### LOB order imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2PBgQwhTBOqQ",
    "outputId": "0ecf046a-2ed1-4878-d1b4-7d827b4e8a96",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ord_imb_store = ord_imb(bid_size, ask_size)\n",
    "# plot_LOB_subplot(ord_imb_store, depth, '_ord_imb') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9Kb9NpmaHg4"
   },
   "source": [
    "### LOB sum of order imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "MXydsI2nC76g",
    "outputId": "6078375e-4473-448c-9d64-f7f19f302101"
   },
   "outputs": [],
   "source": [
    "# ord_imb_store = np.asarray(ord_imb_store)\n",
    "# sum_ord_imb_store = sum_ord_imb(ord_imb_store)\n",
    "# plot_sum_ord_imb(sum_ord_imb_store, \"sum_ord_imb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12iZkIMdaS_l"
   },
   "source": [
    "### LOB mid price (subplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YhRaxfpg_w2t",
    "outputId": "48a2657d-8c53-4a33-f8f7-949ce90fc2b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mid_price_store = mid_price(bid_price, ask_price)\n",
    "# plot_LOB_subplot(mid_price_store, depth, '_mid_price')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWPlbUqebcJi"
   },
   "source": [
    "### LOB mid prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "CZ724VMdbbWs",
    "outputId": "1a8af35b-4592-4b5d-e17d-9ddeb9f409c2"
   },
   "outputs": [],
   "source": [
    "# plot_mid_prices(mid_price_store,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JV5v-yYZdqO"
   },
   "source": [
    "### LOB bid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PS-hHA9Mx1CZ",
    "outputId": "1f3f4ea1-e607-4612-a8d8-2efd8a833bfa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(bid_size, depth, '_bid_size')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUotX7pQZiXP"
   },
   "source": [
    "### LOB ask size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iYuL9s5ex5So",
    "outputId": "4108d574-fa52-4ee6-bf24-efeeb8c7471c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(ask_size, depth, '_ask_size')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKiDC3C2ZnW4"
   },
   "source": [
    "### LOB bid price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "X2qQ-O01x5ER",
    "outputId": "90f18010-dd45-476b-cc78-dc42f613703c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(bid_price, depth, '_bid_price')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3HcZH-KZuLd"
   },
   "source": [
    "### LOB ask price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FifBnZAdx6uT",
    "outputId": "562f8092-b909-44dd-d1bd-cd3637164986",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(ask_price, depth, '_ask_price')    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CDA_NSP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
