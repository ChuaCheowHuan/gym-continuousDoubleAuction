{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f05ZH97QkoJf"
   },
   "source": [
    "# Sample training script with naive competitive self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcLSdJuUkTrX"
   },
   "source": [
    "### Switch directory in Google drive so as to import CDA env.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "PAqVG2cqjLXM",
    "outputId": "2cd035eb-6b74-4432-b82e-76b9162f439a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# %cd \"/root/ray_results/\"\n",
    "# !ls -l\n",
    "# #!rm -rf PPO_continuousDoubleAuction-v0_*\n",
    "# !ls -l\n",
    "# !pwd\n",
    "\n",
    "# %cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
    "# !ls -l\n",
    "\n",
    "# #!pip install -r requirements.txt\n",
    "\n",
    "# #!pip install tensorflow==2.2.0\n",
    "# #!pip install ray[rllib]==0.8.5\n",
    "\n",
    "# #!pip show tensorflow\n",
    "# #!pip show ray\n",
    "\n",
    "# #!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sortedcontainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall continuousDoubleAuction\n",
    "# !pip uninstall continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show continuousDoubleAuction\n",
    "# !pip show continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('gym_continuousDoubleAuction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7ZHcwBWkXVM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UW3INjDipTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports all OK.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::DeprecationWarning'\n",
    "\n",
    "import argparse\n",
    "\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
    "\n",
    "from gym_continuousDoubleAuction.train.model.model_handler import CustomRLModule\n",
    "\n",
    "from gym_continuousDoubleAuction.train.policy.policy_handler import (\n",
    "    # make_RandomPolicy, \n",
    "    # gen_policy, \n",
    "    # set_agents_policies, \n",
    "    # create_train_policy_list,\n",
    "    create_multi_agent_config,\n",
    "    policy_mapping_fn,\n",
    "    # create_and_train_algorithm,\n",
    ")\n",
    "from gym_continuousDoubleAuction.train.weight.weight_handler import (\n",
    "    get_trained_policies_name, get_max_reward_ind, cp_weight)\n",
    "from gym_continuousDoubleAuction.train.storage.store_handler import storage\n",
    "from gym_continuousDoubleAuction.train.callbk.callbk_handler import store_eps_hist_data\n",
    "from gym_continuousDoubleAuction.train.logger.log_handler import (\n",
    "    create_dir, log_g_store, load_g_store)\n",
    "from gym_continuousDoubleAuction.train.plotter.plot_handler import (\n",
    "    plot_storage, plot_LOB_subplot, plot_sum_ord_imb, plot_mid_prices)\n",
    "from gym_continuousDoubleAuction.train.helper.helper import (\n",
    "    ord_imb, sum_ord_imb, mid_price)\n",
    "\n",
    "print(f'Imports all OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDnpi8k5kbYo"
   },
   "source": [
    "### Global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "UqzjVWUsPykm",
    "outputId": "18a6a936-1477-4e90-98c6-cdf7be851dc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder creation failed or folder already exists: results/\n",
      "Folder creation failed or folder already exists: results/log_g_store/\n",
      "get_action_space, agent_id: agent_3\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "['agent_3', 'agent_1', 'agent_2', 'agent_0']\n",
      "Box(-1.0, 1.0, (40,), float32)\n",
      "get_action_space, agent_id: agent_3\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "Discrete(3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 00:28:15,134\tWARNING services.py:2152 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.73gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-27 00:28:16,185\tINFO worker.py:1917 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m It looks like you're creating a detached actor in an anonymous namespace. In order to access this actor in the future, you will need to explicitly connect to this namespace with ray.init(namespace=\"8c349150-0abc-4a50-8474-bcf6be00ef74\", ...)\n"
     ]
    }
   ],
   "source": [
    "# CDA_env args\n",
    "num_agents = 4\n",
    "num_trained_agent = 2 # \n",
    "num_policies = num_agents # Each agent is using a separate policy\n",
    "num_of_traders = num_agents\n",
    "tape_display_length = 10 \n",
    "tick_size = 1\n",
    "init_cash = 1000000\n",
    "max_step = 4096 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n",
    "is_render = False \n",
    "\n",
    "# RLlib config \n",
    "# train_policy_list = create_train_policy_list(num_trained_agent, \"policy_\")\n",
    "#num_cpus = 0.25                                \n",
    "num_gpus = 0.75 #0                       \n",
    "num_cpus_per_worker = 0.25                                \n",
    "num_gpus_per_worker = 0\n",
    "num_workers = 2\n",
    "num_envs_per_worker = 4\n",
    "batch_mode = \"complete_episodes\" \n",
    "rollout_fragment_length = 128\n",
    "train_batch_size = max_step\n",
    "sgd_minibatch_size = 256\n",
    "num_iters = 2\n",
    "\n",
    "# log_base_dir = \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/results/\"\n",
    "log_base_dir = \"results/\"\n",
    "log_dir = log_base_dir + \"ray_results/\"\n",
    "\n",
    "# Chkpt & restore\n",
    "local_dir = log_base_dir + \"chkpt/\"\n",
    "chkpt_freq = 10\n",
    "chkpt = 320\n",
    "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
    "is_restore = True # True / False\n",
    "\n",
    "# log & load \n",
    "log_g_store_dir = log_base_dir + \"log_g_store/\"\n",
    "create_dir(log_base_dir)    \n",
    "create_dir(log_g_store_dir)    \n",
    "\n",
    "# Environment configuration\n",
    "env_config = {\n",
    "    \"num_of_agents\": num_agents,\n",
    "    \"init_cash\": 10000,\n",
    "    \"tick_size\": 1,\n",
    "    \"tape_display_length\": 10,\n",
    "    \"max_step\": 1000,\n",
    "    \"is_render\": False\n",
    "}\n",
    "\n",
    "# get obs & act spaces from dummy CDA env\n",
    "# single_CDA_env = continuousDoubleAuctionEnv(\n",
    "#     num_of_traders, \n",
    "#     init_cash, \n",
    "#     tick_size, \n",
    "#     tape_display_length, \n",
    "#     max_step, \n",
    "#     is_render)\n",
    "single_CDA_env = continuousDoubleAuctionEnv(env_config)\n",
    "obs_space = single_CDA_env.get_observation_space(single_CDA_env.agents[0])\n",
    "act_space = single_CDA_env.get_action_space(single_CDA_env.agents[0])\n",
    "print(single_CDA_env.agents)  # Should be a non-empty list\n",
    "print(single_CDA_env.get_observation_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "print(single_CDA_env.get_action_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return continuousDoubleAuctionEnv(env_config)\n",
    "\n",
    "# Register environment with ray.tune - this is the key fix!\n",
    "tune.register_env(\"continuousDoubleAuction-v0\", env_creator)\n",
    "\n",
    "# register custom model (neural network)\n",
    "ModelCatalog.register_custom_model(\"model_disc\", CustomRLModule) \n",
    "\n",
    "ray.shutdown()\n",
    "# start ray\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    log_to_driver=True,\n",
    "    num_cpus=2,\n",
    "    dashboard_host=\"127.0.0.1\",  # replaces webui_host\n",
    "    dashboard_port=8265,          # default port; replaces webui_port\n",
    "    # include_dashboard=True,        # default True\n",
    "    include_dashboard=False,        # default True\n",
    "\n",
    ")\n",
    "\n",
    "# Global storage, a ray actor that run on it's own process & it needs to be declared after ray.init().\n",
    "g_store = storage.options(name=\"g_store\", lifetime=\"detached\").remote(num_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cknk9Cnoke_u"
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policies: {'policy_0': <ray.rllib.policy.policy.PolicySpec object at 0x7d4ba1eb7940>, 'policy_1': <ray.rllib.policy.policy.PolicySpec object at 0x7d4ba1eb78e0>, 'policy_2': <ray.rllib.policy.policy.PolicySpec object at 0x7d4ba1eb7880>, 'policy_3': <ray.rllib.policy.policy.PolicySpec object at 0x7d4ba1eb7af0>}\n",
      "policies_to_train: ['policy_0', 'policy_1']\n"
     ]
    }
   ],
   "source": [
    "policies, policies_to_train = create_multi_agent_config(\n",
    "    obs_space, act_space, num_agents, num_trained_agents=num_trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4x0UTsxUhys"
   },
   "source": [
    "### Call back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEnp5UpxkDve"
   },
   "source": [
    "### RLlib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "\n",
    "def get_config():\n",
    "    config = (\n",
    "        PPOConfig()            \n",
    "        .environment(\n",
    "            \"continuousDoubleAuction-v0\",\n",
    "            # continuousDoubleAuctionEnv,\n",
    "            # env_config={\n",
    "            #     \"num_of_agents\": num_of_traders, \n",
    "            #     \"init_cash\": init_cash, \n",
    "            #     \"tick_size\": tick_size, \n",
    "            #     \"tape_display_length\": tape_display_length,\n",
    "            #     \"max_step\": max_step - 1, \n",
    "            #     \"is_render\": is_render,\n",
    "            # }\n",
    "            env_config=env_config,\n",
    "            # env_config={\"disable_env_checker\": True},            \n",
    "        ) \n",
    "        .multi_agent(\n",
    "            policies=policies,\n",
    "            policy_mapping_fn=policy_mapping_fn,               \n",
    "            policies_to_train=policies_to_train,            \n",
    "        )\n",
    "        # .training(\n",
    "        #     model={\n",
    "        #         \"custom_model\": CustomLSTMRLModule,\n",
    "        #         # \"custom_model_config\": {\n",
    "        #         #     \"fcnet_hiddens\": [256, 256],  # Neural network architecture\n",
    "        #         #     \"fcnet_activation\": \"relu\",\n",
    "        #         # },\n",
    "        #     }\n",
    "        # )            \n",
    "        .env_runners(\n",
    "            # num_env_runners=num_workers,\n",
    "            num_env_runners=0,           \n",
    "            # num_envs_per_env_runner=num_envs_per_worker,\n",
    "            # rollout_fragment_length=rollout_fragment_length,\n",
    "            # batch_mode=batch_mode,\n",
    "        )\n",
    "        .learners(\n",
    "            num_learners=0,  # Typically 1 learner unless using distributed training\n",
    "            num_gpus_per_learner=num_gpus,  # Trainer GPU allocation\n",
    "            # num_cpus_per_learner=num_cpus_per_worker,\n",
    "        )\n",
    "        .training(\n",
    "            train_batch_size_per_learner=train_batch_size,\n",
    "            train_batch_size=1000,\n",
    "            num_epochs=5,            \n",
    "        )\n",
    "        # .callbacks(MyCallbacks)\n",
    "        # .output_dir(log_dir)\n",
    "        .framework(\"torch\")  # Explicitly set framework if needed\n",
    "        .debugging(log_level=\"DEBUG\")\n",
    "        # .api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)\n",
    "    )\n",
    "\n",
    "    # # Optional: Configure resources more granularly if needed\n",
    "    # if num_gpus_per_worker > 0:\n",
    "    #     config.env_runners(\n",
    "    #         num_gpus_per_env_runner=num_gpus_per_worker\n",
    "    #     )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKLNyViDkI9O"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 00:28:16,530\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-07-27 00:28:16,532\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset (observations): {'agent_0': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), 'agent_1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), 'agent_2': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), 'agent_3': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.])}\n",
      "get_action_space, agent_id: agent_0\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "get_action_space, agent_id: agent_1\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "get_action_space, agent_id: agent_2\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "get_action_space, agent_id: agent_3\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "step: self.terminateds {'agent_3': False, 'agent_1': False, 'agent_2': False, 'agent_0': False, '__all__': False}\n",
      "step: self.truncateds {'agent_3': False, 'agent_1': False, 'agent_2': False, 'agent_0': False, '__all__': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-07-27 00:28:16,741\tINFO connector_pipeline_v2.py:272 -- Added AddObservationsFromEpisodesToBatch to the end of EnvToModulePipeline.\n",
      "2025-07-27 00:28:16,750\tINFO connector_pipeline_v2.py:272 -- Added AddTimeDimToBatchAndZeroPad to the end of EnvToModulePipeline.\n",
      "2025-07-27 00:28:16,760\tINFO connector_pipeline_v2.py:272 -- Added AddStatesFromEpisodesToBatch to the end of EnvToModulePipeline.\n",
      "2025-07-27 00:28:16,779\tINFO connector_pipeline_v2.py:272 -- Added AgentToModuleMapping to the end of EnvToModulePipeline.\n",
      "2025-07-27 00:28:16,789\tINFO connector_pipeline_v2.py:272 -- Added BatchIndividualItems to the end of EnvToModulePipeline.\n",
      "2025-07-27 00:28:16,798\tINFO connector_pipeline_v2.py:272 -- Added NumpyToTensor to the end of EnvToModulePipeline.\n",
      "2025-07-27 00:28:16,800\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-07-27 00:28:16,810\tINFO connector_pipeline_v2.py:258 -- Added RemoveSingleTsTimeRankFromBatch to the beginning of ModuleToEnvPipeline.\n",
      "2025-07-27 00:28:16,811\tINFO connector_pipeline_v2.py:258 -- Added ModuleToAgentUnmapping to the beginning of ModuleToEnvPipeline.\n",
      "2025-07-27 00:28:16,811\tINFO connector_pipeline_v2.py:258 -- Added UnBatchToIndividualItems to the beginning of ModuleToEnvPipeline.\n",
      "2025-07-27 00:28:16,812\tINFO connector_pipeline_v2.py:258 -- Added TensorToNumpy to the beginning of ModuleToEnvPipeline.\n",
      "2025-07-27 00:28:16,812\tINFO connector_pipeline_v2.py:258 -- Added GetActions to the beginning of ModuleToEnvPipeline.\n",
      "2025-07-27 00:28:16,827\tINFO connector_pipeline_v2.py:272 -- Added NormalizeAndClipActions to the end of ModuleToEnvPipeline.\n",
      "2025-07-27 00:28:16,828\tINFO connector_pipeline_v2.py:272 -- Added ListifyDataForVectorEnv to the end of ModuleToEnvPipeline.\n",
      "2025-07-27 00:28:16,829\tINFO env_runner_group.py:320 -- Inferred observation/action spaces from remote worker (local worker has no env): {'__env__': (None, None), '__env_single__': (Dict('agent_0': Box(-1.0, 1.0, (40,), float32), 'agent_1': Box(-1.0, 1.0, (40,), float32), 'agent_2': Box(-1.0, 1.0, (40,), float32), 'agent_3': Box(-1.0, 1.0, (40,), float32)), Dict('agent_0': Discrete(3), 'agent_1': Discrete(3), 'agent_2': Discrete(3), 'agent_3': Discrete(3))), 'policy_0': (Box(-1.0, 1.0, (40,), float32), Discrete(3)), 'policy_1': (Box(-1.0, 1.0, (40,), float32), Discrete(3)), 'policy_2': (Box(-1.0, 1.0, (40,), float32), Discrete(3)), 'policy_3': (Box(-1.0, 1.0, (40,), float32), Discrete(3))}\n",
      "2025-07-27 00:28:16,863\tINFO connector_pipeline_v2.py:272 -- Added AddObservationsFromEpisodesToBatch to the end of LearnerConnectorPipeline.\n",
      "2025-07-27 00:28:16,864\tINFO connector_pipeline_v2.py:272 -- Added AddColumnsFromEpisodesToTrainBatch to the end of LearnerConnectorPipeline.\n",
      "2025-07-27 00:28:16,873\tINFO connector_pipeline_v2.py:272 -- Added AddTimeDimToBatchAndZeroPad to the end of LearnerConnectorPipeline.\n",
      "2025-07-27 00:28:16,882\tINFO connector_pipeline_v2.py:272 -- Added AddStatesFromEpisodesToBatch to the end of LearnerConnectorPipeline.\n",
      "2025-07-27 00:28:16,891\tINFO connector_pipeline_v2.py:272 -- Added AgentToModuleMapping to the end of LearnerConnectorPipeline.\n",
      "2025-07-27 00:28:16,901\tINFO connector_pipeline_v2.py:272 -- Added BatchIndividualItems to the end of LearnerConnectorPipeline.\n",
      "2025-07-27 00:28:16,910\tINFO connector_pipeline_v2.py:272 -- Added NumpyToTensor to the end of LearnerConnectorPipeline.\n",
      "2025-07-27 00:28:17,810\tINFO connector_pipeline_v2.py:258 -- Added AddOneTsToEpisodesAndTruncate to the beginning of LearnerConnectorPipeline.\n",
      "2025-07-27 00:28:17,845\tINFO connector_pipeline_v2.py:272 -- Added GeneralAdvantageEstimation to the end of LearnerConnectorPipeline.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_action_space, agent_id: agent_3\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "get_action_space, agent_id: agent_1\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "get_action_space, agent_id: agent_2\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "get_action_space, agent_id: agent_0\n",
      "get_action_space, self.action_space[agent_id]: Discrete(3)\n",
      "reset (observations): {'agent_0': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), 'agent_1': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), 'agent_2': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), 'agent_3': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.])}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "`observation` [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] does NOT fit SingleAgentEpisode's observation_space: Box(-1.0, 1.0, (40,), float32)!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# run everything\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[43mgo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m            \n",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m, in \u001b[0;36mgo_train\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# if is_restore == True:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     trainer.restore(restore_path) \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# g_store = ray.util.get_actor(\"g_store\")          \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# result = None\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[0;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m       \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     print(pretty_print(result)) # includes result[\"custom_metrics\"]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     print(\"training loop = {} of {}\".format(i + 1, num_iters))            \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# return result[\"experiment_id\"]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py:330\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    329\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m skipped \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py:327\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    329\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:1035\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[0;32m-> 1035\u001b[0m         train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         (\n\u001b[1;32m   1038\u001b[0m             train_results,\n\u001b[1;32m   1039\u001b[0m             train_iter_ctx,\n\u001b[1;32m   1040\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_old_api_stack()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:3309\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3307\u001b[0m \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mlog_time((TIMERS, TRAINING_STEP_TIMER)):\n\u001b[0;32m-> 3309\u001b[0m     training_step_return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3310\u001b[0m     has_run_once \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3312\u001b[0m \u001b[38;5;66;03m# On the new API stack, results should NOT be returned anymore as\u001b[39;00m\n\u001b[1;32m   3313\u001b[0m \u001b[38;5;66;03m# a dict, but purely logged through the `MetricsLogger` API. This\u001b[39;00m\n\u001b[1;32m   3314\u001b[0m \u001b[38;5;66;03m# way, we make sure to never miss a single stats/counter/timer\u001b[39;00m\n\u001b[1;32m   3315\u001b[0m \u001b[38;5;66;03m# when calling `self.training_step()` more than once within the same\u001b[39;00m\n\u001b[1;32m   3316\u001b[0m \u001b[38;5;66;03m# iteration.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/ppo/ppo.py:407\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m     episodes, env_runner_results \u001b[38;5;241m=\u001b[39m synchronous_parallel_sample(\n\u001b[1;32m    398\u001b[0m         worker_set\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_group,\n\u001b[1;32m    399\u001b[0m         max_agent_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtotal_train_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m         _return_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    405\u001b[0m     )\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     episodes, env_runner_results \u001b[38;5;241m=\u001b[39m \u001b[43msynchronous_parallel_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runner_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_env_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_timeout_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_timeout_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_uses_new_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_return_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Return early if all our workers failed.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episodes:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/execution/rollout_ops.py:101\u001b[0m, in \u001b[0;36msynchronous_parallel_sample\u001b[0;34m(worker_set, max_agent_steps, max_env_steps, concat, sample_timeout_s, random_actions, _uses_new_env_runners, _return_metrics)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (max_agent_or_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m agent_or_env_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     95\u001b[0m     max_agent_or_env_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m agent_or_env_steps \u001b[38;5;241m<\u001b[39m max_agent_or_env_steps\n\u001b[1;32m     97\u001b[0m ):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# No remote workers in the set -> Use local worker for collecting\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# samples.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worker_set\u001b[38;5;241m.\u001b[39mnum_remote_workers() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 101\u001b[0m         sampled_data \u001b[38;5;241m=\u001b[39m [\u001b[43mworker_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrandom_action_kwargs\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _return_metrics:\n\u001b[1;32m    103\u001b[0m             stats_dicts \u001b[38;5;241m=\u001b[39m [worker_set\u001b[38;5;241m.\u001b[39mlocal_env_runner\u001b[38;5;241m.\u001b[39mget_metrics()]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/multi_agent_env_runner.py:222\u001b[0m, in \u001b[0;36mMultiAgentEnvRunner.sample\u001b[0;34m(self, num_timesteps, num_episodes, explore, random_actions, force_reset)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Sample n timesteps.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_timesteps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_reset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_reset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Sample m episodes.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_episodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/multi_agent_env_runner.py:277\u001b[0m, in \u001b[0;36mMultiAgentEnvRunner._sample\u001b[0;34m(self, num_timesteps, num_episodes, explore, random_actions, force_reset)\u001b[0m\n\u001b[1;32m    274\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_episodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs)]\n\u001b[1;32m    276\u001b[0m shared_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset_envs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# We just reset the env. Don't have to force this again in the next\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# call to `self._sample_timesteps()`.\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_initial_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/multi_agent_env_runner.py:552\u001b[0m, in \u001b[0;36mMultiAgentEnvRunner._reset_envs\u001b[0;34m(self, episodes, shared_data, explore)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Set the initial obs and infos in the episodes.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m--> 552\u001b[0m     \u001b[43mepisodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_env_reset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfos\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Run the env-to-module connector to make sure the reset-obs/infos have\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# properly been processed (if applicable).\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_to_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/multi_agent_episode.py:338\u001b[0m, in \u001b[0;36mMultiAgentEpisode.add_env_reset\u001b[0;34m(self, observations, infos)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_episodes[agent_id] \u001b[38;5;241m=\u001b[39m SingleAgentEpisode(\n\u001b[1;32m    331\u001b[0m             agent_id\u001b[38;5;241m=\u001b[39magent_id,\n\u001b[1;32m    332\u001b[0m             module_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_for(agent_id),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m             action_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mget(agent_id),\n\u001b[1;32m    336\u001b[0m         )\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# Add initial observations (and infos) to the agent's episode.\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_episodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_env_reset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Validate our data.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/single_agent_episode.py:377\u001b[0m, in \u001b[0;36mSingleAgentEpisode.add_env_reset\u001b[0;34m(self, observation, infos)\u001b[0m\n\u001b[1;32m    374\u001b[0m infos \u001b[38;5;241m=\u001b[39m infos \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mcontains(observation), (\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`observation` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does NOT fit SingleAgentEpisode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation_space: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m     )\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfos\u001b[38;5;241m.\u001b[39mappend(infos)\n",
      "\u001b[0;31mAssertionError\u001b[0m: `observation` [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] does NOT fit SingleAgentEpisode's observation_space: Box(-1.0, 1.0, (40,), float32)!"
     ]
    }
   ],
   "source": [
    "def go_train(config):    \n",
    "    # trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")      \n",
    "    # algo = config.build()   \n",
    "    algo = config.build()\n",
    "    \n",
    "    # if is_restore == True:\n",
    "    #     trainer.restore(restore_path) \n",
    "\n",
    "    # g_store = ray.util.get_actor(\"g_store\")          \n",
    "    # result = None\n",
    "    for i in range(num_iters):\n",
    "        result = algo.train()       \n",
    "\n",
    "    #     print(pretty_print(result)) # includes result[\"custom_metrics\"]\n",
    "    #     print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
    "    #     print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n",
    "\n",
    "    #     if i % chkpt_freq == 0:\n",
    "    #         checkpoint = algo.save(local_dir)\n",
    "    #         print(\"checkpoint saved at\", checkpoint)\n",
    "    \n",
    "    # checkpoint = algo.save(local_dir)\n",
    "    # print(\"checkpoint saved at\", checkpoint)\n",
    "    # print(\"result['experiment_id']\", result[\"experiment_id\"])\n",
    "\n",
    "    # return result[\"experiment_id\"]\n",
    "    return None\n",
    "    \n",
    "# run everything\n",
    "experiment_id = go_train(get_config())            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmRFarjpD7Wt"
   },
   "source": [
    "### Plot all steps.\n",
    "\n",
    "Agt_0, 1, 2 are trained agents (with PPO) while the rest are random agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "wzMc3PY_D7qv",
    "outputId": "17e774a8-2d2f-46a8-ec0b-d56894c84250"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "e0NDLLyiESgx",
    "outputId": "f16045e7-bb85-44a4-f67a-73fd1b0e861f"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"NAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "GrqwrFwsETlK",
    "outputId": "0da3cef4-aa85-47ae-c771-9d2f84b84ba6"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-1mV5YFYwkN"
   },
   "source": [
    "### Log/load last episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "khD_kiwaJVSQ",
    "outputId": "01e3109b-a024-44fc-8374-98db6be9f170"
   },
   "outputs": [],
   "source": [
    "# log_g_store(log_g_store_dir, num_agents, experiment_id)\n",
    "# load_g_store(log_g_store_dir, num_agents, experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSu7XmUSDVOE"
   },
   "source": [
    "### Plot steps from last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "rNraawhLmH68",
    "outputId": "d9d3d2b3-b345-4597-c49f-863b52355c2e"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "Lke086wkL4If",
    "outputId": "8bc0af37-3841-456f-8d1d-ba581cae5221"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"NAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "2uqneDvkL4x6",
    "outputId": "d93996d1-a637-4383-bedf-c5687e250d5b"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dL1TZkz-Wvqx"
   },
   "source": [
    "### LOB from last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR5iU78gCePl"
   },
   "outputs": [],
   "source": [
    "# g_store = ray.util.get_actor(\"g_store\")          \n",
    "# #store = ray.get(g_rere.get_storage.remote())\n",
    "\n",
    "# depth = 10\n",
    "# bid_size, bid_price, ask_size, ask_price = ray.get(g_store.get_obs_from_agt.remote(0, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPa_j4M2Z6mh"
   },
   "source": [
    "### LOB order imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2PBgQwhTBOqQ",
    "outputId": "0ecf046a-2ed1-4878-d1b4-7d827b4e8a96",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ord_imb_store = ord_imb(bid_size, ask_size)\n",
    "# plot_LOB_subplot(ord_imb_store, depth, '_ord_imb') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9Kb9NpmaHg4"
   },
   "source": [
    "### LOB sum of order imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "MXydsI2nC76g",
    "outputId": "6078375e-4473-448c-9d64-f7f19f302101"
   },
   "outputs": [],
   "source": [
    "# ord_imb_store = np.asarray(ord_imb_store)\n",
    "# sum_ord_imb_store = sum_ord_imb(ord_imb_store)\n",
    "# plot_sum_ord_imb(sum_ord_imb_store, \"sum_ord_imb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12iZkIMdaS_l"
   },
   "source": [
    "### LOB mid price (subplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YhRaxfpg_w2t",
    "outputId": "48a2657d-8c53-4a33-f8f7-949ce90fc2b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mid_price_store = mid_price(bid_price, ask_price)\n",
    "# plot_LOB_subplot(mid_price_store, depth, '_mid_price')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWPlbUqebcJi"
   },
   "source": [
    "### LOB mid prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "CZ724VMdbbWs",
    "outputId": "1a8af35b-4592-4b5d-e17d-9ddeb9f409c2"
   },
   "outputs": [],
   "source": [
    "# plot_mid_prices(mid_price_store,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JV5v-yYZdqO"
   },
   "source": [
    "### LOB bid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PS-hHA9Mx1CZ",
    "outputId": "1f3f4ea1-e607-4612-a8d8-2efd8a833bfa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(bid_size, depth, '_bid_size')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUotX7pQZiXP"
   },
   "source": [
    "### LOB ask size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iYuL9s5ex5So",
    "outputId": "4108d574-fa52-4ee6-bf24-efeeb8c7471c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(ask_size, depth, '_ask_size')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKiDC3C2ZnW4"
   },
   "source": [
    "### LOB bid price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "X2qQ-O01x5ER",
    "outputId": "90f18010-dd45-476b-cc78-dc42f613703c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(bid_price, depth, '_bid_price')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3HcZH-KZuLd"
   },
   "source": [
    "### LOB ask price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FifBnZAdx6uT",
    "outputId": "562f8092-b909-44dd-d1bd-cd3637164986",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(ask_price, depth, '_ask_price')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CDA_NSP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
