{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f05ZH97QkoJf"
   },
   "source": [
    "# Sample training script with naive competitive self-play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcLSdJuUkTrX"
   },
   "source": [
    "### Switch directory in Google drive so as to import CDA env.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "PAqVG2cqjLXM",
    "outputId": "2cd035eb-6b74-4432-b82e-76b9162f439a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# %cd \"/root/ray_results/\"\n",
    "# !ls -l\n",
    "# #!rm -rf PPO_continuousDoubleAuction-v0_*\n",
    "# !ls -l\n",
    "# !pwd\n",
    "\n",
    "# %cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
    "# !ls -l\n",
    "\n",
    "# #!pip install -r requirements.txt\n",
    "\n",
    "# #!pip install tensorflow==2.2.0\n",
    "# #!pip install ray[rllib]==0.8.5\n",
    "\n",
    "# #!pip show tensorflow\n",
    "# #!pip show ray\n",
    "\n",
    "# #!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sortedcontainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall continuousDoubleAuction\n",
    "# !pip uninstall continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show continuousDoubleAuction\n",
    "# !pip show continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('gym_continuousDoubleAuction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7ZHcwBWkXVM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UW3INjDipTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports all OK.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "# from ray.rllib.agents.ppo import ppo\n",
    "# from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "# from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "# from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
    "\n",
    "# from gym_continuousDoubleAuction.train.model.model_handler import CustomModel_1\n",
    "from gym_continuousDoubleAuction.train.model.model_handler import CustomLSTMRLModule\n",
    "\n",
    "from gym_continuousDoubleAuction.train.policy.policy_handler import (\n",
    "    make_RandomPolicy, \n",
    "    # gen_policy, \n",
    "    # set_agents_policies, \n",
    "    # create_train_policy_list,\n",
    "    create_multi_agent_config,\n",
    "    policy_mapping_fn,\n",
    "    create_and_train_algorithm,\n",
    ")\n",
    "from gym_continuousDoubleAuction.train.weight.weight_handler import (\n",
    "    get_trained_policies_name, get_max_reward_ind, cp_weight)\n",
    "from gym_continuousDoubleAuction.train.storage.store_handler import storage\n",
    "from gym_continuousDoubleAuction.train.callbk.callbk_handler import store_eps_hist_data\n",
    "from gym_continuousDoubleAuction.train.logger.log_handler import (\n",
    "    create_dir, log_g_store, load_g_store)\n",
    "from gym_continuousDoubleAuction.train.plotter.plot_handler import (\n",
    "    plot_storage, plot_LOB_subplot, plot_sum_ord_imb, plot_mid_prices)\n",
    "from gym_continuousDoubleAuction.train.helper.helper import (\n",
    "    ord_imb, sum_ord_imb, mid_price)\n",
    "\n",
    "# tf = try_import_tf()\n",
    "print(f'Imports all OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDnpi8k5kbYo"
   },
   "source": [
    "### Global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cknk9Cnoke_u"
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "UqzjVWUsPykm",
    "outputId": "18a6a936-1477-4e90-98c6-cdf7be851dc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder creation failed or folder already exists: results/\n",
      "Folder creation failed or folder already exists: results/log_g_store/\n",
      "['agent_0', 'agent_1']\n",
      "{'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}\n",
      "{'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 13:59:50,579\tWARNING services.py:2152 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.89gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-22 13:59:51,732\tINFO worker.py:1917 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m It looks like you're creating a detached actor in an anonymous namespace. In order to access this actor in the future, you will need to explicitly connect to this namespace with ray.init(namespace=\"c75b8f4a-49cb-4835-920e-5979783fa8be\", ...)\n"
     ]
    }
   ],
   "source": [
    "# CDA_env args\n",
    "num_agents = 9\n",
    "num_trained_agent = 3 # \n",
    "num_policies = num_agents # Each agent is using a separate policy\n",
    "num_of_traders = num_agents\n",
    "tape_display_length = 10 \n",
    "tick_size = 1\n",
    "init_cash = 1000000\n",
    "max_step = 4096 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n",
    "is_render = False \n",
    "\n",
    "# RLlib config \n",
    "# train_policy_list = create_train_policy_list(num_trained_agent, \"policy_\")\n",
    "#num_cpus = 0.25                                \n",
    "num_gpus = 0.75 #0                       \n",
    "num_cpus_per_worker = 0.25                                \n",
    "num_gpus_per_worker = 0\n",
    "num_workers = 2\n",
    "num_envs_per_worker = 4\n",
    "batch_mode = \"complete_episodes\" \n",
    "rollout_fragment_length = 128\n",
    "train_batch_size = max_step\n",
    "sgd_minibatch_size = 256\n",
    "num_iters = 2\n",
    "\n",
    "# log_base_dir = \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/results/\"\n",
    "log_base_dir = \"results/\"\n",
    "log_dir = log_base_dir + \"ray_results/\"\n",
    "\n",
    "# Chkpt & restore\n",
    "local_dir = log_base_dir + \"chkpt/\"\n",
    "chkpt_freq = 10\n",
    "chkpt = 320\n",
    "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
    "is_restore = True # True / False\n",
    "\n",
    "# log & load \n",
    "log_g_store_dir = log_base_dir + \"log_g_store/\"\n",
    "create_dir(log_base_dir)    \n",
    "create_dir(log_g_store_dir)    \n",
    "\n",
    "# Environment configuration\n",
    "env_config = {\n",
    "    \"num_of_agents\": 2,\n",
    "    \"init_cash\": 10000,\n",
    "    \"tick_size\": 1,\n",
    "    \"tape_display_length\": 10,\n",
    "    \"max_step\": 1000,\n",
    "    \"is_render\": False\n",
    "}\n",
    "\n",
    "# get obs & act spaces from dummy CDA env\n",
    "# single_CDA_env = continuousDoubleAuctionEnv(\n",
    "#     num_of_traders, \n",
    "#     init_cash, \n",
    "#     tick_size, \n",
    "#     tape_display_length, \n",
    "#     max_step, \n",
    "#     is_render)\n",
    "single_CDA_env = continuousDoubleAuctionEnv(env_config)\n",
    "obs_space = single_CDA_env.get_observation_space(single_CDA_env.agents[0])\n",
    "act_space = single_CDA_env.get_action_space(single_CDA_env.agents[0])\n",
    "print(single_CDA_env.agents)  # Should be a non-empty list\n",
    "print(single_CDA_env.get_observation_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "print(single_CDA_env.get_action_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return continuousDoubleAuctionEnv(env_config)\n",
    "\n",
    "# Register environment with ray.tune - this is the key fix!\n",
    "tune.register_env(\"continuousDoubleAuction-v0\", env_creator)\n",
    "\n",
    "# register custom model (neural network)\n",
    "ModelCatalog.register_custom_model(\"model_disc\", CustomLSTMRLModule) \n",
    "\n",
    "ray.shutdown()\n",
    "# start ray\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    log_to_driver=True,\n",
    "    num_cpus=2,\n",
    "    dashboard_host=\"127.0.0.1\",  # replaces webui_host\n",
    "    dashboard_port=8265,          # default port; replaces webui_port\n",
    "    # include_dashboard=True,        # default True\n",
    "    include_dashboard=False,        # default True\n",
    "\n",
    ")\n",
    "\n",
    "# Global storage, a ray actor that run on it's own process & it needs to be declared after ray.init().\n",
    "# g_store = storage.options(name=\"g_store\", detached=True).remote(num_agents)\n",
    "g_store = storage.options(name=\"g_store\", lifetime=\"detached\").remote(num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "QN5IfMMvP4VA",
    "outputId": "7ff317cf-ab1b-49a1-dd83-24818635e8be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Dictionary of policies\n",
    "# policies = {\"policy_{}\".format(i): gen_policy(i, obs_space, act_space) for i in range(num_policies)}\n",
    "# set_agents_policies(policies, obs_space, act_space, num_agents, num_trained_agent)\n",
    "# policy_ids = list(policies.keys())\n",
    "\n",
    "# def policy_mapper(agent_id):\n",
    "#     \"\"\"\n",
    "#     Required in RLlib config.\n",
    "#     \"\"\"\n",
    "#     for i in range(num_agents):\n",
    "#         if agent_id == i:            \n",
    "#             return \"policy_{}\".format(i)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policies: {'policy_0': (None, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_1': (None, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_2': (None, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {'model': {'custom_model': 'model_disc'}, 'gamma': 0.99}), 'policy_3': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {}), 'policy_4': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {}), 'policy_5': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {}), 'policy_6': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {}), 'policy_7': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {}), 'policy_8': (<class 'gym_continuousDoubleAuction.train.policy.policy_handler.make_RandomPolicy.<locals>.RandomPolicy'>, {'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, {'agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))}, {})}\n",
      "policies_to_train: ['policy_0', 'policy_1', 'policy_2']\n"
     ]
    }
   ],
   "source": [
    "policies, policies_to_train = create_multi_agent_config(\n",
    "    obs_space, act_space, num_agents, num_trained_agents=num_trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4x0UTsxUhys"
   },
   "source": [
    "### Call back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqW_h3UrRs4z"
   },
   "outputs": [],
   "source": [
    "class MyCallbacks(DefaultCallbacks):\n",
    "    # def on_episode_start(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                      policies: Dict[str, Policy],\n",
    "    #                      episode: MultiAgentEpisode, **kwargs):\n",
    "    def on_episode_start(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        print(f\"Episode {episode.episode_id} started.\")\n",
    "        \"\"\"\n",
    "        info[\"episode\"] is a MultiAgentEpisode object.\n",
    "\n",
    "        user_data dicts at 100000 items max, will auto replace old with new item at 1st index.\n",
    "        hist_data dicts at 100 items max, will auto replace old with new item at 1st index.\n",
    "        \"\"\"\n",
    "        #print(\"on_episode_start {}, _agent_to_policy {}\".format(episode.episode_id, episode._agent_to_policy))\n",
    "\n",
    "        prefix = \"agt_\"\n",
    "        for i in range(num_agents):\n",
    "            episode.user_data[prefix + str(i) + \"_obs\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_act\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_reward\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_NAV\"] = []\n",
    "            episode.user_data[prefix + str(i) + \"_num_trades\"] = []\n",
    "\n",
    "            episode.hist_data[prefix + str(i) + \"_reward\"] = []\n",
    "            episode.hist_data[prefix + str(i) + \"_NAV\"] = []\n",
    "            episode.hist_data[prefix + str(i) + \"_num_trades\"] = []\n",
    "\n",
    "    # def on_episode_step(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                     episode: MultiAgentEpisode, **kwargs):\n",
    "    def on_episode_step(self, *, worker, base_env, episode, **kwargs):\n",
    "        # Example: track something each step\n",
    "        episode.user_data[\"my_metric\"].append(1)        \n",
    "        \"\"\"\n",
    "        pole_angle = abs(episode.last_observation_for()[2])\n",
    "        raw_angle = abs(episode.last_raw_obs_for()[2])\n",
    "        assert pole_angle == raw_angle\n",
    "        episode.user_data[\"pole_angles\"].append(pole_angle)\n",
    "        \"\"\"\n",
    "\n",
    "        prefix = \"agt_\"\n",
    "        for i in range(num_agents):\n",
    "            obs = episode.last_raw_obs_for(i)\n",
    "            #obs = episode.last_observation_for(i)\n",
    "            act = episode.last_action_for(i)\n",
    "            reward = episode.last_info_for(i).get(\"reward\")\n",
    "            NAV = episode.last_info_for(i).get(\"NAV\")\n",
    "            NAV = None if NAV is None else float(NAV)\n",
    "            num_trades = episode.last_info_for(i).get(\"num_trades\")\n",
    "        \n",
    "            if reward is None:      # goto next agent.\n",
    "                continue\n",
    "\n",
    "            episode.user_data[prefix + str(i) + \"_obs\"].append(obs)    \n",
    "            episode.user_data[prefix + str(i) + \"_act\"].append(act)    \n",
    "            episode.user_data[prefix + str(i) + \"_reward\"].append(reward)    \n",
    "            episode.user_data[prefix + str(i) + \"_NAV\"].append(NAV)    \n",
    "            episode.user_data[prefix + str(i) + \"_num_trades\"].append(num_trades)          \n",
    "\n",
    "    # def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "    #                    policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "    #                    **kwargs):\n",
    "    def on_episode_end(self, *, worker, base_env, policies, episode, **kwargs):\n",
    "        # Example: summarize at end\n",
    "        mean_metric = sum(episode.user_data[\"my_metric\"]) / len(episode.user_data[\"my_metric\"])\n",
    "        print(f\"Episode {episode.episode_id} ended. Mean my_metric: {mean_metric}\")        \n",
    "        #print(\"on_episode_end {}, episode.agent_rewards {}\".format(episode.episode_id, episode.agent_rewards))\n",
    "        \"\"\"\n",
    "        arg: {\"env\": .., \"episode\": ...}\n",
    "        \"\"\"\n",
    "\n",
    "        g_store = ray.util.get_actor(\"g_store\")\n",
    "        prefix = \"agt_\"\n",
    "        for agt_id in range(num_agents):\n",
    "            obs_key = prefix + str(agt_id) + \"_obs\"\n",
    "            act_key = prefix + str(agt_id) + \"_act\"\n",
    "            reward_key = prefix + str(agt_id) + \"_reward\"\n",
    "            NAV_key = prefix + str(agt_id) + \"_NAV\"\n",
    "            num_trades_key = prefix + str(agt_id) + \"_num_trades\"      \n",
    "\n",
    "            # store into episode.hist_data\n",
    "            store_eps_hist_data(episode, reward_key)\n",
    "            store_eps_hist_data(episode, NAV_key)\n",
    "            store_eps_hist_data(episode, num_trades_key)\n",
    "\n",
    "            # store step data\n",
    "            obs = episode.user_data[obs_key]\n",
    "            act = episode.user_data[act_key]\n",
    "            reward = episode.user_data[reward_key]\n",
    "            NAV = episode.user_data[NAV_key]\n",
    "            num_trades = episode.user_data[num_trades_key]\n",
    "            ray.get(g_store.store_agt_step.remote(agt_id, obs, act, reward, NAV, num_trades))       \n",
    "        \n",
    "            # Store episode data.   \n",
    "            eps_reward = np.sum(reward)\n",
    "            eps_NAV = np.sum(NAV)\n",
    "            eps_num_trades = np.sum(num_trades)\n",
    "            ray.get(g_store.store_agt_eps.remote(agt_id, eps_reward, eps_NAV, eps_num_trades))\n",
    "\n",
    "        ray.get(g_store.inc_eps_counter.remote())  \n",
    "\n",
    "    # def on_sample_end(self, worker: RolloutWorker, samples: SampleBatch,\n",
    "    #                   **kwargs):\n",
    "    #     \"\"\"\n",
    "    #     arg: {\"samples\": .., \"worker\": ...}\n",
    "\n",
    "    #     Notes:\n",
    "    #         https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py        \n",
    "    #     \"\"\"\n",
    "    #     #print(\"on_sample_end returned sample batch of size {}\".format(samples.count))\n",
    "\n",
    "    #     \"\"\"\n",
    "    #     MultiAgentBatch_obj = info[\"samples\"]\n",
    "    #     MultiAgentBatch_policy_batches = MultiAgentBatch_obj.policy_batches\n",
    "    #     MultiAgentBatch_count = MultiAgentBatch_obj.count\n",
    "\n",
    "    #     access_sample_batches(MultiAgentBatch_policy_batches)\n",
    "    #     print(\"info['samples'].policy_batches = {}\".format(info[\"samples\"].policy_batches))\n",
    "    #     print(\"info['worker'] = {}\".format(info[\"worker\"])) # RolloutWorker object\n",
    "    #     \"\"\"\n",
    "\n",
    "    def on_train_result(self, trainer, result: dict, **kwargs):\n",
    "        \"\"\"\n",
    "        info[\"trainer\"] is the trainer object.\n",
    "\n",
    "        info[\"result\"] contains a bunch of info such as episodic rewards for \n",
    "        each policy in info[\"result\"][hist_stats] dictionary.\n",
    "        \"\"\"\n",
    "        #print(\"trainer.train() result: {} -> {} episodes\".format(trainer, result[\"episodes_this_iter\"]))\n",
    "        # you can mutate the result dict to add new fields to return\n",
    "        result[\"callback_ok\"] = True\n",
    "        #print(\"on_train_result result\", result)\n",
    "    \n",
    "        train_policies_name = get_trained_policies_name(policies, num_trained_agent)    \n",
    "        max_reward_ind = get_max_reward_ind(result, train_policies_name)\n",
    "        max_reward_policy_name = train_policies_name[max_reward_ind]\n",
    "        cp_weight(trainer, train_policies_name, max_reward_policy_name)    \n",
    "\n",
    "        g_store = ray.util.get_actor(\"g_store\")      \n",
    "        prefix = \"policy_policy_\"\n",
    "        suffix = \"_reward\"\n",
    "        hist_stats = result[\"hist_stats\"]\n",
    "        eps_this_iter = result[\"episodes_this_iter\"]\n",
    "        for agt_id in range(num_agents):\n",
    "            key = prefix + str(agt_id) + suffix\n",
    "            for i in range(eps_this_iter):\n",
    "                ray.get(g_store.store_agt_train.remote(agt_id, hist_stats[key][i]))\n",
    "\n",
    "        #print(\"on_train_result info['result'] {}\".format(info[\"result\"]))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEnp5UpxkDve"
   },
   "source": [
    "### RLlib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fr_a0y6hjn6"
   },
   "outputs": [],
   "source": [
    "# def get_config():\n",
    "#     config = ppo.DEFAULT_CONFIG.copy()\n",
    "#     config[\"multiagent\"] = {\"policies_to_train\": train_policy_list,\n",
    "#                             \"policies\": policies,\n",
    "#                             \"policy_mapping_fn\": policy_mapper,\n",
    "#                            }    \n",
    "#     #config[\"num_cpus\"] = num_cpus     # trainer, applicable only when using tune.\n",
    "#     config[\"num_gpus\"] = num_gpus     # trainer\n",
    "#     config[\"num_cpus_per_worker\"] = num_cpus_per_worker                                \n",
    "#     config[\"num_gpus_per_worker\"] = num_gpus_per_worker                      \n",
    "#     config[\"num_workers\"] = num_workers\n",
    "#     config[\"num_envs_per_worker\"] = num_envs_per_worker  \n",
    "#     config[\"batch_mode\"] = batch_mode       # \"complete_episodes\" / \"truncate_episodes\"\n",
    "#     config[\"train_batch_size\"] = train_batch_size # Training batch size, if applicable. Should be >= rollout_fragment_length.\n",
    "#                                                   # Samples batches will be concatenated together to a batch of this size,\n",
    "#                                                   # which is then passed to SGD.\n",
    "#     config[\"rollout_fragment_length\"] = rollout_fragment_length # replaced \"sample_batch_size\",\n",
    "#     config[\"sgd_minibatch_size\"] = sgd_minibatch_size \n",
    "#     config[\"log_level\"] = \"WARN\" # WARN/INFO/DEBUG \n",
    "#     config[\"callbacks\"] = MyCallbacks\n",
    "#     config[\"output\"] = log_dir\n",
    "\n",
    "#     return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "\n",
    "def get_config():\n",
    "    config = (\n",
    "        PPOConfig()            \n",
    "        .environment(\n",
    "            \"continuousDoubleAuction-v0\",\n",
    "            # continuousDoubleAuctionEnv,\n",
    "            # env_config={\n",
    "            #     \"num_of_agents\": num_of_traders, \n",
    "            #     \"init_cash\": init_cash, \n",
    "            #     \"tick_size\": tick_size, \n",
    "            #     \"tape_display_length\": tape_display_length,\n",
    "            #     \"max_step\": max_step - 1, \n",
    "            #     \"is_render\": is_render\n",
    "            # }\n",
    "            # env_config={\"disable_env_checker\": True},            \n",
    "        ) \n",
    "        .multi_agent(\n",
    "            policies=policies,\n",
    "            \n",
    "            # policy_mapping_fn=policy_mapper,\n",
    "            policy_mapping_fn=policy_mapping_fn,\n",
    "                \n",
    "            # policies_to_train=train_policy_list,\n",
    "            policies_to_train=policies_to_train,            \n",
    "        )\n",
    "        .env_runners(\n",
    "            # num_env_runners=num_workers,\n",
    "            num_env_runners=0,\n",
    "            \n",
    "            num_envs_per_env_runner=num_envs_per_worker,\n",
    "            rollout_fragment_length=rollout_fragment_length,\n",
    "            batch_mode=batch_mode,\n",
    "        )\n",
    "        .learners(\n",
    "            num_learners=1,  # Typically 1 learner unless using distributed training\n",
    "            num_gpus_per_learner=num_gpus,  # Trainer GPU allocation\n",
    "            num_cpus_per_learner=num_cpus_per_worker,\n",
    "        )\n",
    "        .training(\n",
    "            train_batch_size_per_learner=train_batch_size,\n",
    "            # sgd_minibatch_size,\n",
    "        )\n",
    "        .callbacks(MyCallbacks)\n",
    "        # .output_dir(log_dir)\n",
    "        .framework(\"torch\")  # Explicitly set framework if needed\n",
    "    )\n",
    "\n",
    "    # Optional: Configure resources more granularly if needed\n",
    "    if num_gpus_per_worker > 0:\n",
    "        config.env_runners(\n",
    "            num_gpus_per_env_runner=num_gpus_per_worker\n",
    "        )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKLNyViDkI9O"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iyMEBA0uC8ZW",
    "outputId": "b490fc96-4321-4150-b3c4-34d391389446"
   },
   "outputs": [],
   "source": [
    "# def go_train(config):    \n",
    "#     trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")\n",
    "    \n",
    "#     if is_restore == True:\n",
    "#         trainer.restore(restore_path) \n",
    "\n",
    "#     g_store = ray.util.get_actor(\"g_store\")          \n",
    "#     result = None\n",
    "#     for i in range(num_iters):\n",
    "#         result = trainer.train()       \n",
    "#         print(pretty_print(result)) # includes result[\"custom_metrics\"]\n",
    "#         print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
    "#         print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n",
    "\n",
    "#         if i % chkpt_freq == 0:\n",
    "#             checkpoint = trainer.save(local_dir)\n",
    "#             print(\"checkpoint saved at\", checkpoint)\n",
    "    \n",
    "#     checkpoint = trainer.save(local_dir)\n",
    "#     print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "#     print(\"result['experiment_id']\", result[\"experiment_id\"])\n",
    "    \n",
    "#     return result[\"experiment_id\"]\n",
    "    \n",
    "# # run everything\n",
    "# experiment_id = go_train(get_config())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 14:00:46,026\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-07-22 14:00:46,028\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-07-22 14:00:46,081\tERROR multi_agent_env_runner.py:834 -- 'dict' object has no attribute 'sample'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/env/multi_agent_env_runner.py\", line 832, in make_env\n",
      "    check_multiagent_environments(env.unwrapped)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py\", line 57, in check_multiagent_environments\n",
      "    sampled_action = {\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/pre_checks/env.py\", line 58, in <dictcomp>\n",
      "    aid: env.get_action_space(aid).sample() for aid in reset_obs.keys()\n",
      "AttributeError: 'dict' object has no attribute 'sample'\n",
      "2025-07-22 14:00:46,174\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-07-22 14:00:46,174\tWARNING rl_module.py:436 -- Didn't create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No default encoder config for obs space={'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, lstm=False found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/rl_module.py:474\u001b[0m, in \u001b[0;36mRLModule.__init__\u001b[0;34m(self, config, observation_space, action_space, inference_only, learner_only, model_config, catalog_class, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/ppo/default_ppo_rl_module.py:31\u001b[0m, in \u001b[0;36mDefaultPPORLModule.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@override\u001b[39m(RLModule)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# __sphinx_doc_begin__\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# point the encoder is not built, yet and therefore `is_stateful()` does\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# not work.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     is_stateful \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m---> 31\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic_encoder_config\u001b[49m\u001b[38;5;241m.\u001b[39mbase_encoder_config,\n\u001b[1;32m     32\u001b[0m         RecurrentEncoderConfig,\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_stateful:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'actor_critic_encoder_config'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# run everything\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[43mgo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m            \n",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m, in \u001b[0;36mgo_train\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgo_train\u001b[39m(config):    \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")      \u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# algo = config.build()   \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     algo \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# if is_restore == True:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#     trainer.restore(restore_path) \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# return result[\"experiment_id\"]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/utils/deprecation.py:128\u001b[0m, in \u001b[0;36mDeprecated.<locals>._inner.<locals>._ctor\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     deprecation_warning(\n\u001b[1;32m    122\u001b[0m         old\u001b[38;5;241m=\u001b[39mold \u001b[38;5;129;01mor\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    123\u001b[0m         new\u001b[38;5;241m=\u001b[39mnew,\n\u001b[1;32m    124\u001b[0m         help\u001b[38;5;241m=\u001b[39mhelp,\n\u001b[1;32m    125\u001b[0m         error\u001b[38;5;241m=\u001b[39merror,\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Call the deprecated method/function.\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm_config.py:5748\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5746\u001b[0m \u001b[38;5;129m@Deprecated\u001b[39m(new\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgorithmConfig.build_algo\u001b[39m\u001b[38;5;124m\"\u001b[39m, error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   5747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 5748\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm_config.py:999\u001b[0m, in \u001b[0;36mAlgorithmConfig.build_algo\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    997\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[0;32m--> 999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:536\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_env_runner_group: Optional[EnvRunnerGroup] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py:157\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    153\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:644\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_online \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_group \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# New API stack: User decides whether to create local env runner.\u001b[39;49;00m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Old API stack: Always create local EnvRunner.\u001b[39;49;00m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_local_env_runner\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune_trial_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Compile, validate, and freeze an evaluation config.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_evaluation_config_object()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/env_runner_group.py:198\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, local_env_runner, logdir, _setup, tune_trial_id, pg_offset, num_env_runners, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    204\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/env_runner_group.py:292\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_env_runner:\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_env_runner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/env_runner_group.py:1294\u001b[0m, in \u001b[0;36mEnvRunnerGroup._make_worker\u001b[0;34m(self, env_creator, validate_env, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m   1279\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   1280\u001b[0m     env_creator\u001b[38;5;241m=\u001b[39menv_creator,\n\u001b[1;32m   1281\u001b[0m     validate_env\u001b[38;5;241m=\u001b[39mvalidate_env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     tune_trial_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tune_trial_id,\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m worker_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runner_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m pg_bundle_idx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mget_current_placement_group() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pg_offset \u001b[38;5;241m+\u001b[39m worker_index\n\u001b[1;32m   1300\u001b[0m )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m   1302\u001b[0m     ray\u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_args)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_cls)\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;241m.\u001b[39moptions(placement_group_bundle_index\u001b[38;5;241m=\u001b[39mpg_bundle_idx)\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1305\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/multi_agent_env_runner.py:133\u001b[0m, in \u001b[0;36mMultiAgentEnvRunner.__init__\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Construct the MultiRLModule.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule: Optional[MultiRLModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Create the module-to-env connector pipeline.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_to_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbuild_module_to_env_connector(\n\u001b[1;32m    137\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39munwrapped \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspaces\n\u001b[1;32m    138\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/env/multi_agent_env_runner.py:878\u001b[0m, in \u001b[0;36mMultiAgentEnvRunner.make_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    874\u001b[0m module_spec: MultiRLModuleSpec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_multi_rl_module_spec(\n\u001b[1;32m    875\u001b[0m     env\u001b[38;5;241m=\u001b[39menv, spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_spaces(), inference_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    876\u001b[0m )\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# Build the module from its spec.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# Move the RLModule to our device.\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# TODO (sven): In order to make this framework-agnostic, we should maybe\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;66;03m#  make the MultiRLModule.build() method accept a device OR create an\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;66;03m#  additional `(Multi)RLModule.to()` override.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/multi_rl_module.py:611\u001b[0m, in \u001b[0;36mMultiRLModuleSpec.build\u001b[0;34m(self, module_id)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Return MultiRLModule.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_rl_module_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataclasses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_dataclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrl_module_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_module_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Older custom model might still require the old `MultiRLModuleConfig` under\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# the `config` arg.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/multi_rl_module.py:127\u001b[0m, in \u001b[0;36mMultiRLModule.__init__\u001b[0;34m(self, config, observation_space, action_space, inference_only, learner_only, model_config, rl_module_specs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_module_specs(rl_module_specs)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_module_specs \u001b[38;5;241m=\u001b[39m rl_module_specs\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearner_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatalog_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/rl_module.py:474\u001b[0m, in \u001b[0;36mRLModule.__init__\u001b[0;34m(self, config, observation_space, action_space, inference_only, learner_only, model_config, catalog_class, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`RLModule.setup()` called twice within your RLModule implementation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m! Make sure you are using the proper inheritance order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly the `setup()` method of your subclass.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    472\u001b[0m     )\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoneType\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/multi_rl_module.py:145\u001b[0m, in \u001b[0;36mMultiRLModule.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_id, rl_module_spec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_module_specs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rl_modules[module_id] \u001b[38;5;241m=\u001b[39m \u001b[43mrl_module_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m         framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rl_modules[module_id]\u001b[38;5;241m.\u001b[39mframework\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/rl_module.py:102\u001b[0m, in \u001b[0;36mRLModuleSpec.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction space is not set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatalog_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatalog_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Older custom model might still require the old `RLModuleConfig` under\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# the `config` arg.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py:24\u001b[0m, in \u001b[0;36mDefaultPPOTorchRLModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m catalog_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     catalog_class \u001b[38;5;241m=\u001b[39m PPOCatalog\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatalog_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatalog_class\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py:50\u001b[0m, in \u001b[0;36mTorchRLModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     nn\u001b[38;5;241m.\u001b[39mModule\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mRLModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# If an inference-only class AND self.inference_only is True,\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# remove all attributes that are returned by\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# `self.get_non_inference_attributes()`.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, InferenceOnlyAPI):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/rl_module.py:477\u001b[0m, in \u001b[0;36mRLModule.__init__\u001b[0;34m(self, config, observation_space, action_space, inference_only, learner_only, model_config, catalog_class, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoneType\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 477\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_catalog_ctor_error \u001b[38;5;129;01mor\u001b[39;00m e)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_setup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/rl_module/rl_module.py:430\u001b[0m, in \u001b[0;36mRLModule.__init__\u001b[0;34m(self, config, observation_space, action_space, inference_only, learner_only, model_config, catalog_class, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m catalog_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcatalog \u001b[38;5;241m=\u001b[39m \u001b[43mcatalog_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m            \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    436\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    437\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDidn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create a Catalog object for your RLModule! If you are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot using the new API stack yet, make sure to switch it off in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRLModule does not use a Catalog to build its sub-components.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/ppo/ppo_catalog.py:83\u001b[0m, in \u001b[0;36mPPOCatalog.__init__\u001b[0;34m(self, observation_space, action_space, model_config_dict)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     observation_space: gym\u001b[38;5;241m.\u001b[39mSpace,\n\u001b[1;32m     73\u001b[0m     action_space: gym\u001b[38;5;241m.\u001b[39mSpace,\n\u001b[1;32m     74\u001b[0m     model_config_dict: \u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m     75\u001b[0m ):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes the PPOCatalog.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        model_config_dict: The model config to use.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Replace EncoderConfig by ActorCriticEncoderConfig\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic_encoder_config \u001b[38;5;241m=\u001b[39m ActorCriticEncoderConfig(\n\u001b[1;32m     90\u001b[0m         base_encoder_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder_config,\n\u001b[1;32m     91\u001b[0m         shared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvf_share_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/models/catalog.py:122\u001b[0m, in \u001b[0;36mCatalog.__init__\u001b[0;34m(self, observation_space, action_space, model_config_dict, view_requirements)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_config_dict \u001b[38;5;241m=\u001b[39m default_config \u001b[38;5;241m|\u001b[39m model_config_dict\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_latent_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_determine_components_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/models/catalog.py:142\u001b[0m, in \u001b[0;36mCatalog._determine_components_hook\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@OverrideToImplementCustomLogic_CallToSuperRecommended\u001b[39m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_determine_components_hook\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decision tree hook for subclasses to override.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    By default, this method executes the decision tree that determines the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    should be set so that heads can be built using that information.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encoder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_encoder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# Create a function that can be called when framework is known to retrieve the\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# class type for action distributions\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_dist_class_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dist_cls_from_action_space, action_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\n\u001b[1;32m    152\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/rllib/core/models/catalog.py:361\u001b[0m, in \u001b[0;36mCatalog._get_encoder_config\u001b[0;34m(cls, observation_space, model_config_dict, action_space)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo default encoder config for obs space=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m lstm=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_lstm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found. 2D Box \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspaces are not supported. They should be either flattened to a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1D Box space or enhanced to be a 3D box space.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m         )\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# input_space is a possibly nested structure of spaces.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;66;03m# NestedModelConfig\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    362\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo default encoder config for obs space=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m lstm=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_lstm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_config\n",
      "\u001b[0;31mValueError\u001b[0m: No default encoder config for obs space={'agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32)}, lstm=False found."
     ]
    }
   ],
   "source": [
    "def go_train(config):    \n",
    "    # trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")      \n",
    "    # algo = config.build()   \n",
    "    algo = config.build()\n",
    "    \n",
    "    # if is_restore == True:\n",
    "    #     trainer.restore(restore_path) \n",
    "\n",
    "    # g_store = ray.util.get_actor(\"g_store\")          \n",
    "    # result = None\n",
    "    # for i in range(num_iters):\n",
    "    #     result = algo.train()       \n",
    "\n",
    "    #     print(pretty_print(result)) # includes result[\"custom_metrics\"]\n",
    "    #     print(\"training loop = {} of {}\".format(i + 1, num_iters))            \n",
    "    #     print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n",
    "\n",
    "    #     if i % chkpt_freq == 0:\n",
    "    #         checkpoint = algo.save(local_dir)\n",
    "    #         print(\"checkpoint saved at\", checkpoint)\n",
    "    \n",
    "    # checkpoint = algo.save(local_dir)\n",
    "    # print(\"checkpoint saved at\", checkpoint)\n",
    "    # print(\"result['experiment_id']\", result[\"experiment_id\"])\n",
    "\n",
    "    # return result[\"experiment_id\"]\n",
    "    return None\n",
    "    \n",
    "# run everything\n",
    "experiment_id = go_train(get_config())            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmRFarjpD7Wt"
   },
   "source": [
    "### Plot all steps.\n",
    "\n",
    "Agt_0, 1, 2 are trained agents (with PPO) while the rest are random agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "wzMc3PY_D7qv",
    "outputId": "17e774a8-2d2f-46a8-ec0b-d56894c84250"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "e0NDLLyiESgx",
    "outputId": "f16045e7-bb85-44a4-f67a-73fd1b0e861f"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"NAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "GrqwrFwsETlK",
    "outputId": "0da3cef4-aa85-47ae-c771-9d2f84b84ba6"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-1mV5YFYwkN"
   },
   "source": [
    "### Log/load last episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "khD_kiwaJVSQ",
    "outputId": "01e3109b-a024-44fc-8374-98db6be9f170"
   },
   "outputs": [],
   "source": [
    "# log_g_store(log_g_store_dir, num_agents, experiment_id)\n",
    "# load_g_store(log_g_store_dir, num_agents, experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSu7XmUSDVOE"
   },
   "source": [
    "### Plot steps from last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "rNraawhLmH68",
    "outputId": "d9d3d2b3-b345-4597-c49f-863b52355c2e"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "Lke086wkL4If",
    "outputId": "8bc0af37-3841-456f-8d1d-ba581cae5221"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"NAV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "2uqneDvkL4x6",
    "outputId": "d93996d1-a637-4383-bedf-c5687e250d5b"
   },
   "outputs": [],
   "source": [
    "# plot_storage(num_agents, init_cash, \"step\", \"num_trades\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dL1TZkz-Wvqx"
   },
   "source": [
    "### LOB from last episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR5iU78gCePl"
   },
   "outputs": [],
   "source": [
    "# g_store = ray.util.get_actor(\"g_store\")          \n",
    "# #store = ray.get(g_rere.get_storage.remote())\n",
    "\n",
    "# depth = 10\n",
    "# bid_size, bid_price, ask_size, ask_price = ray.get(g_store.get_obs_from_agt.remote(0, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPa_j4M2Z6mh"
   },
   "source": [
    "### LOB order imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2PBgQwhTBOqQ",
    "outputId": "0ecf046a-2ed1-4878-d1b4-7d827b4e8a96",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ord_imb_store = ord_imb(bid_size, ask_size)\n",
    "# plot_LOB_subplot(ord_imb_store, depth, '_ord_imb') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9Kb9NpmaHg4"
   },
   "source": [
    "### LOB sum of order imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "MXydsI2nC76g",
    "outputId": "6078375e-4473-448c-9d64-f7f19f302101"
   },
   "outputs": [],
   "source": [
    "# ord_imb_store = np.asarray(ord_imb_store)\n",
    "# sum_ord_imb_store = sum_ord_imb(ord_imb_store)\n",
    "# plot_sum_ord_imb(sum_ord_imb_store, \"sum_ord_imb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12iZkIMdaS_l"
   },
   "source": [
    "### LOB mid price (subplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YhRaxfpg_w2t",
    "outputId": "48a2657d-8c53-4a33-f8f7-949ce90fc2b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mid_price_store = mid_price(bid_price, ask_price)\n",
    "# plot_LOB_subplot(mid_price_store, depth, '_mid_price')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWPlbUqebcJi"
   },
   "source": [
    "### LOB mid prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "CZ724VMdbbWs",
    "outputId": "1a8af35b-4592-4b5d-e17d-9ddeb9f409c2"
   },
   "outputs": [],
   "source": [
    "# plot_mid_prices(mid_price_store,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JV5v-yYZdqO"
   },
   "source": [
    "### LOB bid size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PS-hHA9Mx1CZ",
    "outputId": "1f3f4ea1-e607-4612-a8d8-2efd8a833bfa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(bid_size, depth, '_bid_size')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUotX7pQZiXP"
   },
   "source": [
    "### LOB ask size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iYuL9s5ex5So",
    "outputId": "4108d574-fa52-4ee6-bf24-efeeb8c7471c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(ask_size, depth, '_ask_size')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKiDC3C2ZnW4"
   },
   "source": [
    "### LOB bid price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "X2qQ-O01x5ER",
    "outputId": "90f18010-dd45-476b-cc78-dc42f613703c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(bid_price, depth, '_bid_price')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3HcZH-KZuLd"
   },
   "source": [
    "### LOB ask price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FifBnZAdx6uT",
    "outputId": "562f8092-b909-44dd-d1bd-cd3637164986",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_LOB_subplot(ask_price, depth, '_ask_price')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "# from ray.rllib.algorithms.ppo import PPOConfig\n",
    "# from ray.rllib.policy.policy import PolicySpec\n",
    "# from ray import tune  # Important: use ray.tune, not ray.tune.registry\n",
    "# from gymnasium import spaces\n",
    "# import numpy as np\n",
    "# import ray\n",
    "\n",
    "# class CustomMARLEnv(MultiAgentEnv):\n",
    "#     # def __init__(self, config=None):\n",
    "#     def __init__(self, p1):    \n",
    "#         super().__init__()\n",
    "#         self.agents = [\"agent_0\", \"agent_1\", \"agent_2\"]\n",
    "#         self._agent_ids = set(self.agents)\n",
    "        \n",
    "#         # Define spaces as dictionaries mapping agent_id to space\n",
    "#         obs_space = spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "#         act_space = spaces.Discrete(3)\n",
    "        \n",
    "#         self.observation_space = {agent: obs_space for agent in self.agents}\n",
    "#         self.action_space = {agent: act_space for agent in self.agents}\n",
    "        \n",
    "#     def reset(self, *, seed=None, options=None):\n",
    "#         observations = {agent: self.observation_space[agent].sample() for agent in self.agents}\n",
    "#         infos = {agent: {} for agent in self.agents}\n",
    "#         return observations, infos\n",
    "    \n",
    "#     def step(self, action_dict):\n",
    "#         observations = {agent: self.observation_space[agent].sample() for agent in self.agents}\n",
    "#         rewards = {agent: np.random.random() for agent in self.agents}\n",
    "        \n",
    "#         terminateds = {agent: False for agent in self.agents}\n",
    "#         truncateds = {agent: False for agent in self.agents}\n",
    "#         # Global episode termination (required!)\n",
    "#         terminateds[\"__all__\"] = False  # Set to True when episode should end\n",
    "#         truncateds[\"__all__\"] = False   # Set to True when episode should be truncated        \n",
    "        \n",
    "#         infos = {agent: {} for agent in self.agents}\n",
    "        \n",
    "#         return observations, rewards, terminateds, truncateds, infos\n",
    "\n",
    "# def env_creator(env_config):\n",
    "#     return CustomMARLEnv(env_config)\n",
    "\n",
    "# # Initialize Ray first\n",
    "# ray.shutdown()\n",
    "# ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# # Register environment with ray.tune - this is the key fix!\n",
    "# tune.register_env(\"custom_marl_env\", env_creator)\n",
    "\n",
    "# # Create config with correct new API\n",
    "# config = (\n",
    "#     PPOConfig()\n",
    "#     .environment(\"custom_marl_env\")\n",
    "#     .multi_agent(\n",
    "#         policies={\n",
    "#             \"shared_policy\": PolicySpec(),\n",
    "#         },\n",
    "#         policy_mapping_fn=lambda agent_id, episode, **kwargs: \"shared_policy\",\n",
    "#     )\n",
    "#     .env_runners(\n",
    "#         num_env_runners=2,\n",
    "#     )\n",
    "#     .training(\n",
    "#         train_batch_size=1000,\n",
    "#         num_sgd_iter=10,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Build and train\n",
    "# try:\n",
    "#     algo = config.build()\n",
    "    \n",
    "#     for i in range(3):\n",
    "#         result = algo.train()\n",
    "#         # Try different possible result keys\n",
    "#         reward_key = None\n",
    "#         for key in ['env_runners/episode_reward_mean', 'episode_reward_mean', 'hist_stats/episode_reward']:\n",
    "#             if key in result:\n",
    "#                 reward_key = key\n",
    "#                 break\n",
    "        \n",
    "#         if reward_key:\n",
    "#             print(f\"Iteration {i}: reward = {result[reward_key]}\")\n",
    "#         else:\n",
    "#             print(f\"Iteration {i}: training completed\")\n",
    "    \n",
    "#     algo.stop()\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "#     if 'algo' in locals():\n",
    "#         algo.stop()\n",
    "\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray import tune  # Important: use ray.tune, not ray.tune.registry\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "class CustomMARLEnv(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Extract parameters from config with defaults\n",
    "        config = config or {}\n",
    "        self._num_agents = config.get(\"num_agents\", 3)  # Use underscore to avoid conflicts\n",
    "        self.obs_dim = config.get(\"obs_dim\", 4)\n",
    "        self.num_actions = config.get(\"num_actions\", 3)\n",
    "        self.max_steps = config.get(\"max_steps\", 100)\n",
    "        \n",
    "        # Initialize agents based on parameters\n",
    "        self.agents = [f\"agent_{i}\" for i in range(self._num_agents)]\n",
    "        self._agent_ids = set(self.agents)\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Define spaces as dictionaries mapping agent_id to space\n",
    "        obs_space = spaces.Box(low=-1, high=1, shape=(self.obs_dim,), dtype=np.float32)\n",
    "        act_space = spaces.Discrete(self.num_actions)\n",
    "        \n",
    "        self.observation_space = {agent: obs_space for agent in self.agents}\n",
    "        self.action_space = {agent: act_space for agent in self.agents}\n",
    "        \n",
    "        print(f\"Initialized environment with {len(self.agents)} agents: {self.agents}\")\n",
    "    \n",
    "    def get_agent_ids(self):\n",
    "        \"\"\"Return the set of agent IDs.\"\"\"\n",
    "        return self._agent_ids\n",
    "        \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.current_step = 0\n",
    "        observations = {agent: self.observation_space[agent].sample() for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        return observations, infos\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Validate that we received actions for active agents\n",
    "        if not action_dict:\n",
    "            raise ValueError(\"No actions received\")\n",
    "        \n",
    "        print(f\"Step {self.current_step}: Received actions for {list(action_dict.keys())}\")\n",
    "        print(f\"Expected agents: {self.agents}\")\n",
    "        \n",
    "        # Only generate observations for agents that are still active\n",
    "        active_agents = [agent for agent in self.agents if agent in action_dict]\n",
    "        \n",
    "        observations = {}\n",
    "        rewards = {}\n",
    "        terminateds = {}\n",
    "        truncateds = {}\n",
    "        infos = {}\n",
    "        \n",
    "        for agent in active_agents:\n",
    "            observations[agent] = self.observation_space[agent].sample()\n",
    "            rewards[agent] = np.random.random()\n",
    "            terminateds[agent] = False\n",
    "            truncateds[agent] = False\n",
    "            infos[agent] = {}\n",
    "        \n",
    "        # Global episode termination (required!)\n",
    "        terminateds[\"__all__\"] = False\n",
    "        truncateds[\"__all__\"] = self.current_step >= self.max_steps\n",
    "        \n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "\n",
    "def env_creator(env_config):\n",
    "    print(f\"Creating environment with config: {env_config}\")\n",
    "    env = CustomMARLEnv(env_config)\n",
    "    print(f\"Environment created successfully with agents: {env.agents}\")\n",
    "    return env\n",
    "\n",
    "# Initialize Ray first\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Register environment with ray.tune - this is the key fix!\n",
    "tune.register_env(\"custom_marl_env\", env_creator)\n",
    "\n",
    "# Create config with environment parameters\n",
    "env_config = {\n",
    "    \"num_agents\": 3,  # Start with fewer agents for debugging\n",
    "    \"obs_dim\": 4,\n",
    "    \"num_actions\": 3,\n",
    "    \"max_steps\": 50,  # Shorter episodes for testing\n",
    "}\n",
    "\n",
    "print(f\"Using env_config: {env_config}\")\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        \"custom_marl_env\",\n",
    "        env_config=env_config  # Pass parameters to your environment\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"shared_policy\": PolicySpec(),\n",
    "        },\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: \"shared_policy\",\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=1,  # Start with 1 for debugging\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=100,  # Smaller batch for testing\n",
    "        # sgd_minibatch_size=64,\n",
    "        num_sgd_iter=5,\n",
    "    )\n",
    "    .debugging(log_level=\"DEBUG\")  # Add debugging\n",
    ")\n",
    "\n",
    "# Build and train\n",
    "try:\n",
    "    algo = config.build()\n",
    "    \n",
    "    for i in range(10):\n",
    "        result = algo.train()\n",
    "        # Try different possible result keys\n",
    "        reward_key = None\n",
    "        for key in ['env_runners/episode_reward_mean', 'episode_reward_mean', 'hist_stats/episode_reward']:\n",
    "            if key in result:\n",
    "                reward_key = key\n",
    "                break\n",
    "        \n",
    "        if reward_key:\n",
    "            print(f\"Iteration {i}: reward = {result[reward_key]}\")\n",
    "        else:\n",
    "            print(f\"Iteration {i}: training completed\")\n",
    "    \n",
    "    algo.stop()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    if 'algo' in locals():\n",
    "        algo.stop()\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CDA_NSP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
