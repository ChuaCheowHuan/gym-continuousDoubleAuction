{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f05ZH97QkoJf"
   },
   "source": [
    "# Sample training script with naive competitive self-play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GPU Diagnostics\n",
    "# import torch\n",
    "# print(\"=\"*50)\n",
    "# print(\"GPU Diagnostics:\")\n",
    "# print(\"=\"*50)\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# print(f\"CUDA version (built with): {torch.version.cuda}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "#     print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "#     print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "# else:\n",
    "#     print(\"❌ No GPU detected by PyTorch!\")\n",
    "#     print(\"\\nPossible solutions:\")\n",
    "#     print(\"1. Install PyTorch with CUDA support:\")\n",
    "#     print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "#     print(\"2. Check NVIDIA drivers: nvidia-smi\")\n",
    "#     print(\"3. Verify CUDA toolkit is installed\")\n",
    "# print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcLSdJuUkTrX"
   },
   "source": [
    "### Switch directory in Google drive so as to import CDA env.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756087231922,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "0roHXj0tvvLg"
   },
   "outputs": [],
   "source": [
    "is_colab = False\n",
    "# is_colab = True\n",
    "\n",
    "# is_1st_run = False\n",
    "is_1st_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1756087232001,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "PAqVG2cqjLXM"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# %cd \"/root/ray_results/\"\n",
    "# !ls -l\n",
    "# #!rm -rf PPO_continuousDoubleAuction-v0_*\n",
    "# !ls -l\n",
    "# !pwd\n",
    "\n",
    "# %cd \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/\"\n",
    "# !ls -l\n",
    "\n",
    "# #!pip install -r requirements.txt\n",
    "\n",
    "# #!pip install tensorflow==2.2.0\n",
    "# #!pip install ray[rllib]==0.8.5\n",
    "\n",
    "# #!pip show tensorflow\n",
    "# #!pip show ray\n",
    "\n",
    "# #!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232034,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "_ZJO7gUwngjr",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if is_colab == False and is_1st_run == True:\n",
    "    !pip install sortedcontainers\n",
    "    !!pip install scikit-learn\n",
    "    !pip install tabulate\n",
    "    !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232036,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "vgzysJOX0HZJ"
   },
   "outputs": [],
   "source": [
    "# !pip install -U ipywidgets\n",
    "# !pip install pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232038,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "e9q-QyPhngjt"
   },
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1756087232056,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "ZtVHJhPMngju"
   },
   "outputs": [],
   "source": [
    "# os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756087232069,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "CsWAV-_mngju"
   },
   "outputs": [],
   "source": [
    "# !pip install -e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1756087232086,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "uZpGXbLJngju"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall continuousDoubleAuction\n",
    "# !pip uninstall continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232116,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "t8WyPN_qngju"
   },
   "outputs": [],
   "source": [
    "# !pip show continuousDoubleAuction\n",
    "# !pip show continuousDoubleAuction-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232118,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "DYuxehQengjv"
   },
   "outputs": [],
   "source": [
    "# os.chdir('gym_continuousDoubleAuction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087232119,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "r5E-HRDDngjv"
   },
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17865,
     "status": "ok",
     "timestamp": 1756087249985,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "D9EIlrs1pFq6",
    "outputId": "1fbfa0a4-3d1a-469e-f192-ec15a35c53de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_colab == True:\n",
    "    !pip install -U ray[rllib]==2.48.0\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "    %cd 'gdrive/MyDrive/Colab Notebooks/MARL/gym-continuousDoubleAuction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18197,
     "status": "ok",
     "timestamp": 1756087268180,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "WavBRshypJfb",
    "outputId": "caa88e03-1469-4d4a-e271-1b3079b750e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 10:46:17,349\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-11-30 10:46:20,264\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray version: 2.47.1\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib\n",
    "import ray.tune\n",
    "\n",
    "print(\"Ray version:\", ray.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3777,
     "status": "ok",
     "timestamp": 1756087271959,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "auFbWGSNpFyK",
    "outputId": "198343fe-c5a0-427a-faed-035053791616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gymnasium\n",
      "Version: 1.0.0\n",
      "Summary: A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym).\n",
      "Home-page: https://farama.org\n",
      "Author: \n",
      "Author-email: Farama Foundation <contact@farama.org>\n",
      "License: MIT License\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: cloudpickle, farama-notifications, numpy, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7ZHcwBWkXVM"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1756087272286,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "7UW3INjDipTC",
    "outputId": "e75fd1c2-c9a3-4a6e-a19b-86c497bfd501",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports all OK.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ['RAY_DEBUG_DISABLE_MEMORY_MONITOR'] = \"True\"\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::DeprecationWarning'\n",
    "\n",
    "import argparse\n",
    "\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.envs.continuousDoubleAuction_env import continuousDoubleAuctionEnv\n",
    "\n",
    "from gym_continuousDoubleAuction.train.model.model_handler import CustomRLModule\n",
    "\n",
    "from gym_continuousDoubleAuction.train.policy.policy_handler import (\n",
    "    # make_RandomPolicy,\n",
    "    # gen_policy,\n",
    "    # set_agents_policies,\n",
    "    # create_train_policy_list,\n",
    "    create_multi_agent_config,\n",
    "    policy_mapping_fn,\n",
    "    # create_and_train_algorithm,\n",
    ")\n",
    "from gym_continuousDoubleAuction.train.weight.weight_handler import (\n",
    "    get_trained_policies_name, get_max_reward_ind, cp_weight)\n",
    "from gym_continuousDoubleAuction.train.storage.store_handler import storage\n",
    "\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.train.callbk.callbk_handler import store_eps_hist_data\n",
    "from gym_continuousDoubleAuction.train.callbk.league_based_self_play import LeagueBasedSelfPlayCallback\n",
    "\n",
    "\n",
    "\n",
    "from gym_continuousDoubleAuction.train.logger.log_handler import (\n",
    "    create_dir, log_g_store, load_g_store)\n",
    "from gym_continuousDoubleAuction.train.plotter.plot_handler import (\n",
    "    plot_storage, plot_LOB_subplot, plot_sum_ord_imb, plot_mid_prices)\n",
    "from gym_continuousDoubleAuction.train.helper.helper import (\n",
    "    ord_imb, sum_ord_imb, mid_price)\n",
    "\n",
    "\n",
    "print(f'Imports all OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDnpi8k5kbYo"
   },
   "source": [
    "### Global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9751,
     "status": "ok",
     "timestamp": 1756087282038,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "UqzjVWUsPykm",
    "outputId": "29b59972-64ec-4d61-e448-ad4b94ab11c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder creation failed or folder already exists: results/\n",
      "Folder creation failed or folder already exists: results/log_g_store/\n",
      "['agent_2', 'agent_3', 'agent_0', 'agent_1']\n",
      "Box(-inf, inf, (40,), float32)\n",
      "Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 10:46:23,136\tWARNING services.py:2152 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.88gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-11-30 10:46:24,289\tINFO worker.py:1917 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m It looks like you're creating a detached actor in an anonymous namespace. In order to access this actor in the future, you will need to explicitly connect to this namespace with ray.init(namespace=\"985b1d94-6402-4461-9fd5-d844aca189eb\", ...)\n"
     ]
    }
   ],
   "source": [
    "# CDA_env args\n",
    "num_agents = 4\n",
    "num_trained_agent = 2 #\n",
    "num_policies = num_agents # Each agent is using a separate policy\n",
    "num_of_traders = num_agents\n",
    "tape_display_length = 10\n",
    "tick_size = 1\n",
    "init_cash = 1000000\n",
    "# max_step = 4096 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n",
    "max_step = 1024 # per episode, -1 in arg. (~7.2s/1000steps/iter)\n",
    "is_render = False\n",
    "\n",
    "# RLlib config\n",
    "# train_policy_list = create_train_policy_list(num_trained_agent, \"policy_\")\n",
    "#num_cpus = 0.25\n",
    "num_gpus = 0.75 #0\n",
    "num_cpus_per_worker = 0.25\n",
    "num_gpus_per_worker = 0\n",
    "num_workers = 2\n",
    "num_envs_per_worker = 4\n",
    "batch_mode = \"complete_episodes\"\n",
    "# rollout_fragment_length = 128\n",
    "num_episodes_per_iter = 4\n",
    "# agent_time_step_per_episode = max_step * num_agents\n",
    "# train_batch_size = agent_time_step_per_episode * num_episodes_per_iter\n",
    "train_batch_size = max_step * num_episodes_per_iter\n",
    "# sgd_minibatch_size = 256\n",
    "num_iters = 128\n",
    "\n",
    "# log_base_dir = \"/content/gdrive/My Drive/Colab Notebooks/gym-continuousDoubleAuction/results/\"\n",
    "log_base_dir = \"results/\"\n",
    "log_dir = log_base_dir + \"ray_results/\"\n",
    "\n",
    "# Chkpt & restore\n",
    "local_dir = log_base_dir + \"chkpt/\"\n",
    "chkpt_freq = 10\n",
    "chkpt = 320\n",
    "restore_path = \"{}checkpoint_{}/checkpoint-{}\".format(local_dir, chkpt, chkpt)\n",
    "is_restore = True # True / False\n",
    "\n",
    "# log & load\n",
    "log_g_store_dir = log_base_dir + \"log_g_store/\"\n",
    "create_dir(log_base_dir)\n",
    "create_dir(log_g_store_dir)\n",
    "\n",
    "# Environment configuration\n",
    "env_config = {\n",
    "    \"num_of_agents\": num_agents,\n",
    "    \"init_cash\": init_cash,\n",
    "    \"tick_size\": tick_size,\n",
    "    \"tape_display_length\": tape_display_length,\n",
    "    \"max_step\": max_step,\n",
    "    \"is_render\": is_render\n",
    "}\n",
    "\n",
    "# get obs & act spaces from dummy CDA env\n",
    "# single_CDA_env = continuousDoubleAuctionEnv(\n",
    "#     num_of_traders,\n",
    "#     init_cash,\n",
    "#     tick_size,\n",
    "#     tape_display_length,\n",
    "#     max_step,\n",
    "#     is_render)\n",
    "single_CDA_env = continuousDoubleAuctionEnv(env_config)\n",
    "obs_space = single_CDA_env.get_observation_space(single_CDA_env.agents[0])\n",
    "act_space = single_CDA_env.get_action_space(single_CDA_env.agents[0])\n",
    "print(single_CDA_env.agents)  # Should be a non-empty list\n",
    "print(single_CDA_env.get_observation_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "print(single_CDA_env.get_action_space(single_CDA_env.agents[0]))  # Should return a valid gym.Space\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return continuousDoubleAuctionEnv(env_config)\n",
    "\n",
    "# Register environment with ray.tune - this is the key fix!\n",
    "tune.register_env(\"continuousDoubleAuction-v0\", env_creator)\n",
    "\n",
    "# register custom model (neural network)\n",
    "ModelCatalog.register_custom_model(\"model_disc\", CustomRLModule)\n",
    "\n",
    "ray.shutdown()\n",
    "# start ray\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    log_to_driver=True,\n",
    "    num_cpus=2,\n",
    "    dashboard_host=\"127.0.0.1\",  # replaces webui_host\n",
    "    dashboard_port=8265,          # default port; replaces webui_port\n",
    "    # include_dashboard=True,        # default True\n",
    "    include_dashboard=False,        # default True\n",
    "\n",
    ")\n",
    "\n",
    "# Global storage, a ray actor that run on it's own process & it needs to be declared after ray.init().\n",
    "g_store = storage.options(name=\"g_store\", lifetime=\"detached\").remote(num_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cknk9Cnoke_u"
   },
   "source": [
    "### Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1756087282068,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "X_CVJpl4ngjw",
    "outputId": "e46188db-3568-44f0-cb04-79a9f7c342ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policies: {'policy_0': <ray.rllib.policy.policy.PolicySpec object at 0x701c4c848580>, 'policy_1': <ray.rllib.policy.policy.PolicySpec object at 0x701c4c8485e0>, 'policy_2': <ray.rllib.policy.policy.PolicySpec object at 0x701c4c8485b0>, 'policy_3': <ray.rllib.policy.policy.PolicySpec object at 0x701c4c8483d0>}\n",
      "policies_to_train: ['policy_0', 'policy_1']\n"
     ]
    }
   ],
   "source": [
    "policies, policies_to_train = create_multi_agent_config(\n",
    "    obs_space, act_space, num_agents, num_trained_agents=num_trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEnp5UpxkDve"
   },
   "source": [
    "### RLlib config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1756087282137,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "AnniWlAwngjx"
   },
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "\n",
    "def get_config():\n",
    "\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(\n",
    "            \"continuousDoubleAuction-v0\",\n",
    "            # continuousDoubleAuctionEnv,\n",
    "            # env_config={\n",
    "            #     \"num_of_agents\": num_of_traders,\n",
    "            #     \"init_cash\": init_cash,\n",
    "            #     \"tick_size\": tick_size,\n",
    "            #     \"tape_display_length\": tape_display_length,\n",
    "            #     \"max_step\": max_step - 1,\n",
    "            #     \"is_render\": is_render,\n",
    "            # }\n",
    "            env_config=env_config,\n",
    "            # env_config={\"disable_env_checker\": True},\n",
    "        )\n",
    "        .multi_agent(\n",
    "            policies=policies,\n",
    "            policy_mapping_fn=policy_mapping_fn,\n",
    "            policies_to_train=policies_to_train,\n",
    "            count_steps_by = \"env_steps\"  # DEFAULT - but this changes everything!\n",
    "            # count_steps_by=\"agent_steps\",  # ← ADD THIS!\n",
    "        )\n",
    "        # .training(\n",
    "        #     model={\n",
    "        #         \"custom_model\": CustomLSTMRLModule,\n",
    "        #         # \"custom_model_config\": {\n",
    "        #         #     \"fcnet_hiddens\": [256, 256],  # Neural network architecture\n",
    "        #         #     \"fcnet_activation\": \"relu\",\n",
    "        #         # },\n",
    "        #     }\n",
    "        # )\n",
    "        .env_runners(\n",
    "            # num_env_runners=num_workers,\n",
    "\n",
    "            num_env_runners=0, \n",
    "            \n",
    "            # num_envs_per_env_runner=num_envs_per_worker,\n",
    "            # rollout_fragment_length=rollout_fragment_length,\n",
    "            # batch_mode=batch_mode,\n",
    "        )\n",
    "        .learners(\n",
    "            \n",
    "            # Local Learner running on the main process (driver/head node).\n",
    "            # Training runs on CPUs by default, or on a single GPU if num_gpus_per_learner > 0 is set. \n",
    "            # This is suitable for single-node training or simple, non-distributed setups.\n",
    "            num_learners=0,  # Typically 1 learner unless using distributed training\n",
    "\n",
    "            num_gpus_per_learner=num_gpus,  # Trainer GPU allocation\n",
    "            # num_cpus_per_learner=num_cpus_per_worker,\n",
    "        )\n",
    "        .training(\n",
    "            # train_batch_size_per_learner=train_batch_size / 4,\n",
    "            train_batch_size_per_learner=train_batch_size,\n",
    "            train_batch_size=train_batch_size,\n",
    "            num_epochs=4,\n",
    "        )\n",
    "        # .callbacks(SelfPlayCallback)\n",
    "        # .callbacks(lambda: SelfPlayCallback(win_rate_threshold=0.60))           \n",
    "        # .callbacks(lambda: MinimalLeagueCallback(\n",
    "        #     return_threshold=100.0,\n",
    "        #     check_every_n_iters=1,\n",
    "        # ))\n",
    "        .callbacks(lambda: LeagueBasedSelfPlayCallback(\n",
    "            relative_improvement=0.1, \n",
    "            check_every_n_iters=4,\n",
    "            ))\n",
    "\n",
    "        # .output_dir(log_dir)\n",
    "        .framework(\"torch\")  # Explicitly set framework if needed\n",
    "        .debugging(log_level=\"DEBUG\")\n",
    "        # .api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)\n",
    "    )\n",
    "\n",
    "    # # Optional: Configure resources more granularly if needed\n",
    "    # if num_gpus_per_worker > 0:\n",
    "    #     config.env_runners(\n",
    "    #         num_gpus_per_env_runner=num_gpus_per_worker\n",
    "    #     )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKLNyViDkI9O"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163996,
     "status": "ok",
     "timestamp": 1756087446130,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "_Cq_T6fungjx",
    "outputId": "ed6c1255-2795-4496-ac2f-744d5fad9dfd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 10:46:24,621\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-11-30 10:46:24,623\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEBUG: train_batch_size = 4096\n",
      "DEBUG: Expected episodes per iter = 4\n",
      "================================================================================\n",
      "League callback: Using RELATIVE threshold (10% improvement)\n",
      "League opponent probability: 70%\n",
      "League callback: Using RELATIVE threshold (10% improvement)\n",
      "League opponent probability: 70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-11-30 10:46:24,887\tINFO connector_pipeline_v2.py:272 -- Added AddObservationsFromEpisodesToBatch to the end of EnvToModulePipeline.\n",
      "2025-11-30 10:46:24,897\tINFO connector_pipeline_v2.py:272 -- Added AddTimeDimToBatchAndZeroPad to the end of EnvToModulePipeline.\n",
      "2025-11-30 10:46:24,906\tINFO connector_pipeline_v2.py:272 -- Added AddStatesFromEpisodesToBatch to the end of EnvToModulePipeline.\n",
      "2025-11-30 10:46:24,924\tINFO connector_pipeline_v2.py:272 -- Added AgentToModuleMapping to the end of EnvToModulePipeline.\n",
      "2025-11-30 10:46:24,933\tINFO connector_pipeline_v2.py:272 -- Added BatchIndividualItems to the end of EnvToModulePipeline.\n",
      "2025-11-30 10:46:24,943\tINFO connector_pipeline_v2.py:272 -- Added NumpyToTensor to the end of EnvToModulePipeline.\n",
      "2025-11-30 10:46:24,944\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-11-30 10:46:24,976\tINFO connector_pipeline_v2.py:258 -- Added RemoveSingleTsTimeRankFromBatch to the beginning of ModuleToEnvPipeline.\n",
      "2025-11-30 10:46:24,976\tINFO connector_pipeline_v2.py:258 -- Added ModuleToAgentUnmapping to the beginning of ModuleToEnvPipeline.\n",
      "2025-11-30 10:46:24,977\tINFO connector_pipeline_v2.py:258 -- Added UnBatchToIndividualItems to the beginning of ModuleToEnvPipeline.\n",
      "2025-11-30 10:46:24,977\tINFO connector_pipeline_v2.py:258 -- Added TensorToNumpy to the beginning of ModuleToEnvPipeline.\n",
      "2025-11-30 10:46:24,978\tINFO connector_pipeline_v2.py:258 -- Added GetActions to the beginning of ModuleToEnvPipeline.\n",
      "2025-11-30 10:46:24,991\tINFO connector_pipeline_v2.py:272 -- Added NormalizeAndClipActions to the end of ModuleToEnvPipeline.\n",
      "2025-11-30 10:46:24,992\tINFO connector_pipeline_v2.py:272 -- Added ListifyDataForVectorEnv to the end of ModuleToEnvPipeline.\n",
      "2025-11-30 10:46:24,993\tINFO env_runner_group.py:320 -- Inferred observation/action spaces from remote worker (local worker has no env): {'__env__': (None, None), '__env_single__': (Dict('agent_0': Box(-inf, inf, (40,), float32), 'agent_1': Box(-inf, inf, (40,), float32), 'agent_2': Box(-inf, inf, (40,), float32), 'agent_3': Box(-inf, inf, (40,), float32)), Dict('agent_0': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_1': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_2': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)), 'agent_3': Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)))), 'policy_0': (Box(-inf, inf, (40,), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))), 'policy_1': (Box(-inf, inf, (40,), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))), 'policy_2': (Box(-inf, inf, (40,), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12))), 'policy_3': (Box(-inf, inf, (40,), float32), Tuple(Discrete(3), Discrete(4), Box(-1.0, 1.0, (1,), float32), Box(0.0, 1.0, (1,), float32), Discrete(12)))}\n",
      "2025-11-30 10:46:25,031\tINFO connector_pipeline_v2.py:272 -- Added AddObservationsFromEpisodesToBatch to the end of LearnerConnectorPipeline.\n",
      "2025-11-30 10:46:25,032\tINFO connector_pipeline_v2.py:272 -- Added AddColumnsFromEpisodesToTrainBatch to the end of LearnerConnectorPipeline.\n",
      "2025-11-30 10:46:25,041\tINFO connector_pipeline_v2.py:272 -- Added AddTimeDimToBatchAndZeroPad to the end of LearnerConnectorPipeline.\n",
      "2025-11-30 10:46:25,050\tINFO connector_pipeline_v2.py:272 -- Added AddStatesFromEpisodesToBatch to the end of LearnerConnectorPipeline.\n",
      "2025-11-30 10:46:25,059\tINFO connector_pipeline_v2.py:272 -- Added AgentToModuleMapping to the end of LearnerConnectorPipeline.\n",
      "2025-11-30 10:46:25,069\tINFO connector_pipeline_v2.py:272 -- Added BatchIndividualItems to the end of LearnerConnectorPipeline.\n",
      "2025-11-30 10:46:25,078\tINFO connector_pipeline_v2.py:272 -- Added NumpyToTensor to the end of LearnerConnectorPipeline.\n",
      "2025-11-30 10:46:26,826\tINFO connector_pipeline_v2.py:258 -- Added AddOneTsToEpisodesAndTruncate to the beginning of LearnerConnectorPipeline.\n",
      "2025-11-30 10:46:26,860\tINFO connector_pipeline_v2.py:272 -- Added GeneralAdvantageEstimation to the end of LearnerConnectorPipeline.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ACTUAL CONFIG train_batch_size: 4096\n",
      "ACTUAL CONFIG num_env_runners: 0\n",
      "ACTUAL CONFIG num_envs_per_env_runner: 1\n",
      "================================================================================\n",
      "\n",
      "=== Iteration 1 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 2 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 3 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "Trainable policies initialized: {'policy_1', 'policy_0'}\n",
      "\n",
      "================================================================================\n",
      "Iteration 4 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -10379420644.214823, 'agent_3': -12359611082.602528, 'agent_2': -5071004090.60125, 'agent_0': -4251343446.943968}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -7315382045.58\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -6583843841.02\n",
      "  → Agents must achieve at least -731538204.56 less loss\n",
      "  → Example: -6583843841.02 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -10379420644.21 | threshold: -6583843841.02 | diff: -3795576803.19 ( -57.6%) | NAV:   2206229.88 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -4251343446.94 | threshold: -6583843841.02 | diff: +2332500394.08 ( +35.4%) | NAV:   -677634.27 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 4 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 5 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 6 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 7 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 8 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -10879708465.826597, 'agent_3': -12229554261.574364, 'agent_2': -6012822327.980139, 'agent_0': -5015597618.214817}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -7947653042.02\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -7152887737.82\n",
      "  → Agents must achieve at least -794765304.20 less loss\n",
      "  → Example: -7152887737.82 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -10879708465.83 | threshold: -7152887737.82 | diff: -3726820728.01 ( -52.1%) | NAV:   2188056.19 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -5015597618.21 | threshold: -7152887737.82 | diff: +2137290119.60 ( +29.9%) | NAV:   -528151.75 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 8 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 9 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 10 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 11 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 12 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -11561946600.924675, 'agent_3': -12090981019.391233, 'agent_2': -6223855297.50551, 'agent_0': -5346259569.033107}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -8454103084.98\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -7608692776.48\n",
      "  → Agents must achieve at least -845410308.50 less loss\n",
      "  → Example: -7608692776.48 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -11561946600.92 | threshold: -7608692776.48 | diff: -3953253824.44 ( -52.0%) | NAV:   2032694.66 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -5346259569.03 | threshold: -7608692776.48 | diff: +2262433207.45 ( +29.7%) | NAV:   -320682.77 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 12 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 13 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 14 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 15 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 16 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -11795256331.61614, 'agent_3': -12571140715.87259, 'agent_2': -6015815333.080416, 'agent_0': -4913801635.699943}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -8354528983.66\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -7519076085.29\n",
      "  → Agents must achieve at least -835452898.37 less loss\n",
      "  → Example: -7519076085.29 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -11795256331.62 | threshold: -7519076085.29 | diff: -4276180246.32 ( -56.9%) | NAV:   2159365.75 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -4913801635.70 | threshold: -7519076085.29 | diff: +2605274449.59 ( +34.6%) | NAV:   -167546.59 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 16 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 17 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 18 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 19 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 20 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -11567886150.586512, 'agent_3': -12181722674.728445, 'agent_2': -5626538959.184119, 'agent_0': -4558098417.887389}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -8062992284.24\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -7256693055.81\n",
      "  → Agents must achieve at least -806299228.42 less loss\n",
      "  → Example: -7256693055.81 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -11567886150.59 | threshold: -7256693055.81 | diff: -4311193094.77 ( -59.4%) | NAV:   2260113.57 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -4558098417.89 | threshold: -7256693055.81 | diff: +2698594637.93 ( +37.2%) | NAV:     49673.66 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 20 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 21 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 22 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 23 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 24 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -11812592992.71791, 'agent_3': -12850386772.637205, 'agent_2': -5566178879.924072, 'agent_0': -4584806518.591587}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -8198699755.65\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -7378829780.09\n",
      "  → Agents must achieve at least -819869975.57 less loss\n",
      "  → Example: -7378829780.09 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -11812592992.72 | threshold: -7378829780.09 | diff: -4433763212.63 ( -60.1%) | NAV:   2343340.28 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -4584806518.59 | threshold: -7378829780.09 | diff: +2794023261.50 ( +37.9%) | NAV:    201904.12 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 24 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 25 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 26 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 27 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 28 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -10808425044.833565, 'agent_3': -12239682662.639606, 'agent_2': -5544098397.398541, 'agent_0': -4435929918.01796}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -7622177481.43\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -6859959733.28\n",
      "  → Agents must achieve at least -762217748.14 less loss\n",
      "  → Example: -6859959733.28 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -10808425044.83 | threshold: -6859959733.28 | diff: -3948465311.55 ( -57.6%) | NAV:   2346595.20 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -4435929918.02 | threshold: -6859959733.28 | diff: +2424029815.27 ( +35.3%) | NAV:    483571.49 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 28 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 29 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 30 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 31 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 32 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -10038175992.441172, 'agent_3': -12360970163.71476, 'agent_2': -5081490486.074503, 'agent_0': -3860710310.6424804}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -6949443151.54\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -6254498836.39\n",
      "  → Agents must achieve at least -694944315.15 less loss\n",
      "  → Example: -6254498836.39 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -10038175992.44 | threshold: -6254498836.39 | diff: -3783677156.05 ( -60.5%) | NAV:   2297202.65 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -3860710310.64 | threshold: -6254498836.39 | diff: +2393788525.75 ( +38.3%) | NAV:    531842.37 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 32 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 33 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 34 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 35 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 36 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -9068165732.418198, 'agent_3': -12189681372.048134, 'agent_2': -4795587474.498013, 'agent_0': -3720316607.9349327}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -6394241170.18\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -5754817053.16\n",
      "  → Agents must achieve at least -639424117.02 less loss\n",
      "  → Example: -5754817053.16 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -9068165732.42 | threshold: -5754817053.16 | diff: -3313348679.26 ( -57.6%) | NAV:   2364276.56 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -3720316607.93 | threshold: -5754817053.16 | diff: +2034500445.22 ( +35.4%) | NAV:    618002.32 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 36 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 37 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 38 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 39 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 40 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -7963829353.899794, 'agent_3': -11743115775.242407, 'agent_2': -4649031240.946002, 'agent_0': -3831738454.56624}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -5897783904.23\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -5308005513.81\n",
      "  → Agents must achieve at least -589778390.42 less loss\n",
      "  → Example: -5308005513.81 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -7963829353.90 | threshold: -5308005513.81 | diff: -2655823840.09 ( -50.0%) | NAV:   2336379.64 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -3831738454.57 | threshold: -5308005513.81 | diff: +1476267059.24 ( +27.8%) | NAV:    799457.94 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 40 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 41 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 42 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 43 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 44 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -6750050859.282714, 'agent_3': -11071632669.854221, 'agent_2': -4314415758.188502, 'agent_0': -3718622571.1975145}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -5234336715.24\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -4710903043.72\n",
      "  → Agents must achieve at least -523433671.52 less loss\n",
      "  → Example: -4710903043.72 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -6750050859.28 | threshold: -4710903043.72 | diff: -2039147815.57 ( -43.3%) | NAV:   2183123.67 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -3718622571.20 | threshold: -4710903043.72 | diff: +992280472.52 ( +21.1%) | NAV:    836041.86 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 44 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 45 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 46 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 47 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 48 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -5917209736.802498, 'agent_3': -11224964352.665205, 'agent_2': -4409039843.691623, 'agent_0': -3827242567.1128917}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -4872226151.96\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -4385003536.76\n",
      "  → Agents must achieve at least -487222615.20 less loss\n",
      "  → Example: -4385003536.76 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -5917209736.80 | threshold: -4385003536.76 | diff: -1532206200.04 ( -34.9%) | NAV:   2108387.00 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -3827242567.11 | threshold: -4385003536.76 | diff: +557760969.65 ( +12.7%) | NAV:    969515.74 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 48 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 49 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 50 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 51 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 52 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -5521935977.87358, 'agent_3': -11332723504.827469, 'agent_2': -4256881716.445942, 'agent_0': -3810027825.736585}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -4665981901.81\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -4199383711.62\n",
      "  → Agents must achieve at least -466598190.18 less loss\n",
      "  → Example: -4199383711.62 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -5521935977.87 | threshold: -4199383711.62 | diff: -1322552266.25 ( -31.5%) | NAV:   2052938.45 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "  → policy_0 qualifies for league!\n",
      "  → League opponent 'league_0' created successfully!\n",
      "  → Current league size: 1\n",
      "  → Snapshot of policy_0 with return -3810027825.74\n",
      "  -> Policy mapping updated successfully\n",
      "  -> League opponent probability: 70%\n",
      "  -> Trainables will now play against: ['league_0']\n",
      "policy_0: -3810027825.74 | threshold: -4199383711.62 | diff: +389355885.89 (  +9.3%) | NAV:   1070044.91 | ✓ EXCEEDS THRESHOLD AND NAV > START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 52 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 53 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 54 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 55 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 56 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -4449798856.545786, 'agent_3': -9624008154.187372, 'agent_2': -3742395731.029216, 'agent_0': -3299277306.9882383}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -3874538081.77\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -3487084273.59\n",
      "  → Agents must achieve at least -387453808.18 less loss\n",
      "  → Example: -3487084273.59 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -4449798856.55 | threshold: -3487084273.59 | diff: -962714582.96 ( -27.6%) | NAV:   1852749.94 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "  → policy_0 qualifies for league!\n",
      "  → League opponent 'league_1' created successfully!\n",
      "  → Current league size: 2\n",
      "  → Snapshot of policy_0 with return -3299277306.99\n",
      "  -> Policy mapping updated successfully\n",
      "  -> League opponent probability: 70%\n",
      "  -> Trainables will now play against: ['league_0', 'league_1']\n",
      "policy_0: -3299277306.99 | threshold: -3487084273.59 | diff: +187806966.60 (  +5.4%) | NAV:   1088276.05 | ✓ EXCEEDS THRESHOLD AND NAV > START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 56 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 57 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 58 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 59 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 60 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -3565825678.615866, 'agent_3': -8131958866.81815, 'agent_2': -3186164819.9726353, 'agent_0': -2960958350.9184375}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -3263392014.77\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -2937052813.29\n",
      "  → Agents must achieve at least -326339201.48 less loss\n",
      "  → Example: -2937052813.29 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -3565825678.62 | threshold: -2937052813.29 | diff: -628772865.33 ( -21.4%) | NAV:   1790432.92 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -2960958350.92 | threshold: -2937052813.29 | diff: -23905537.63 (  -0.8%) | NAV:   1086266.13 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 60 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 61 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 62 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 63 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 64 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -3049753719.8694158, 'agent_3': -6682113915.111366, 'agent_2': -2710772330.021459, 'agent_0': -2384681852.2071843}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -2717217786.04\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -2445496007.43\n",
      "  → Agents must achieve at least -271721778.60 less loss\n",
      "  → Example: -2445496007.43 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -3049753719.87 | threshold: -2445496007.43 | diff: -604257712.43 ( -24.7%) | NAV:   1641285.57 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "  → policy_0 qualifies for league!\n",
      "  → League opponent 'league_2' created successfully!\n",
      "  → Current league size: 3\n",
      "  → Snapshot of policy_0 with return -2384681852.21\n",
      "  -> Policy mapping updated successfully\n",
      "  -> League opponent probability: 70%\n",
      "  -> Trainables will now play against: ['league_0', 'league_1', 'league_2']\n",
      "policy_0: -2384681852.21 | threshold: -2445496007.43 | diff: +60814155.23 (  +2.5%) | NAV:   1095383.19 | ✓ EXCEEDS THRESHOLD AND NAV > START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 64 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 65 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 66 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 67 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 68 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -2600306509.649992, 'agent_3': -5238696548.299071, 'agent_2': -2300113373.856558, 'agent_0': -2046462999.36696}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -2323384754.51\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -2091046279.06\n",
      "  → Agents must achieve at least -232338475.45 less loss\n",
      "  → Example: -2091046279.06 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -2600306509.65 | threshold: -2091046279.06 | diff: -509260230.59 ( -24.4%) | NAV:   1503493.60 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "  → policy_0 qualifies for league!\n",
      "  → League opponent 'league_3' created successfully!\n",
      "  → Current league size: 4\n",
      "  → Snapshot of policy_0 with return -2046462999.37\n",
      "  -> Policy mapping updated successfully\n",
      "  -> League opponent probability: 70%\n",
      "  -> Trainables will now play against: ['league_0', 'league_1', 'league_2', 'league_3']\n",
      "policy_0: -2046462999.37 | threshold: -2091046279.06 | diff: +44583279.69 (  +2.1%) | NAV:   1089321.55 | ✓ EXCEEDS THRESHOLD AND NAV > START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 68 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 69 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 70 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 71 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 72 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -1951182083.0772114, 'agent_3': -3485210586.1119328, 'agent_2': -1687761865.2854087, 'agent_0': -1472477687.3124926}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -1711829885.19\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -1540646896.68\n",
      "  → Agents must achieve at least -171182988.52 less loss\n",
      "  → Example: -1540646896.68 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -1951182083.08 | threshold: -1540646896.68 | diff: -410535186.40 ( -26.6%) | NAV:   1546237.20 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "  → policy_0 qualifies for league!\n",
      "  → League opponent 'league_4' created successfully!\n",
      "  → Current league size: 5\n",
      "  → Snapshot of policy_0 with return -1472477687.31\n",
      "  -> Policy mapping updated successfully\n",
      "  -> League opponent probability: 70%\n",
      "  -> Trainables will now play against: ['league_0', 'league_1', 'league_2', 'league_3', 'league_4']\n",
      "policy_0: -1472477687.31 | threshold: -1540646896.68 | diff: +68169209.36 (  +4.4%) | NAV:   1020874.78 | ✓ EXCEEDS THRESHOLD AND NAV > START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 72 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 73 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 74 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 75 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 76 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -1309864468.2053094, 'agent_3': -1743749171.5481598, 'agent_2': -1159426109.9504926, 'agent_0': -1111643264.58644}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -1210753866.40\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -1089678479.76\n",
      "  → Agents must achieve at least -121075386.64 less loss\n",
      "  → Example: -1089678479.76 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -1309864468.21 | threshold: -1089678479.76 | diff: -220185988.45 ( -20.2%) | NAV:   1501459.44 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -1111643264.59 | threshold: -1089678479.76 | diff: -21964784.83 (  -2.0%) | NAV:    999242.66 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 76 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 77 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 78 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 79 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 80 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -1103667520.851938, 'agent_3': -921809571.7593341, 'agent_2': -873889642.3959913, 'agent_0': -839656122.5349629}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -971661821.69\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -874495639.52\n",
      "  → Agents must achieve at least -97166182.17 less loss\n",
      "  → Example: -874495639.52 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -1103667520.85 | threshold: -874495639.52 | diff: -229171881.33 ( -26.2%) | NAV:   1464334.60 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -839656122.53 | threshold: -874495639.52 | diff: +34839516.99 (  +4.0%) | NAV:    904666.12 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 80 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 81 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 82 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 83 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 84 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -960539180.7248394, 'agent_3': -746788550.3609784, 'agent_2': -792007094.4982971, 'agent_0': -761453247.7105216}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -860996214.22\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -774896592.80\n",
      "  → Agents must achieve at least -86099621.42 less loss\n",
      "  → Example: -774896592.80 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -960539180.72 | threshold: -774896592.80 | diff: -185642587.93 ( -24.0%) | NAV:   1413830.61 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -761453247.71 | threshold: -774896592.80 | diff: +13443345.09 (  +1.7%) | NAV:    978884.14 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 84 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 85 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 86 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 87 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 88 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -938635902.7456801, 'agent_3': -684217260.3134136, 'agent_2': -689322848.6530095, 'agent_0': -762285213.4258261}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -850460558.09\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -765414502.28\n",
      "  → Agents must achieve at least -85046055.81 less loss\n",
      "  → Example: -765414502.28 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -938635902.75 | threshold: -765414502.28 | diff: -173221400.47 ( -22.6%) | NAV:   1326576.70 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "  → policy_0 qualifies for league!\n",
      "  → League opponent 'league_5' created successfully!\n",
      "  → Current league size: 6\n",
      "  → Snapshot of policy_0 with return -762285213.43\n",
      "  -> Policy mapping updated successfully\n",
      "  -> League opponent probability: 70%\n",
      "  -> Trainables will now play against: ['league_0', 'league_1', 'league_2', 'league_3', 'league_4', 'league_5']\n",
      "policy_0: -762285213.43 | threshold: -765414502.28 | diff: +3129288.85 (  +0.4%) | NAV:   1017522.47 | ✓ EXCEEDS THRESHOLD AND NAV > START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 88 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 89 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 90 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 91 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 92 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -672377032.2873962, 'agent_3': -644914976.0607312, 'agent_2': -599899170.2526517, 'agent_0': -688948228.9589897}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -680662630.62\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -612596367.56\n",
      "  → Agents must achieve at least -68066263.06 less loss\n",
      "  → Example: -612596367.56 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -672377032.29 | threshold: -612596367.56 | diff: -59780664.73 (  -9.8%) | NAV:   1271807.34 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -688948228.96 | threshold: -612596367.56 | diff: -76351861.40 ( -12.5%) | NAV:   1001071.99 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 92 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 93 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 94 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 95 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 96 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -682516127.4701452, 'agent_3': -563359229.7572212, 'agent_2': -584925553.5010973, 'agent_0': -631934481.6574687}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -657225304.56\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -591502774.11\n",
      "  → Agents must achieve at least -65722530.46 less loss\n",
      "  → Example: -591502774.11 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -682516127.47 | threshold: -591502774.11 | diff: -91013353.36 ( -15.4%) | NAV:   1256906.10 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -631934481.66 | threshold: -591502774.11 | diff: -40431707.55 (  -6.8%) | NAV:   1020149.69 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 96 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 97 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 98 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 99 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 100 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -618298479.9409516, 'agent_3': -543328016.1693646, 'agent_2': -570308888.8522269, 'agent_0': -592137939.9515852}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -605218209.95\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -544696388.95\n",
      "  → Agents must achieve at least -60521820.99 less loss\n",
      "  → Example: -544696388.95 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -618298479.94 | threshold: -544696388.95 | diff: -73602090.99 ( -13.5%) | NAV:   1152130.97 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -592137939.95 | threshold: -544696388.95 | diff: -47441551.00 (  -8.7%) | NAV:    984914.70 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 100 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 101 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 102 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 103 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 104 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -543239519.4391185, 'agent_3': -494741294.46942353, 'agent_2': -489577479.020673, 'agent_0': -532228383.7864687}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -537733951.61\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -483960556.45\n",
      "  → Agents must achieve at least -53773395.16 less loss\n",
      "  → Example: -483960556.45 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -543239519.44 | threshold: -483960556.45 | diff: -59278962.99 ( -12.2%) | NAV:   1183869.15 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -532228383.79 | threshold: -483960556.45 | diff: -48267827.33 ( -10.0%) | NAV:    930681.64 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 104 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 105 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 106 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 107 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 108 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -548743076.7262077, 'agent_3': -519727250.7979257, 'agent_2': -512747199.91709894, 'agent_0': -528390136.4099909}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -538566606.57\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -484709945.91\n",
      "  → Agents must achieve at least -53856660.66 less loss\n",
      "  → Example: -484709945.91 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -548743076.73 | threshold: -484709945.91 | diff: -64033130.81 ( -13.2%) | NAV:   1136926.16 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -528390136.41 | threshold: -484709945.91 | diff: -43680190.50 (  -9.0%) | NAV:    917674.86 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 108 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 109 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 110 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 111 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 112 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -501369048.79200524, 'agent_3': -474480981.21152776, 'agent_2': -474957387.45866007, 'agent_0': -483077576.44493735}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -492223312.62\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -443000981.36\n",
      "  → Agents must achieve at least -49222331.26 less loss\n",
      "  → Example: -443000981.36 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -501369048.79 | threshold: -443000981.36 | diff: -58368067.44 ( -13.2%) | NAV:   1126765.40 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -483077576.44 | threshold: -443000981.36 | diff: -40076595.09 (  -9.0%) | NAV:    936117.06 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 112 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 113 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 114 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 115 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 116 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -471356451.8289301, 'agent_3': -456341268.0531271, 'agent_2': -460243548.72377425, 'agent_0': -447969498.2677189}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -459662975.05\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -413696677.54\n",
      "  → Agents must achieve at least -45966297.50 less loss\n",
      "  → Example: -413696677.54 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -471356451.83 | threshold: -413696677.54 | diff: -57659774.29 ( -13.9%) | NAV:   1072294.72 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -447969498.27 | threshold: -413696677.54 | diff: -34272820.72 (  -8.3%) | NAV:    957321.55 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 116 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 117 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 118 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 119 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 120 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -454304959.86918414, 'agent_3': -485739711.2444395, 'agent_2': -459469354.0011861, 'agent_0': -454198645.87532496}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -454251802.87\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -408826622.59\n",
      "  → Agents must achieve at least -45425180.29 less loss\n",
      "  → Example: -408826622.59 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -454304959.87 | threshold: -408826622.59 | diff: -45478337.28 ( -11.1%) | NAV:   1051718.80 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -454198645.88 | threshold: -408826622.59 | diff: -45372023.29 ( -11.1%) | NAV:    904738.32 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 120 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 121 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 122 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 123 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 124 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -433213118.73322415, 'agent_3': -434029852.3893119, 'agent_2': -437103041.5211287, 'agent_0': -493959139.32410234}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -463586129.03\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -417227516.13\n",
      "  → Agents must achieve at least -46358612.90 less loss\n",
      "  → Example: -417227516.13 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -433213118.73 | threshold: -417227516.13 | diff: -15985602.61 (  -3.8%) | NAV:    988316.40 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -493959139.32 | threshold: -417227516.13 | diff: -76731623.20 ( -18.4%) | NAV:    955091.58 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 124 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 125 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 126 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "=== Iteration 127 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n",
      "\n",
      "================================================================================\n",
      "Iteration 128 - League Evaluation\n",
      "================================================================================\n",
      "Agent returns: {'agent_1': -398936342.24325013, 'agent_3': -423044814.0375639, 'agent_2': -411286082.64524055, 'agent_0': -453951819.002016}\n",
      "\n",
      "--- Relative Threshold Mode ---\n",
      "Baseline (mean of trainable): -426444080.62\n",
      "Required improvement: 10%\n",
      "Calculated threshold: -383799672.56\n",
      "  → Agents must achieve at least -42644408.06 less loss\n",
      "  → Example: -383799672.56 or better (less negative)\n",
      "\n",
      "--- Policy Evaluation ---\n",
      "policy_1: -398936342.24 | threshold: -383799672.56 | diff: -15136669.68 (  -3.9%) | NAV:   1029837.99 | ✗ below threshold or NAV < START_CAP\n",
      "policy_0: -453951819.00 | threshold: -383799672.56 | diff: -70152146.44 ( -18.3%) | NAV:    952213.35 | ✗ below threshold or NAV < START_CAP\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Iteration 128 ===\n",
      "num_env_steps_sampled: 4096\n",
      "num_agent_steps_sampled: {'agent_1': 4096, 'agent_3': 4096, 'agent_2': 4096, 'agent_0': 4096}\n",
      "num_env_steps_trained: N/A\n",
      "num_agent_steps_trained: N/A\n"
     ]
    }
   ],
   "source": [
    "def go_train(config):\n",
    "    # trainer = ppo.PPOTrainer(config=config, env=\"continuousDoubleAuction-v0\")\n",
    "\n",
    "    # In your notebook, add this right before config.build():\n",
    "    print(\"=\" * 80)  \n",
    "    print(f\"DEBUG: train_batch_size = {train_batch_size}\")\n",
    "    print(f\"DEBUG: Expected episodes per iter = {num_episodes_per_iter}\")\n",
    "    # print(f\"DEBUG: Agent timesteps per episode = {agent_time_step_per_episode}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    algo = config.build()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ACTUAL CONFIG train_batch_size: {algo.config.train_batch_size}\")\n",
    "    print(f\"ACTUAL CONFIG num_env_runners: {algo.config.num_env_runners}\")\n",
    "    print(f\"ACTUAL CONFIG num_envs_per_env_runner: {algo.config.num_envs_per_env_runner}\")  # ← KEY!\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # if is_restore == True:\n",
    "    #     trainer.restore(restore_path)\n",
    "\n",
    "    # g_store = ray.util.get_actor(\"g_store\")\n",
    "    # result = None\n",
    "    for i in range(num_iters):\n",
    "        result = algo.train()\n",
    "\n",
    "    #     print(pretty_print(result)) # includes result[\"custom_metrics\"]\n",
    "    #     print(\"training loop = {} of {}\".format(i + 1, num_iters))\n",
    "    #     print(\"eps sampled so far {}\".format(ray.get(g_store.get_eps_counter.remote())))\n",
    "\n",
    "    #     if i % chkpt_freq == 0:\n",
    "    #         checkpoint = algo.save(local_dir)\n",
    "    #         print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "    # checkpoint = algo.save(local_dir)\n",
    "    # print(\"checkpoint saved at\", checkpoint)\n",
    "    # print(\"result['experiment_id']\", result[\"experiment_id\"])\n",
    "\n",
    "                # Print step counts\n",
    "        env_runner_results = result.get('env_runners', {})\n",
    "        \n",
    "        print(f\"\\n=== Iteration {i+1} ===\")\n",
    "        print(f\"num_env_steps_sampled: {env_runner_results.get('num_env_steps_sampled', 'N/A')}\")\n",
    "        print(f\"num_agent_steps_sampled: {env_runner_results.get('num_agent_steps_sampled', 'N/A')}\")\n",
    "        print(f\"num_env_steps_trained: {env_runner_results.get('num_env_steps_trained', 'N/A')}\")\n",
    "        print(f\"num_agent_steps_trained: {env_runner_results.get('num_agent_steps_trained', 'N/A')}\")\n",
    "\n",
    "    # return result[\"experiment_id\"]\n",
    "    return None\n",
    "\n",
    "# run everything\n",
    "experiment_id = go_train(get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756087446179,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "BMikbPugngj9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1756087446266,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "MrcLYiHrngj9",
    "outputId": "9a2fee4b-538b-4286-ad42-c2fb9af8f535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-30 11:28:25\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_datetime = datetime.now()\n",
    "formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(formatted_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756087446269,
     "user": {
      "displayName": "Cheow Huan Chua",
      "userId": "07361709631473644347"
     },
     "user_tz": -480
    },
    "id": "sF-9OyQangj-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
