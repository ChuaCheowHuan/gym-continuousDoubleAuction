{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d8dad7-30b7-403f-84c7-32f84ebcf99f",
   "metadata": {},
   "source": [
    "# TEST tf GPU with rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab4ad28-4f63-4479-b0cd-bec9a3d50c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957fce23-628b-442f-a814-205f423169ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !su - jovyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417c7343-54ec-492c-bc86-4843bc87eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cut -d: -f1 /etc/passwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b54be12-b80e-4534-8962-6bbac8810a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !getent group sudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20aaa76c-0216-42c3-bcaf-f2d40a26a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !getent group wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa37ac1-2e55-459f-99cd-fd9c0518b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !grep man /etc/passwd\n",
    "# !grep nobody /etc/passwd\n",
    "# !grep gnats /etc/passwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef26ff9-2f81-4c93-ba02-da3403a81521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef5895d4-1fb1-4d4d-a594-c0e03a05901d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808712e9-f5ff-4a52-b521-b57650cd1948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a362715-d645-45d7-ab21-150c8a361a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip show ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135e9af0-918c-4ce8-9723-bd18f6bfba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbd16e1b-c4a6-4666-9fb1-dbea151a72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat /usr/local/cuda/version.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cdb8ba3-6b02-4921-b515-55e0f9a9b29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee38ced1-aeff-4d29-bc1b-847d04a5a30b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Check TensorFlow version\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# # Check if GPU is available\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# # List all physical devices\n",
    "# devices = tf.config.list_physical_devices()\n",
    "# print(\"\\nAll Physical Devices:\")\n",
    "# for device in devices:\n",
    "#     print(device)\n",
    "\n",
    "# # Optional: Run a small computation to test GPU usage\n",
    "# def test_gpu():\n",
    "#     a = tf.constant([1.0, 2.0, 3.0], shape=[1, 3])\n",
    "#     b = tf.constant([4.0, 5.0, 6.0], shape=[3, 1])\n",
    "#     c = tf.matmul(a, b)\n",
    "#     # print(\"\\nMatmul result (should be [[32.]]):\", c.numpy())\n",
    "#     print(\"\\nMatmul result (should be [[32.]]):\", c)\n",
    "\n",
    "# test_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d2bf1b-1bda-4845-8a08-737277788ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"CUDA version:\", tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "# print(\"cuDNN version:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6685c948-db7f-4802-bfd8-068cb28f47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ldconfig -p | grep cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3fd3b65-3f14-43d7-994e-95a1e640e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ldconfig -p | grep nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37927128-7442-4d6b-ab01-5ed805af1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ldconfig -p | grep cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4050bccb-d8bf-4cca-a798-0a9454a9671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fde6c44-7ed0-4fcd-8b53-74c72082119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "560d11fd-ae01-4262-8441-71d26e69b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21de666d-8ea7-4a9f-aadc-9523af0d1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13cd028b-134c-4470-aaf0-7f7afcf46258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ray[rllib]==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d604e07-06b6-4b96-a640-cb733e293d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U \"ray[rllib]==2.35\" tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a35527-0db1-41a4-a82b-743df4466de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee1e35f8-eaaf-4461-873a-8d169f2aa649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e4aacc8-723a-4826-81e4-42adbd0abdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:557: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-07-02 16:01:43,770\tWARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67096576 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.89gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-02 16:01:44,915\tINFO worker.py:1783 -- Started a local Ray instance.\n",
      "2025-07-02 16:01:48,744\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2025-07-02 16:01:50,589\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "2025-07-02 16:01:50,618\tWARNING tf_utils.py:797 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "2025-07-02 16:01:50,733\tWARNING tf_utils.py:797 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "2025-07-02 16:01:51,158\tWARNING tf_utils.py:797 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224,\n",
       "     'cur_lr': 4.999999873689376e-05,\n",
       "     'total_loss': 8.728438,\n",
       "     'policy_loss': -0.037841685,\n",
       "     'vf_loss': 8.760701,\n",
       "     'vf_explained_var': 0.008172772,\n",
       "     'kl': 0.02789545,\n",
       "     'entropy': 0.66543263,\n",
       "     'entropy_coeff': 0.0},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': 125.0,\n",
       "    'num_grad_updates_lifetime': 480.5,\n",
       "    'diff_num_grad_updates_vs_sampler_policy': 479.5}},\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 4000,\n",
       "  'num_agent_steps_trained': 4000},\n",
       " 'env_runners': {'episode_reward_max': 76.0,\n",
       "  'episode_reward_min': 9.0,\n",
       "  'episode_reward_mean': 21.912087912087912,\n",
       "  'episode_len_mean': 21.912087912087912,\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 3988,\n",
       "  'policy_reward_min': {'default_policy': 9.0},\n",
       "  'policy_reward_max': {'default_policy': 76.0},\n",
       "  'policy_reward_mean': {'default_policy': 21.912087912087912},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [15.0,\n",
       "    60.0,\n",
       "    15.0,\n",
       "    27.0,\n",
       "    9.0,\n",
       "    12.0,\n",
       "    13.0,\n",
       "    48.0,\n",
       "    45.0,\n",
       "    27.0,\n",
       "    12.0,\n",
       "    17.0,\n",
       "    13.0,\n",
       "    15.0,\n",
       "    13.0,\n",
       "    23.0,\n",
       "    15.0,\n",
       "    17.0,\n",
       "    18.0,\n",
       "    11.0,\n",
       "    16.0,\n",
       "    34.0,\n",
       "    26.0,\n",
       "    37.0,\n",
       "    51.0,\n",
       "    32.0,\n",
       "    26.0,\n",
       "    19.0,\n",
       "    18.0,\n",
       "    21.0,\n",
       "    21.0,\n",
       "    18.0,\n",
       "    44.0,\n",
       "    14.0,\n",
       "    76.0,\n",
       "    11.0,\n",
       "    12.0,\n",
       "    29.0,\n",
       "    18.0,\n",
       "    12.0,\n",
       "    12.0,\n",
       "    23.0,\n",
       "    24.0,\n",
       "    10.0,\n",
       "    17.0,\n",
       "    70.0,\n",
       "    30.0,\n",
       "    17.0,\n",
       "    11.0,\n",
       "    50.0,\n",
       "    27.0,\n",
       "    18.0,\n",
       "    13.0,\n",
       "    17.0,\n",
       "    23.0,\n",
       "    22.0,\n",
       "    16.0,\n",
       "    10.0,\n",
       "    23.0,\n",
       "    21.0,\n",
       "    10.0,\n",
       "    14.0,\n",
       "    17.0,\n",
       "    16.0,\n",
       "    46.0,\n",
       "    20.0,\n",
       "    15.0,\n",
       "    30.0,\n",
       "    11.0,\n",
       "    16.0,\n",
       "    43.0,\n",
       "    13.0,\n",
       "    27.0,\n",
       "    19.0,\n",
       "    24.0,\n",
       "    20.0,\n",
       "    14.0,\n",
       "    23.0,\n",
       "    18.0,\n",
       "    19.0,\n",
       "    14.0,\n",
       "    20.0,\n",
       "    14.0,\n",
       "    21.0,\n",
       "    32.0,\n",
       "    18.0,\n",
       "    17.0,\n",
       "    27.0,\n",
       "    19.0,\n",
       "    13.0,\n",
       "    9.0,\n",
       "    20.0,\n",
       "    12.0,\n",
       "    19.0,\n",
       "    16.0,\n",
       "    19.0,\n",
       "    14.0,\n",
       "    13.0,\n",
       "    19.0,\n",
       "    15.0,\n",
       "    16.0,\n",
       "    13.0,\n",
       "    12.0,\n",
       "    29.0,\n",
       "    23.0,\n",
       "    12.0,\n",
       "    12.0,\n",
       "    39.0,\n",
       "    28.0,\n",
       "    14.0,\n",
       "    17.0,\n",
       "    10.0,\n",
       "    46.0,\n",
       "    41.0,\n",
       "    10.0,\n",
       "    13.0,\n",
       "    42.0,\n",
       "    12.0,\n",
       "    13.0,\n",
       "    17.0,\n",
       "    18.0,\n",
       "    24.0,\n",
       "    23.0,\n",
       "    19.0,\n",
       "    21.0,\n",
       "    10.0,\n",
       "    12.0,\n",
       "    14.0,\n",
       "    20.0,\n",
       "    35.0,\n",
       "    18.0,\n",
       "    19.0,\n",
       "    11.0,\n",
       "    51.0,\n",
       "    16.0,\n",
       "    34.0,\n",
       "    66.0,\n",
       "    14.0,\n",
       "    12.0,\n",
       "    14.0,\n",
       "    33.0,\n",
       "    28.0,\n",
       "    9.0,\n",
       "    18.0,\n",
       "    50.0,\n",
       "    26.0,\n",
       "    11.0,\n",
       "    16.0,\n",
       "    19.0,\n",
       "    19.0,\n",
       "    44.0,\n",
       "    13.0,\n",
       "    56.0,\n",
       "    25.0,\n",
       "    13.0,\n",
       "    20.0,\n",
       "    13.0,\n",
       "    14.0,\n",
       "    20.0,\n",
       "    25.0,\n",
       "    13.0,\n",
       "    16.0,\n",
       "    42.0,\n",
       "    15.0,\n",
       "    23.0,\n",
       "    24.0,\n",
       "    15.0,\n",
       "    11.0,\n",
       "    21.0,\n",
       "    11.0,\n",
       "    13.0,\n",
       "    36.0,\n",
       "    25.0,\n",
       "    15.0,\n",
       "    20.0,\n",
       "    20.0,\n",
       "    22.0,\n",
       "    15.0,\n",
       "    14.0,\n",
       "    34.0,\n",
       "    21.0,\n",
       "    40.0],\n",
       "   'episode_lengths': [15,\n",
       "    60,\n",
       "    15,\n",
       "    27,\n",
       "    9,\n",
       "    12,\n",
       "    13,\n",
       "    48,\n",
       "    45,\n",
       "    27,\n",
       "    12,\n",
       "    17,\n",
       "    13,\n",
       "    15,\n",
       "    13,\n",
       "    23,\n",
       "    15,\n",
       "    17,\n",
       "    18,\n",
       "    11,\n",
       "    16,\n",
       "    34,\n",
       "    26,\n",
       "    37,\n",
       "    51,\n",
       "    32,\n",
       "    26,\n",
       "    19,\n",
       "    18,\n",
       "    21,\n",
       "    21,\n",
       "    18,\n",
       "    44,\n",
       "    14,\n",
       "    76,\n",
       "    11,\n",
       "    12,\n",
       "    29,\n",
       "    18,\n",
       "    12,\n",
       "    12,\n",
       "    23,\n",
       "    24,\n",
       "    10,\n",
       "    17,\n",
       "    70,\n",
       "    30,\n",
       "    17,\n",
       "    11,\n",
       "    50,\n",
       "    27,\n",
       "    18,\n",
       "    13,\n",
       "    17,\n",
       "    23,\n",
       "    22,\n",
       "    16,\n",
       "    10,\n",
       "    23,\n",
       "    21,\n",
       "    10,\n",
       "    14,\n",
       "    17,\n",
       "    16,\n",
       "    46,\n",
       "    20,\n",
       "    15,\n",
       "    30,\n",
       "    11,\n",
       "    16,\n",
       "    43,\n",
       "    13,\n",
       "    27,\n",
       "    19,\n",
       "    24,\n",
       "    20,\n",
       "    14,\n",
       "    23,\n",
       "    18,\n",
       "    19,\n",
       "    14,\n",
       "    20,\n",
       "    14,\n",
       "    21,\n",
       "    32,\n",
       "    18,\n",
       "    17,\n",
       "    27,\n",
       "    19,\n",
       "    13,\n",
       "    9,\n",
       "    20,\n",
       "    12,\n",
       "    19,\n",
       "    16,\n",
       "    19,\n",
       "    14,\n",
       "    13,\n",
       "    19,\n",
       "    15,\n",
       "    16,\n",
       "    13,\n",
       "    12,\n",
       "    29,\n",
       "    23,\n",
       "    12,\n",
       "    12,\n",
       "    39,\n",
       "    28,\n",
       "    14,\n",
       "    17,\n",
       "    10,\n",
       "    46,\n",
       "    41,\n",
       "    10,\n",
       "    13,\n",
       "    42,\n",
       "    12,\n",
       "    13,\n",
       "    17,\n",
       "    18,\n",
       "    24,\n",
       "    23,\n",
       "    19,\n",
       "    21,\n",
       "    10,\n",
       "    12,\n",
       "    14,\n",
       "    20,\n",
       "    35,\n",
       "    18,\n",
       "    19,\n",
       "    11,\n",
       "    51,\n",
       "    16,\n",
       "    34,\n",
       "    66,\n",
       "    14,\n",
       "    12,\n",
       "    14,\n",
       "    33,\n",
       "    28,\n",
       "    9,\n",
       "    18,\n",
       "    50,\n",
       "    26,\n",
       "    11,\n",
       "    16,\n",
       "    19,\n",
       "    19,\n",
       "    44,\n",
       "    13,\n",
       "    56,\n",
       "    25,\n",
       "    13,\n",
       "    20,\n",
       "    13,\n",
       "    14,\n",
       "    20,\n",
       "    25,\n",
       "    13,\n",
       "    16,\n",
       "    42,\n",
       "    15,\n",
       "    23,\n",
       "    24,\n",
       "    15,\n",
       "    11,\n",
       "    21,\n",
       "    11,\n",
       "    13,\n",
       "    36,\n",
       "    25,\n",
       "    15,\n",
       "    20,\n",
       "    20,\n",
       "    22,\n",
       "    15,\n",
       "    14,\n",
       "    34,\n",
       "    21,\n",
       "    40],\n",
       "   'policy_default_policy_reward': [15.0,\n",
       "    60.0,\n",
       "    15.0,\n",
       "    27.0,\n",
       "    9.0,\n",
       "    12.0,\n",
       "    13.0,\n",
       "    48.0,\n",
       "    45.0,\n",
       "    27.0,\n",
       "    12.0,\n",
       "    17.0,\n",
       "    13.0,\n",
       "    15.0,\n",
       "    13.0,\n",
       "    23.0,\n",
       "    15.0,\n",
       "    17.0,\n",
       "    18.0,\n",
       "    11.0,\n",
       "    16.0,\n",
       "    34.0,\n",
       "    26.0,\n",
       "    37.0,\n",
       "    51.0,\n",
       "    32.0,\n",
       "    26.0,\n",
       "    19.0,\n",
       "    18.0,\n",
       "    21.0,\n",
       "    21.0,\n",
       "    18.0,\n",
       "    44.0,\n",
       "    14.0,\n",
       "    76.0,\n",
       "    11.0,\n",
       "    12.0,\n",
       "    29.0,\n",
       "    18.0,\n",
       "    12.0,\n",
       "    12.0,\n",
       "    23.0,\n",
       "    24.0,\n",
       "    10.0,\n",
       "    17.0,\n",
       "    70.0,\n",
       "    30.0,\n",
       "    17.0,\n",
       "    11.0,\n",
       "    50.0,\n",
       "    27.0,\n",
       "    18.0,\n",
       "    13.0,\n",
       "    17.0,\n",
       "    23.0,\n",
       "    22.0,\n",
       "    16.0,\n",
       "    10.0,\n",
       "    23.0,\n",
       "    21.0,\n",
       "    10.0,\n",
       "    14.0,\n",
       "    17.0,\n",
       "    16.0,\n",
       "    46.0,\n",
       "    20.0,\n",
       "    15.0,\n",
       "    30.0,\n",
       "    11.0,\n",
       "    16.0,\n",
       "    43.0,\n",
       "    13.0,\n",
       "    27.0,\n",
       "    19.0,\n",
       "    24.0,\n",
       "    20.0,\n",
       "    14.0,\n",
       "    23.0,\n",
       "    18.0,\n",
       "    19.0,\n",
       "    14.0,\n",
       "    20.0,\n",
       "    14.0,\n",
       "    21.0,\n",
       "    32.0,\n",
       "    18.0,\n",
       "    17.0,\n",
       "    27.0,\n",
       "    19.0,\n",
       "    13.0,\n",
       "    9.0,\n",
       "    20.0,\n",
       "    12.0,\n",
       "    19.0,\n",
       "    16.0,\n",
       "    19.0,\n",
       "    14.0,\n",
       "    13.0,\n",
       "    19.0,\n",
       "    15.0,\n",
       "    16.0,\n",
       "    13.0,\n",
       "    12.0,\n",
       "    29.0,\n",
       "    23.0,\n",
       "    12.0,\n",
       "    12.0,\n",
       "    39.0,\n",
       "    28.0,\n",
       "    14.0,\n",
       "    17.0,\n",
       "    10.0,\n",
       "    46.0,\n",
       "    41.0,\n",
       "    10.0,\n",
       "    13.0,\n",
       "    42.0,\n",
       "    12.0,\n",
       "    13.0,\n",
       "    17.0,\n",
       "    18.0,\n",
       "    24.0,\n",
       "    23.0,\n",
       "    19.0,\n",
       "    21.0,\n",
       "    10.0,\n",
       "    12.0,\n",
       "    14.0,\n",
       "    20.0,\n",
       "    35.0,\n",
       "    18.0,\n",
       "    19.0,\n",
       "    11.0,\n",
       "    51.0,\n",
       "    16.0,\n",
       "    34.0,\n",
       "    66.0,\n",
       "    14.0,\n",
       "    12.0,\n",
       "    14.0,\n",
       "    33.0,\n",
       "    28.0,\n",
       "    9.0,\n",
       "    18.0,\n",
       "    50.0,\n",
       "    26.0,\n",
       "    11.0,\n",
       "    16.0,\n",
       "    19.0,\n",
       "    19.0,\n",
       "    44.0,\n",
       "    13.0,\n",
       "    56.0,\n",
       "    25.0,\n",
       "    13.0,\n",
       "    20.0,\n",
       "    13.0,\n",
       "    14.0,\n",
       "    20.0,\n",
       "    25.0,\n",
       "    13.0,\n",
       "    16.0,\n",
       "    42.0,\n",
       "    15.0,\n",
       "    23.0,\n",
       "    24.0,\n",
       "    15.0,\n",
       "    11.0,\n",
       "    21.0,\n",
       "    11.0,\n",
       "    13.0,\n",
       "    36.0,\n",
       "    25.0,\n",
       "    15.0,\n",
       "    20.0,\n",
       "    20.0,\n",
       "    22.0,\n",
       "    15.0,\n",
       "    14.0,\n",
       "    34.0,\n",
       "    21.0,\n",
       "    40.0]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.13977517319159238,\n",
       "   'mean_inference_ms': 0.6931809272742859,\n",
       "   'mean_action_processing_ms': 0.050601408206911995,\n",
       "   'mean_env_wait_ms': 0.026269008302687812,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.002489378164102743,\n",
       "   'StateBufferConnector_ms': 0.0019669532775878906,\n",
       "   'ViewRequirementAgentConnector_ms': 0.047914012447818295},\n",
       "  'num_episodes': 182,\n",
       "  'episode_return_max': 76.0,\n",
       "  'episode_return_min': 9.0,\n",
       "  'episode_return_mean': 21.912087912087912,\n",
       "  'episodes_this_iter': 182},\n",
       " 'num_healthy_workers': 2,\n",
       " 'num_in_flight_async_sample_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 4000,\n",
       " 'num_agent_steps_trained': 4000,\n",
       " 'num_env_steps_sampled': 4000,\n",
       " 'num_env_steps_trained': 4000,\n",
       " 'num_env_steps_sampled_this_iter': 4000,\n",
       " 'num_env_steps_trained_this_iter': 4000,\n",
       " 'num_env_steps_sampled_throughput_per_sec': 311.5544526736365,\n",
       " 'num_env_steps_trained_throughput_per_sec': 311.5544526736365,\n",
       " 'timesteps_total': 4000,\n",
       " 'num_env_steps_sampled_lifetime': 4000,\n",
       " 'num_agent_steps_sampled_lifetime': 4000,\n",
       " 'num_steps_trained_this_iter': 4000,\n",
       " 'agent_timesteps_total': 4000,\n",
       " 'timers': {'training_iteration_time_ms': 12838.866,\n",
       "  'restore_workers_time_ms': 0.016,\n",
       "  'training_step_time_ms': 12838.809,\n",
       "  'sample_time_ms': 1843.764,\n",
       "  'learn_time_ms': 10984.883,\n",
       "  'learn_throughput': 364.137,\n",
       "  'synch_weights_time_ms': 8.934},\n",
       " 'counters': {'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 4000,\n",
       "  'num_agent_steps_trained': 4000},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2025-07-02_16-02-01',\n",
       " 'timestamp': 1751472121,\n",
       " 'time_this_iter_s': 12.845617771148682,\n",
       " 'time_total_s': 12.845617771148682,\n",
       " 'pid': 508,\n",
       " 'hostname': '172967f87dca',\n",
       " 'node_ip': '172.18.0.2',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'enable_rl_module_and_learner': False,\n",
       "  'enable_env_runner_and_connector_v2': False,\n",
       "  'env': 'CartPole-v1',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 2,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'enable_connectors': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size': 4000,\n",
       "  'train_batch_size_per_learner': None,\n",
       "  'model': {'_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  '_learner_class': None,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN(aid, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'input_read_episodes': False,\n",
       "  'input_compress_columns': ['obs', 'new_obs'],\n",
       "  'input_spaces_jsonable': True,\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'output_max_rows_per_file': None,\n",
       "  'output_write_method': 'write_parquet',\n",
       "  'output_write_method_kwargs': {},\n",
       "  'output_filesystem': None,\n",
       "  'output_filesystem_kwargs': {},\n",
       "  'output_write_episodes': True,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_run_training_always_in_thread': False,\n",
       "  '_evaluation_parallel_to_training_wo_thread': False,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'recreate_failed_env_runners': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30,\n",
       "  'env_runner_restore_timeout_s': 1800,\n",
       "  '_model_config_dict': {},\n",
       "  '_rl_module_spec': None,\n",
       "  '_AlgorithmConfig__prior_exploration_config': None,\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'simple_optimizer': True,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'disable_env_checking': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'lr_schedule': None,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'mini_batch_size_per_learner': None,\n",
       "  'num_sgd_iter': 30,\n",
       "  'shuffle_sequences': True,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'vf_share_layers': -1,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'default_policy': (None, None, None, None)},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'tf2'},\n",
       " 'time_since_restore': 12.845617771148682,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': 11.242857142857144,\n",
       "  'ram_util_percent': 18.285714285714285}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "config = PPOConfig().environment(\"CartPole-v1\").framework(\"tf2\").training(gamma=0.99, lr=5e-05)\n",
    "\n",
    "algo = config.build()\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7aecf-ead6-43d7-9ade-de79edb22916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ff8b98a-078f-4b96-991e-dcae57a5ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 16:05:15,943\tWARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67072000 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.74gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-07-02 16:05:15,998\tINFO worker.py:1783 -- Started a local Ray instance.\n",
      "2025-07-02 16:05:18,850\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2025-07-02 16:05:20,678\tWARNING tf_utils.py:797 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "2025-07-02 16:05:20,791\tWARNING tf_utils.py:797 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n",
      "2025-07-02 16:05:21,248\tWARNING tf_utils.py:797 -- KL divergence is non-finite, this will likely destabilize your model and the training process. Action(s) in a specific state have near-zero probability. This can happen naturally in deterministic environments where the optimal policy has zero mass for a specific action. To fix this issue, consider setting the coefficient for the KL loss term to zero or increasing policy entropy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': 8.979079, 'policy_loss': -0.04639703, 'vf_loss': 9.019645, 'vf_explained_var': -0.05377843, 'kl': 0.029162679, 'entropy': 0.6647594, 'entropy_coeff': 0.0}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 480.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'env_runners': {'episode_reward_max': 67.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 21.518918918918917, 'episode_len_mean': 21.518918918918917, 'episode_media': {}, 'episodes_timesteps_total': 3981, 'policy_reward_min': {'default_policy': 8.0}, 'policy_reward_max': {'default_policy': 67.0}, 'policy_reward_mean': {'default_policy': 21.518918918918917}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [15.0, 28.0, 12.0, 32.0, 12.0, 29.0, 30.0, 13.0, 16.0, 11.0, 18.0, 17.0, 15.0, 12.0, 17.0, 11.0, 10.0, 42.0, 41.0, 27.0, 13.0, 30.0, 10.0, 50.0, 17.0, 47.0, 21.0, 12.0, 32.0, 20.0, 17.0, 31.0, 18.0, 57.0, 12.0, 10.0, 26.0, 14.0, 10.0, 12.0, 18.0, 16.0, 17.0, 12.0, 13.0, 30.0, 13.0, 31.0, 24.0, 23.0, 27.0, 17.0, 39.0, 15.0, 27.0, 12.0, 19.0, 10.0, 31.0, 47.0, 17.0, 21.0, 12.0, 21.0, 15.0, 12.0, 17.0, 16.0, 21.0, 22.0, 14.0, 15.0, 12.0, 10.0, 29.0, 26.0, 9.0, 11.0, 42.0, 27.0, 8.0, 50.0, 23.0, 20.0, 32.0, 26.0, 17.0, 15.0, 12.0, 13.0, 20.0, 21.0, 11.0, 17.0, 26.0, 12.0, 19.0, 11.0, 11.0, 18.0, 32.0, 11.0, 11.0, 26.0, 17.0, 26.0, 31.0, 24.0, 14.0, 38.0, 13.0, 16.0, 23.0, 59.0, 67.0, 15.0, 16.0, 11.0, 16.0, 29.0, 13.0, 36.0, 17.0, 18.0, 25.0, 44.0, 13.0, 10.0, 46.0, 15.0, 67.0, 15.0, 26.0, 17.0, 17.0, 14.0, 30.0, 18.0, 19.0, 25.0, 19.0, 14.0, 15.0, 35.0, 13.0, 43.0, 15.0, 32.0, 12.0, 19.0, 58.0, 23.0, 18.0, 15.0, 15.0, 25.0, 14.0, 18.0, 19.0, 11.0, 8.0, 11.0, 17.0, 25.0, 43.0, 19.0, 25.0, 46.0, 22.0, 26.0, 25.0, 17.0, 19.0, 26.0, 32.0, 13.0, 25.0, 17.0, 11.0, 18.0, 15.0, 8.0, 12.0, 14.0, 15.0], 'episode_lengths': [15, 28, 12, 32, 12, 29, 30, 13, 16, 11, 18, 17, 15, 12, 17, 11, 10, 42, 41, 27, 13, 30, 10, 50, 17, 47, 21, 12, 32, 20, 17, 31, 18, 57, 12, 10, 26, 14, 10, 12, 18, 16, 17, 12, 13, 30, 13, 31, 24, 23, 27, 17, 39, 15, 27, 12, 19, 10, 31, 47, 17, 21, 12, 21, 15, 12, 17, 16, 21, 22, 14, 15, 12, 10, 29, 26, 9, 11, 42, 27, 8, 50, 23, 20, 32, 26, 17, 15, 12, 13, 20, 21, 11, 17, 26, 12, 19, 11, 11, 18, 32, 11, 11, 26, 17, 26, 31, 24, 14, 38, 13, 16, 23, 59, 67, 15, 16, 11, 16, 29, 13, 36, 17, 18, 25, 44, 13, 10, 46, 15, 67, 15, 26, 17, 17, 14, 30, 18, 19, 25, 19, 14, 15, 35, 13, 43, 15, 32, 12, 19, 58, 23, 18, 15, 15, 25, 14, 18, 19, 11, 8, 11, 17, 25, 43, 19, 25, 46, 22, 26, 25, 17, 19, 26, 32, 13, 25, 17, 11, 18, 15, 8, 12, 14, 15], 'policy_default_policy_reward': [15.0, 28.0, 12.0, 32.0, 12.0, 29.0, 30.0, 13.0, 16.0, 11.0, 18.0, 17.0, 15.0, 12.0, 17.0, 11.0, 10.0, 42.0, 41.0, 27.0, 13.0, 30.0, 10.0, 50.0, 17.0, 47.0, 21.0, 12.0, 32.0, 20.0, 17.0, 31.0, 18.0, 57.0, 12.0, 10.0, 26.0, 14.0, 10.0, 12.0, 18.0, 16.0, 17.0, 12.0, 13.0, 30.0, 13.0, 31.0, 24.0, 23.0, 27.0, 17.0, 39.0, 15.0, 27.0, 12.0, 19.0, 10.0, 31.0, 47.0, 17.0, 21.0, 12.0, 21.0, 15.0, 12.0, 17.0, 16.0, 21.0, 22.0, 14.0, 15.0, 12.0, 10.0, 29.0, 26.0, 9.0, 11.0, 42.0, 27.0, 8.0, 50.0, 23.0, 20.0, 32.0, 26.0, 17.0, 15.0, 12.0, 13.0, 20.0, 21.0, 11.0, 17.0, 26.0, 12.0, 19.0, 11.0, 11.0, 18.0, 32.0, 11.0, 11.0, 26.0, 17.0, 26.0, 31.0, 24.0, 14.0, 38.0, 13.0, 16.0, 23.0, 59.0, 67.0, 15.0, 16.0, 11.0, 16.0, 29.0, 13.0, 36.0, 17.0, 18.0, 25.0, 44.0, 13.0, 10.0, 46.0, 15.0, 67.0, 15.0, 26.0, 17.0, 17.0, 14.0, 30.0, 18.0, 19.0, 25.0, 19.0, 14.0, 15.0, 35.0, 13.0, 43.0, 15.0, 32.0, 12.0, 19.0, 58.0, 23.0, 18.0, 15.0, 15.0, 25.0, 14.0, 18.0, 19.0, 11.0, 8.0, 11.0, 17.0, 25.0, 43.0, 19.0, 25.0, 46.0, 22.0, 26.0, 25.0, 17.0, 19.0, 26.0, 32.0, 13.0, 25.0, 17.0, 11.0, 18.0, 15.0, 8.0, 12.0, 14.0, 15.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.14215572769422863, 'mean_inference_ms': 0.6662503967476826, 'mean_action_processing_ms': 0.05117219726841968, 'mean_env_wait_ms': 0.026623388678099127, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0024379266274941933, 'StateBufferConnector_ms': 0.0018622424151446368, 'ViewRequirementAgentConnector_ms': 0.04888856733167494}, 'num_episodes': 185, 'episode_return_max': 67.0, 'episode_return_min': 8.0, 'episode_return_mean': 21.518918918918917, 'episodes_this_iter': 185}, 'num_healthy_workers': 2, 'num_in_flight_async_sample_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 300.7152564667332, 'num_env_steps_trained_throughput_per_sec': 300.7152564667332, 'timesteps_total': 4000, 'num_env_steps_sampled_lifetime': 4000, 'num_agent_steps_sampled_lifetime': 4000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 13301.634, 'restore_workers_time_ms': 0.014, 'training_step_time_ms': 13301.586, 'sample_time_ms': 1799.833, 'learn_time_ms': 11493.431, 'learn_throughput': 348.025, 'synch_weights_time_ms': 7.984}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-07-02_16-05-32', 'timestamp': 1751472332, 'time_this_iter_s': 13.309550523757935, 'time_total_s': 13.309550523757935, 'pid': 508, 'hostname': '172967f87dca', 'node_ip': '172.18.0.2', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7d6888c4ad40>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2'}, 'time_since_restore': 13.309550523757935, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 11.073333333333332, 'ram_util_percent': 18.639999999999997}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.30000001192092896, 'cur_lr': 4.999999873689376e-05, 'total_loss': 9.465161, 'policy_loss': -0.028040713, 'vf_loss': 9.487885, 'vf_explained_var': 0.01553769, 'kl': 0.017721536, 'entropy': 0.6104195, 'entropy_coeff': 0.0}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 1440.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 8000}, 'env_runners': {'episode_reward_max': 187.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 40.34, 'episode_len_mean': 40.34, 'episode_media': {}, 'episodes_timesteps_total': 4034, 'policy_reward_min': {'default_policy': 8.0}, 'policy_reward_max': {'default_policy': 187.0}, 'policy_reward_mean': {'default_policy': 40.34}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [15.0, 8.0, 12.0, 14.0, 15.0, 18.0, 23.0, 108.0, 46.0, 17.0, 87.0, 16.0, 11.0, 64.0, 19.0, 26.0, 68.0, 49.0, 20.0, 49.0, 13.0, 39.0, 52.0, 43.0, 40.0, 114.0, 51.0, 28.0, 13.0, 68.0, 17.0, 48.0, 15.0, 77.0, 45.0, 40.0, 26.0, 22.0, 28.0, 58.0, 101.0, 187.0, 43.0, 16.0, 93.0, 20.0, 58.0, 12.0, 51.0, 20.0, 10.0, 15.0, 14.0, 32.0, 53.0, 74.0, 25.0, 16.0, 16.0, 27.0, 47.0, 33.0, 114.0, 74.0, 46.0, 37.0, 20.0, 48.0, 40.0, 117.0, 29.0, 20.0, 50.0, 26.0, 22.0, 39.0, 10.0, 44.0, 14.0, 25.0, 33.0, 23.0, 85.0, 35.0, 16.0, 65.0, 12.0, 51.0, 22.0, 80.0, 29.0, 40.0, 19.0, 24.0, 55.0, 32.0, 11.0, 104.0, 21.0, 17.0], 'episode_lengths': [15, 8, 12, 14, 15, 18, 23, 108, 46, 17, 87, 16, 11, 64, 19, 26, 68, 49, 20, 49, 13, 39, 52, 43, 40, 114, 51, 28, 13, 68, 17, 48, 15, 77, 45, 40, 26, 22, 28, 58, 101, 187, 43, 16, 93, 20, 58, 12, 51, 20, 10, 15, 14, 32, 53, 74, 25, 16, 16, 27, 47, 33, 114, 74, 46, 37, 20, 48, 40, 117, 29, 20, 50, 26, 22, 39, 10, 44, 14, 25, 33, 23, 85, 35, 16, 65, 12, 51, 22, 80, 29, 40, 19, 24, 55, 32, 11, 104, 21, 17], 'policy_default_policy_reward': [15.0, 8.0, 12.0, 14.0, 15.0, 18.0, 23.0, 108.0, 46.0, 17.0, 87.0, 16.0, 11.0, 64.0, 19.0, 26.0, 68.0, 49.0, 20.0, 49.0, 13.0, 39.0, 52.0, 43.0, 40.0, 114.0, 51.0, 28.0, 13.0, 68.0, 17.0, 48.0, 15.0, 77.0, 45.0, 40.0, 26.0, 22.0, 28.0, 58.0, 101.0, 187.0, 43.0, 16.0, 93.0, 20.0, 58.0, 12.0, 51.0, 20.0, 10.0, 15.0, 14.0, 32.0, 53.0, 74.0, 25.0, 16.0, 16.0, 27.0, 47.0, 33.0, 114.0, 74.0, 46.0, 37.0, 20.0, 48.0, 40.0, 117.0, 29.0, 20.0, 50.0, 26.0, 22.0, 39.0, 10.0, 44.0, 14.0, 25.0, 33.0, 23.0, 85.0, 35.0, 16.0, 65.0, 12.0, 51.0, 22.0, 80.0, 29.0, 40.0, 19.0, 24.0, 55.0, 32.0, 11.0, 104.0, 21.0, 17.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.1370808770783189, 'mean_inference_ms': 0.6490727381351742, 'mean_action_processing_ms': 0.050881553863225196, 'mean_env_wait_ms': 0.026287811887967832, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.002301454544067383, 'StateBufferConnector_ms': 0.0017714500427246094, 'ViewRequirementAgentConnector_ms': 0.048459529876708984}, 'num_episodes': 95, 'episode_return_max': 187.0, 'episode_return_min': 8.0, 'episode_return_mean': 40.34, 'episodes_this_iter': 95}, 'num_healthy_workers': 2, 'num_in_flight_async_sample_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 8000, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 354.4276735389865, 'num_env_steps_trained_throughput_per_sec': 354.4276735389865, 'timesteps_total': 8000, 'num_env_steps_sampled_lifetime': 8000, 'num_agent_steps_sampled_lifetime': 8000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 8000, 'timers': {'training_iteration_time_ms': 12293.728, 'restore_workers_time_ms': 0.013, 'training_step_time_ms': 12293.68, 'sample_time_ms': 1751.859, 'learn_time_ms': 10533.322, 'learn_throughput': 379.747, 'synch_weights_time_ms': 8.199}, 'counters': {'num_env_steps_sampled': 8000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 8000}, 'done': False, 'training_iteration': 2, 'trial_id': 'default', 'date': '2025-07-02_16-05-43', 'timestamp': 1751472343, 'time_this_iter_s': 11.290157079696655, 'time_total_s': 24.59970760345459, 'pid': 508, 'hostname': '172967f87dca', 'node_ip': '172.18.0.2', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7d6888c4ad40>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2'}, 'time_since_restore': 24.59970760345459, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': 11.21875, 'ram_util_percent': 18.7}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.30000001192092896, 'cur_lr': 4.999999873689376e-05, 'total_loss': 9.790577, 'policy_loss': -0.009574652, 'vf_loss': 9.797734, 'vf_explained_var': 0.007522292, 'kl': 0.008058092, 'entropy': 0.58212817, 'entropy_coeff': 0.0}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 2400.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 12000, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 12000}, 'env_runners': {'episode_reward_max': 350.0, 'episode_reward_min': 10.0, 'episode_reward_mean': 67.39, 'episode_len_mean': 67.39, 'episode_media': {}, 'episodes_timesteps_total': 6739, 'policy_reward_min': {'default_policy': 10.0}, 'policy_reward_max': {'default_policy': 350.0}, 'policy_reward_mean': {'default_policy': 67.39}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [77.0, 45.0, 40.0, 26.0, 22.0, 28.0, 58.0, 101.0, 187.0, 43.0, 16.0, 93.0, 20.0, 58.0, 12.0, 51.0, 20.0, 10.0, 15.0, 14.0, 32.0, 53.0, 74.0, 25.0, 16.0, 16.0, 27.0, 47.0, 33.0, 114.0, 74.0, 46.0, 37.0, 20.0, 48.0, 40.0, 117.0, 29.0, 20.0, 50.0, 26.0, 22.0, 39.0, 10.0, 44.0, 14.0, 25.0, 33.0, 23.0, 85.0, 35.0, 16.0, 65.0, 12.0, 51.0, 22.0, 80.0, 29.0, 40.0, 19.0, 24.0, 55.0, 32.0, 11.0, 104.0, 21.0, 17.0, 181.0, 350.0, 77.0, 218.0, 170.0, 51.0, 115.0, 98.0, 170.0, 143.0, 200.0, 101.0, 116.0, 35.0, 126.0, 81.0, 125.0, 54.0, 91.0, 169.0, 20.0, 150.0, 90.0, 151.0, 346.0, 100.0, 38.0, 47.0, 156.0, 28.0, 48.0, 42.0, 44.0], 'episode_lengths': [77, 45, 40, 26, 22, 28, 58, 101, 187, 43, 16, 93, 20, 58, 12, 51, 20, 10, 15, 14, 32, 53, 74, 25, 16, 16, 27, 47, 33, 114, 74, 46, 37, 20, 48, 40, 117, 29, 20, 50, 26, 22, 39, 10, 44, 14, 25, 33, 23, 85, 35, 16, 65, 12, 51, 22, 80, 29, 40, 19, 24, 55, 32, 11, 104, 21, 17, 181, 350, 77, 218, 170, 51, 115, 98, 170, 143, 200, 101, 116, 35, 126, 81, 125, 54, 91, 169, 20, 150, 90, 151, 346, 100, 38, 47, 156, 28, 48, 42, 44], 'policy_default_policy_reward': [77.0, 45.0, 40.0, 26.0, 22.0, 28.0, 58.0, 101.0, 187.0, 43.0, 16.0, 93.0, 20.0, 58.0, 12.0, 51.0, 20.0, 10.0, 15.0, 14.0, 32.0, 53.0, 74.0, 25.0, 16.0, 16.0, 27.0, 47.0, 33.0, 114.0, 74.0, 46.0, 37.0, 20.0, 48.0, 40.0, 117.0, 29.0, 20.0, 50.0, 26.0, 22.0, 39.0, 10.0, 44.0, 14.0, 25.0, 33.0, 23.0, 85.0, 35.0, 16.0, 65.0, 12.0, 51.0, 22.0, 80.0, 29.0, 40.0, 19.0, 24.0, 55.0, 32.0, 11.0, 104.0, 21.0, 17.0, 181.0, 350.0, 77.0, 218.0, 170.0, 51.0, 115.0, 98.0, 170.0, 143.0, 200.0, 101.0, 116.0, 35.0, 126.0, 81.0, 125.0, 54.0, 91.0, 169.0, 20.0, 150.0, 90.0, 151.0, 346.0, 100.0, 38.0, 47.0, 156.0, 28.0, 48.0, 42.0, 44.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.13600818392705472, 'mean_inference_ms': 0.6471083654780508, 'mean_action_processing_ms': 0.050929013315593984, 'mean_env_wait_ms': 0.026120496523997887, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0023076534271240234, 'StateBufferConnector_ms': 0.0017614364624023438, 'ViewRequirementAgentConnector_ms': 0.04805731773376465}, 'num_episodes': 33, 'episode_return_max': 350.0, 'episode_return_min': 10.0, 'episode_return_mean': 67.39, 'episodes_this_iter': 33}, 'num_healthy_workers': 2, 'num_in_flight_async_sample_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 12000, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 12000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 333.9482543919243, 'num_env_steps_trained_throughput_per_sec': 333.9482543919243, 'timesteps_total': 12000, 'num_env_steps_sampled_lifetime': 12000, 'num_agent_steps_sampled_lifetime': 12000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 12000, 'timers': {'training_iteration_time_ms': 12188.457, 'restore_workers_time_ms': 0.012, 'training_step_time_ms': 12188.413, 'sample_time_ms': 1726.22, 'learn_time_ms': 10451.094, 'learn_throughput': 382.735, 'synch_weights_time_ms': 10.821}, 'counters': {'num_env_steps_sampled': 12000, 'num_env_steps_trained': 12000, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 12000}, 'done': False, 'training_iteration': 3, 'trial_id': 'default', 'date': '2025-07-02_16-05-55', 'timestamp': 1751472355, 'time_this_iter_s': 11.981786727905273, 'time_total_s': 36.58149433135986, 'pid': 508, 'hostname': '172967f87dca', 'node_ip': '172.18.0.2', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'env_task_fn': None, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'num_envs_per_env_runner': 1, 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'enable_connectors': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 1, 'local_gpu_idx': 0, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'train_batch_size_per_learner': None, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7d6888c4ad40>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_run_training_always_in_thread': False, '_evaluation_parallel_to_training_wo_thread': False, 'ignore_env_runner_failures': False, 'recreate_failed_env_runners': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30, 'env_runner_restore_timeout_s': 1800, '_model_config_dict': {}, '_rl_module_spec': None, '_AlgorithmConfig__prior_exploration_config': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'disable_env_checking': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'mini_batch_size_per_learner': None, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'tf2'}, 'time_since_restore': 36.58149433135986, 'iterations_since_restore': 3, 'perf': {'cpu_util_percent': 10.952941176470588, 'ram_util_percent': 18.78235294117647}}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"CartPole-v1\")\n",
    "    .framework(\"tf2\")\n",
    "    .resources(num_gpus=1)  # GPU for learner instead\n",
    "    # .learners(num_learners=1, num_gpus_per_learner=1)\n",
    "    # .api_stack(enable_rl_module_and_learner=False,\n",
    "    #            enable_env_runner_and_connector_v2=False)\n",
    ")\n",
    "algo = config.build()\n",
    "for _ in range(3):\n",
    "    result = algo.train()\n",
    "    # print(\"iter:\", result[\"iteration\"], \"reward:\", result[\"episode_reward_mean\"])\n",
    "    print(result)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a137d42-470e-431b-ae4f-8ad3e011ffa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
